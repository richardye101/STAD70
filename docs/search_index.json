[["index.html", "STAD70 Course Work Chapter 1 Introduction", " STAD70 Course Work Richard Ye Chapter 1 Introduction Work from the STAD70 course taken Winter 2023. "],["financial-data-and-returns.html", "Chapter 2 Financial Data and Returns 2.1 Trading 2.2 Order Types 2.3 Returns", " Chapter 2 Financial Data and Returns https://richardye101.github.io/STAD70/ Prices of financial instruments (stocks, bonds, futures, options, etc) LIBOR: London Inter-Bank Offered Rate Avg interest rate that major London Banks would charge when borrowing from each other FX Rates: Decentralized market that sets currency prices There are two types of raw (tick) data: Quote data Record of bid/ask prices from the order book, usually just the top \\(N\\) lines of the order book on the bid and ask sides Trade data Trade records (filled orders) Data that is identified using time: Intraday Data Data of the most current bids/asks, last price, volume etc Describes the data of a stock within a given day Daily data For longer term analysis 2.1 Trading There are two types: OTC (Over the counter) Negotiated and traded directly between parties, not for the public to bid on Through an Exchange The TSX, NYSE, LSE (London) Uses an auction system where there are many potential buyers and sellers Uses a continuous double auction (CDA), where the bids and asks are matched in real-time to determine which trades to execute. A matching order (bid \\(\\geq\\) ask) is executed right away Outstanding orders are maintained in an order book 2.2 Order Types Limit order: buy/sell are no more/les than a specified price It is maintained in the order book if not filled immediately Market order: buy/sell at the current market price immediately No control over the price at which the order will execute Iceberg order: Only a portion of the total (giant) order is displayed in the order book, and when it is filled, a new portion is shown Used to maintain anonymity 2.3 Returns The ratio of money gained/lost on an investment relative to the invested amount It is defined relative tot he holding period (daily, monthly, annual) 2.3.1 Net Returns \\[ R_{t} = \\frac{P_{t}-P_{t-1}}{P_{t-1}} = \\frac{P_{t}}{P_{t-1}} - 1 \\] Where \\(P_{t}\\) is the price at time \\(T\\). Gross return is just \\(\\frac{P_{t}}{P_{t-1}} = 1 + R_{t}\\) 2.3.2 Log Returns \\[ \\begin{aligned} r_{t} &amp;= \\log(1+R_{t})\\\\ &amp;= \\log\\left( \\frac{P_{t}}{P_{t-1}} \\right)\\\\ &amp;= \\log(P_{t}) - \\log(P_{t-1}) \\end{aligned} \\] You can obtain the log return from the net return (for small returns, \\(&lt;1\\%\\)) using the Taylor Approximation: \\[ \\begin{aligned} f(x) &amp;\\approx f(x_{0}) + f&#39;(x_{0})(x-x_{0}) + \\frac{1}{2}f&#39;&#39;(x_{0})(x-x_{0})^{2}+\\dots\\\\ \\\\ r_{t} &amp;= \\log(1+R_{t})\\\\ &amp;\\approx \\log(x_{0})+\\log&#39;(x)|_{x=x_{0}}\\cdot (\\underbrace{ 1+R_{t} }_{ x }-x_{0})+\\dots\\\\ &amp; \\text{Expand around } x_{0} = 1\\\\ \\\\ &amp;= 0 + \\frac{1}{1}\\cdot R_{t}\\\\ &amp;\\approx R_{t} \\end{aligned} \\] We like to work with log returns because they are also easy to aggregate! \\[ \\begin{aligned} R_{1-22} &amp;= (1+R_{1})\\cdot(1+R_{2})\\cdot \\dots \\cdot (1+R_{22}) - 1\\\\ r_{1-22} &amp;= r_{1} + r_{2} + \\dots + r_{22}\\\\ &amp;= \\log\\left( \\frac{P_{1}}{P_{0}}\\cdot \\frac{P_{2}}{P_{1}} \\cdot \\dots \\cdot \\frac{P_{22}}{P_{21}} \\right)\\\\ &amp;= \\log\\left( \\frac{P_{22}}{P_{0}} \\right)\\\\ \\end{aligned} \\] 2.3.3 Returns accounting for dividends If a dividend was paid just before time \\(t\\), and after \\(t-1\\), then we would have: \\[ \\begin{aligned} R_{t} &amp;= \\frac{P_{t}-D_{t}}{P_{t-1}} - 1 \\\\ \\\\ r_{t} &amp;= \\log(P_{t}+D_{t}) - \\log(P_{t-1}) \\end{aligned} \\] 2.3.4 Returns accounting for splits \\[ \\begin{aligned} R_{t} &amp;= \\frac{P_{t}}{P_{t-1}/s}-1\\\\ r_{t} &amp;= \\log(P_{t}) - \\log(P_{t-1}/s)\\\\ \\text{Where } s &amp;= \\text{number of shares recieved per 1 share owned} \\end{aligned} \\] 2.3.5 Adjusted returns These returns have already accounted for dividends and splits, so returns should always be calculated on adjusted returns. They should not be used as prices (ie the adjusted close)! 2.3.6 Random Walk Model We say the additive log returns \\[ \\log\\left( \\frac{P_{t}}{P_{0}} \\right) = r_{1} + r_{2} + \\dots + r_{t} \\] Follow a random walk model if: \\[ \\begin{aligned} r_{t} &amp;\\sim (\\mu, \\sigma^{2})\\\\ \\\\ \\text{Where } \\quad \\mathbb{E}[r_{i}] &amp;= \\mu \\text{ is drift}\\\\ \\sqrt{ Var[r_{i}] } &amp;= \\sigma \\text{ is volatility} \\end{aligned} \\] However they follow a Normal random walk if: \\[ r_{t} \\sim N(\\mu, \\sigma^{2}) \\] Which implies that log-returns are Normally distributed. The random walk from \\(1-n\\) has: \\[ \\begin{aligned} r_{1-n} &amp;= r_1 + r_{2} + \\dots +r_{n}\\\\ \\\\ \\text{Mean: }&amp; \\mathbb{E}(r_{1-n}) = n\\mu\\\\ \\text{Variance: }&amp; \\sqrt{ Var[r_{1-n}] } = \\sqrt{ n }\\sigma \\end{aligned} \\] Which can be converted to the asset price simply using \\[ P_{t} = P_{0}\\cdot e^{r_{1} + r_{2}+\\dots +r_{t}} \\] Which is referred to as the Exponential/Geometric Random walk Random walks are not a good description of reality, but they are useful for modelling. "],["return-distributions.html", "Chapter 3 Return Distributions 3.1 Skewness 3.2 Kurtosis 3.3 QQ Plot 3.4 Heavy Tail Distributions 3.5 Mixture Distributions", " Chapter 3 Return Distributions 3.1 Skewness Skewness measures symmetry. Positive skew \\(\\to\\) right skewed Negative skew \\(\\to\\) left skewed \\[ Sk = \\mathbb{E}\\left[ \\left( \\frac{X-\\mu}{\\sigma} \\right)^{3} \\right] \\] 3.2 Kurtosis Kurtosis measures how concentrated the data is around the mean, or how heavy the tails are. You can only measure kurtosis (meaningfully) if the distribution has 0 skew. Kurtosis of a distribution is measured against the kurtosis of the Normal distribution, which is \\(3\\). \\[ Kur = \\mathbb{E}\\left[ \\left( \\frac{X-\\mu}{\\sigma} \\right)^{3} \\right] - 3 \\] 3.3 QQ Plot Top left: The sample quantiles spread further on the right than the theoretical quantiles, which implies a right/pos skewness Top right: The sample quantiles for the tails are very concentrated compared to the theoretical quantiles, which implies the data has short/finite tails implying it is platykurtic/neg kurtosis Bottom left: The sample quantiles spread further on the left than the theoretical quantiles, which implies a left/neg skewness Bottom right: The sample quantiles for the tails are very spread compared to the theoretical quantiles, which implies the data has long/heavy tails implying it is leptokurtic/pos kurtosis 3.4 Heavy Tail Distributions A distribution \\(f(x)\\) is said to have: Exponential tails (short/finite tails) if \\[f(x) \\propto e^{-x/\\lambda}\\] Polynomial tails (long/heavy tails) if \\[ \\begin{aligned} f(x) &amp;\\propto x^{-(1+\\alpha)}\\\\ \\\\ \\text{Where } &amp;\\alpha \\text{ is the tail index} \\end{aligned} \\] The smaller the tail index, the heavier the tail Heavy tailed distributions can also have infinite moments (including the mean!) \\[ \\mathbb{E}(X^{k}) = \\infty \\text{ for } k \\geq \\alpha \\] So if the tail index is high, the tail is lighter, and the first \\(\\alpha\\) moments exist. The tail index can be estimated using: - MLE approximation, which takes the derivative log of the product of n the distributions when it equals 0, and solving for \\(\\alpha\\) - \\(\\hat{\\alpha} = \\frac{n}{\\sum^n_{i=1}\\ln\\left( r_{i}/r_{min} \\right)}\\) \\[ \\begin{aligned} L(\\alpha) &amp;= \\prod^{n}_{i=1}[f(x)]\\\\ \\log(L(\\alpha)) = l(\\alpha) &amp;= \\sum^{n}_{i=1} f(x)\\\\ l&#39;(\\alpha) &amp;= \\sum^{n}_{i=1} f&#39;(x) = 0 \\end{aligned} \\] - Pareto Q-Q plot of the empirical CDF and the returns in log x log scale - \\(\\alpha\\) is the estimated slope of the line of best fit 3.4.1 Common Heavy Tail Distributions 3.4.1.1 Pareto \\[ \\begin{aligned} f(x) &amp;= \\frac{a\\ x^{-(1+\\alpha)}}{l^{-\\alpha}}, \\ x \\geq l\\\\ \\end{aligned} \\] Can be used to model absolute returns above a cutoff \\(r_{min}\\) \\[ \\bar{F}(r) = \\left( \\frac{r_{min}}{r} \\right)^{\\alpha} \\quad \\forall\\ r &gt; r_{min} \\] 3.4.1.2 Standard Cauchy \\[ \\begin{aligned} f(x) &amp;= \\frac{1}{\\pi(1+x^{2})}\\\\ &amp;= \\frac{1+x^{-(\\alpha+1)}}{\\pi} \\implies \\alpha = 1 \\end{aligned} \\] Which is the t-distribution when df = 1. \\[ f(x) \\sim t(df=1) \\] 3.4.1.3 Students t \\[ \\begin{aligned} f(x) &amp;= \\frac{\\Gamma\\left( \\frac{v+1}{2} \\right)}{\\sqrt{ v\\pi }\\Gamma(v/2)}\\left( 1+\\frac{x^{2}}{v} \\right)^{-\\frac{v+1}{2}}\\\\ \\\\ \\text{Where } &amp;\\alpha = v = \\text{degrees of freedom} \\end{aligned} \\] The Students t distributions offers a tractable heavy-tail model of the entire return distribution (not just the tail). It is typically adjusted for the location and scale: \\[ Y = \\mu + \\sigma X \\quad \\text{where } X \\sim t(df=v) \\] 3.4.2 Stable Distributions If we let log returns \\(r_{i} \\sim\\) heavy tail distribution with \\(0&lt;\\alpha&lt;2\\) (the first and second moment - mean/variance exist) The aggregate return \\(r_{1\\to n} = r_{1} = r_{2} + \\dots r_{n} \\sim\\) Stable Distribution 3.4.2.1 Generalized Central Limit Theorem Stable distributions have no closed form expression, although the share heavy tails and the same tail index \\(\\alpha\\) as it’s individual distributions. As you sum independent, STABLE random variables, the sum will follow a Stable Distribution. 3.4.2.2 Measuring tail behaviour We use the complementary CDF \\[ \\bar{F}(x) = 1 - F(x) = P(X &gt; x) \\sim x^{-\\alpha} \\] 3.5 Mixture Distributions We can generate a random variable using one of out a selection of a family of distributions, choosing the distribution using another distribution. They are easy to generate, but harder to work with analytically. We can select from a discrete and finite set of distributions, or of a continuous family of distributions (possibly countable) known as compound distributions. Example: \\[ \\begin{aligned} \\text{Generate RV from: } \\begin{cases} N(0,1) &amp; p=60\\%\\\\ N(5,3) &amp; p=40\\% \\end{cases} \\end{aligned} \\] 3.5.0.1 Normal scale mixture \\(Y = \\mu+\\sqrt{ V } \\cdot Z\\) Where \\(V\\) is a RV with non-negative mixing distribution and represents a random sd of \\(Y\\). Example (probably don’t need to memorize…) which is used when simulating returns, as seem in PS2 Q4c part ii and iii. \\[ \\begin{aligned} \\text{t-dist } t &amp;= Z\\sqrt{ v/W } \\quad \\text{ where } W \\sim \\chi^{2}(df=v)\\\\ \\text{GARCH model } r_{t} &amp;= \\mu + \\sigma_{t}Z_{t} \\quad \\text{ where } \\sigma^{2}_{t} = \\omega + \\sum^{p}_{i=1}a_{i}r^{2}_{t-i}+\\sum^{q}_{j=1}\\beta_{j}\\sigma^{2}_{t-j}\\\\ \\end{aligned} \\] "],["modelling-extreme-events.html", "Chapter 4 Modelling Extreme Events 4.1 Stylized Facts 4.2 Extreme Value Theory", " Chapter 4 Modelling Extreme Events 4.1 Stylized Facts Characteristics of typical empirical asset returns: Absence of simple autocorrelations Returns are not correlated with past time-steps of itself Volatility clustering Large amounts of volatility often occur in clusters through time Heavy tails Returns often have very abnormal (large) values suggesting heavy tail distributions Intermittency Aggregation changes distribution Gain/loss asymmetry The market tends to go up over time 4.2 Extreme Value Theory This theory helps with modelling extreme events that have small probabilities of occurring. 4.2.1 Two main results Maxima of i.i.d. sequences Values exceeding threshold 4.2.2 1st Theorem: Fisher-Tippet-Gnedenko Theres no need to prove any results If \\(X_{1}, X_{2}, \\dots\\) are i.i.d. RVs, we can create a RV that simply takes the maximum of \\(n\\) RVs called \\(M_{n} = max(X_{1}, X_{2}, \\dots X_{n})\\). In certain cases, we can find normalizing constants \\(a_{n}&gt;0, b_{n}\\) such that they can be transformed into one of the three distributions (identified by \\(H(x)\\)) which can be much easier to work with. \\[ \\begin{aligned} P\\left( \\frac{M_{n}-b_{n}}{a_{n}} \\leq x\\right) &amp;= [F(a_{n}x+b_{n})]^{n} \\to H(x)\\\\ F(a_{n}x+b_{n}) &amp;= \\text{Single RV} \\end{aligned} \\] This is just saying that the normalized max RV is less than some value \\(x\\) is just the probability of each individual RV being less than the transformed value of \\(x\\). \\[ \\begin{aligned} H(x) &amp;= \\begin{cases} Gumbel &amp; \\exp\\{-e^{-x}\\} \\quad\\quad x \\in \\mathbb{R}\\\\ Frechet &amp; \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\exp\\{-x^{-\\alpha}\\} &amp; x &gt;0 \\end{cases}\\\\ Weibull &amp; \\begin{cases} \\exp\\{-|x|^{\\alpha}\\} &amp; x&lt;0 \\\\ 1 &amp; x &gt; 0 \\end{cases} \\end{cases} \\end{aligned} \\] Where \\(\\alpha &gt; 0\\) for the Frechet and Weibull distributions. 4.2.3 Generalized Extreme Value (GEV) Distribution The three types of distributions can be represented using: \\[ \\begin{aligned} H(x) &amp;= \\exp\\left\\{ -\\left( 1+\\xi\\frac{x-\\mu}{\\sigma} \\right)^{-1/\\xi}_{+} \\right\\}\\\\ \\text{Where } \\mu &amp;= \\text{location}\\\\ \\sigma &amp;= \\text{scale}\\\\ \\xi &amp;= \\text{shape parameters} \\begin{cases} \\xi&gt;0 &amp; \\text{heavy tails (Frechet)} \\\\ \\xi=0 &amp; \\text{exponential tails (Gumbel)} \\\\ \\xi &lt; 0 &amp; \\text{short/light tails (Weibull)} \\end{cases} \\end{aligned} \\] Where the \\(\\xi\\) value describes the tail behaviour The log-transformation of the Frechet(\\(\\alpha=1\\)) is the Gumbel distribution! \\[ \\begin{aligned} \\ln(H_{F}(x)) &amp;= \\ln\\left[ \\exp\\{-x^{-1}\\}\\right]\\\\ &amp;= -\\left( 1+\\frac{x-\\mu}{\\sigma} \\right)^{-1}\\\\ \\\\ &amp;= H_{G} = \\exp \\left\\{ -\\left( 1+0\\cdot \\frac{x-\\mu}{\\sigma} \\right)^{-1/0} \\right\\} \\end{aligned} \\] As an example, we’ll show that the normalized maximum of i.i.d. Uniform(0,1) aka \\(F_{n}(x) = x\\) with \\(a_{n}=1/n, b_{n}=1\\) converges to the Weibull distribution. \\[ \\begin{aligned} P\\left( \\frac{M_{n}-b_{n}}{a_{n}} \\leq x\\right) &amp;= P\\left( \\frac{M_{n}-1}{1/n} \\leq x\\right)\\\\ &amp; b_{n}\\text{ shifts the Uniform from (0,1) to (-1,0)}\\\\ &amp;= P\\left( M_{n} \\leq \\frac{x}{n}+1 \\right)\\\\ &amp;= P\\left( \\max(U_{1}, U_{2}, \\dots U_{n}) \\leq 1+\\frac{x}{n} \\right)\\\\ &amp;= \\prod^{n}_{i=1} \\underbrace{ P\\left( U_{i}\\leq 1+\\frac{x}{n} \\right) }_{ \\left( 1+\\frac{x}{n} \\right) }\\\\ &amp;= \\left( 1+\\frac{x}{n} \\right)^{n}\\\\ &amp; \\text{because } x&lt;0, \\quad 1+ (-.4) = 1-|-.4|\\\\ &amp;=\\left( 1-\\frac{|x|}{n} \\right)^{n}\\\\ \\\\ \\lim_{ n \\to \\infty } \\left( 1-\\frac{|x|}{n} \\right)^{n} &amp;\\to \\exp\\{((-1)|x|)^{-1/-1}\\}\\\\ &amp;= \\exp\\{-|x|^{1}\\} \\implies Weibull(\\alpha=1)\\\\ \\end{aligned} \\] 4.2.4 2nd Theorem: Pickands-Balkema-De Haan For any RV \\(X\\) with CDF \\(F(\\cdot)\\), it’s conditional distribution when exceeding a certain threshold \\(u\\) is: \\[ \\begin{aligned} F_{u}(y) &amp;= \\frac{F(u+y)-F(u)}{1-F(u)}, \\quad 0\\leq y\\leq x_{F}-u\\\\ \\text{Where } x_{F} &amp;= sup\\{x \\in \\mathbb{R}:F(x) &lt; 1\\} \\text{ is the right endpoint of } F \\end{aligned} \\] \\(x_{F}\\) could be finite, or \\(\\infty\\). As \\(u\\to x_{F}\\), the conditional distribution converges to something belonging to the Generalized Pareto Distribution (GPD). \\[ F_{u}(y) \\to G_{\\xi, \\sigma}(y) \\] The Generalized Pareto Distribution (GPD) is given by: \\[ \\begin{aligned} G_{\\xi,\\sigma}(y) &amp;= 1 - \\left( 1+\\xi \\frac{y}{\\sigma} \\right)^{-1/\\xi}_{+} = \\begin{cases} G_{\\xi, \\sigma}(y) = 1-\\left( 1+\\xi \\frac{y}{\\sigma} \\right)^{-1/\\xi} &amp; \\xi \\ne 0\\\\ G_{\\xi, \\sigma}(y) = 1-\\exp\\left\\{ -\\frac{y}{\\sigma} \\right\\} &amp; \\xi = 0 \\end{cases}\\\\ \\text{Where } &amp; \\sigma &gt; 0, y\\geq 0, \\quad y\\leq-\\frac{\\sigma}{\\xi} \\text{ when } \\xi &lt; 0 \\end{aligned} \\] \\(f(x) = \\lambda e^{-\\lambda x}, \\lambda&gt;0\\) bottom=-1; left=-0.5; right=1.5; --- y=2 \\exp(-2x) x=.9|dashed The exponential distribution is used to model waiting times, no matter how much time has passed. The conditional distribution "],["multivariate-return-modelling.html", "Chapter 5 Multivariate Return Modelling 5.1 Covariance &amp; Correlation Matrix 5.2 Multivariate Student’s t Distribution", " Chapter 5 Multivariate Return Modelling When modelling returns of multiple assets, it’s common to assume the correlation between them is constant. Many investment strategies combine multiple assets together, but how those assets are related is an important piece of information to understand. It’s also difficult to model. Say we start with a simple multivariate Normal, where dependence \\(\\leftrightarrow\\) covariance, so we can model their relations with the covariance matrix. 5.1 Covariance &amp; Correlation Matrix For a linear combination of assets, we can attempt to use the following covariance matrix to describe their linear dependence: \\[ \\begin{aligned} Cov[A^{T}R] = A^{T}Cov[R]A, \\quad \\text{where }\\begin{cases} A = \\text{Constant matrix} \\\\ R = \\text{random vector} \\end{cases} \\end{aligned} \\] Unfortunately, sample covariance estimation is very sensitive to extreme values, which happen frequently with return (See heavy tails) Returns are often treated as independent samples, but that is not realistic. 5.2 Multivariate Student’s t Distribution We should never remove outliers in finance. We must model the heavy tails (extreme returns) somehow, and Student’s t distribution is one way to do that. A multivariate Normal scale mixture model can be used to describe a multivariate Student’s t distribution: \\[ \\begin{aligned} \\mathbf{R} &amp;= \\boldsymbol{\\mu} + \\mathbf{Z} \\sqrt{ v/W } \\sim t_{v}(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})\\\\ \\\\ \\text{Where } &amp; W \\sim \\chi^{2}(df=v)\\\\ &amp; \\mathbf{Z} \\sim N(\\mathbf{0}, \\boldsymbol{\\Lambda}) \\quad \\text{where } \\boldsymbol{\\Lambda} = Indentity \\ Mat \\end{aligned} \\] It’s mean and variance are: \\[ \\begin{aligned} \\mathbb{E}(\\mathbf{R}) &amp;= \\mathbb{E}\\left( \\boldsymbol{\\mu}+\\mathbf{Z}\\sqrt{ \\frac{v}{W} }\\right)\\\\ &amp;= \\boldsymbol{\\mu} + \\underbrace{ \\mathbb{E}(\\mathbf{Z}) }_{ =0 }\\cdot \\mathbb{E}\\left( \\sqrt{ \\frac{v}{W} } \\right)\\\\ &amp;=\\boldsymbol{\\mu}\\\\ \\\\ Cov(\\mathbf{R}) &amp;= Cov\\left( \\boldsymbol{\\mu} + \\mathbf{Z} \\sqrt{ \\frac{v}{W} } \\right)\\\\ &amp;= Cov(\\mathbf{Z}) \\cdot Cov\\left( \\sqrt{ \\frac{v}{W} } \\right)\\\\ &amp;= \\boldsymbol{\\Lambda}\\cdot \\frac{v}{v-2} \\quad \\text{where } v &gt; 2 \\end{aligned} \\] From these plots, we see that with the Multivariate t distribution, the extreme values between the two RVs are more evenly spread (the round shape of the points) and imply extreme values for both assets happen together. This is because they both share the same \\(\\Lambda\\). This shows that the multivariate t distribution is more practical and realistic than using the Normal to model multiple assets. We can linearly combine multivariate t-distributions with the same degrees of freedom: \\[ \\begin{aligned} \\mathbf{Y} &amp;\\sim t_{v}(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda}) \\implies \\mathbf{w}^{T}\\mathbf{Y} \\sim t_{v}(\\mathbf{w}^{T}\\boldsymbol{\\mu}, \\mathbf{w}^{T}\\boldsymbol{\\Lambda}\\mathbf{w})\\\\ \\\\ \\mathbb{E}[\\mathbf{w}^{T}\\mathbf{Y}] &amp;= \\mathbf{w}^{T}\\mathbb{E}(\\mathbf{Y})\\\\ Var[\\mathbf{w}^{T}\\mathbf{Y}] &amp;= \\mathbf{w}^{T}Var\\left( \\boldsymbol{\\mu}+\\mathbf{Z}\\sqrt{ \\frac{v}{W} } \\right)\\mathbf{w}\\\\ &amp;= \\mathbf{w}^{T}Var(\\mathbf{Z})Var\\left( \\sqrt{ \\frac{v}{W} } \\right)\\mathbf{w}\\\\ &amp;= \\frac{v}{v-2}(\\mathbf{w}^{T}\\boldsymbol{\\Lambda}\\mathbf{w}) \\end{aligned} \\] However, as seen above, the multivariate t distribution is restrictive because all the marginal distributions share the same degrees of freedom, as they all depend on the variance of \\(W \\sim\\chi^{2}\\). "],["copulas.html", "Chapter 6 Copulas 6.1 The independence copula 6.2 Sklar’s Theorem 6.3 Gaussian Copula 6.4 Creating Copula’s from Multivariate Distributions 6.5 Elliptical Copulas 6.6 Archimedean Copulas 6.7 Fitting Copula’s", " Chapter 6 Copulas A more flexible way of modelling dependencies of RVs is using copulas. Definition: A copula (C) is a multivariate CDF with Uniform(0,1) marginals \\[ \\begin{aligned} C(u_{1}, u_{2}, \\dots u_{d}) &amp;=P(u_{1}, u_{2}, \\dots u_{d}) \\in [0,1], \\quad \\forall \\ u_{1},\\dots u_{d} \\in [0,1]\\\\ \\\\ &amp;\\text{Where }\\begin{cases} C(0,0, \\ldots, 0)=0 \\\\ C(1,1, \\ldots, 1)=1 \\\\ C\\left(\\ldots, u_{i-1}, 0, u_{i+1}, \\ldots\\right)=0 \\\\ C\\left(1, \\ldots, 1, u_i, 1, \\ldots, 1\\right)=u_i \\end{cases} \\end{aligned} \\] Third case: The cumulative probability of one RV being less than or equal to 0 regardless of what the other RVs are in a copula is 0. Fourth case: The cumulative probability that the other RVs have values less than 1 is 1 but the \\(i^{th}\\) RV is \\(u_{i}\\) is simply \\(u_{i}\\) 6.1 The independence copula Definition: \\(C_{indep} (u_{1}, \\dots u_{d}) = u_{1} \\times \\dots \\times u_{d}\\) By the Frechet-Hoeffding theorem, any/every copula is bounded by \\[ \\begin{aligned} &amp; \\underline{C}\\left(u_1, \\ldots, u_d\\right) \\leq C\\left(u_1, \\ldots, u_d\\right) \\leq \\bar{C}\\left(u_1, \\ldots, u_d\\right) \\\\ &amp; \\text { where } \\begin{cases} \\underline{C}\\left(u_1, \\ldots, u_d\\right)=\\max \\left\\{1-d+\\sum_{i=1}^d u_i, 0\\right\\} = \\max\\left\\{ 1 - \\left(\\sum^d_{i=1} 1 - u_{i}\\right),0 \\right\\} \\\\ \\bar{C}\\left(u_1, \\ldots, u_d\\right)=\\min \\left\\{u_1, \\ldots, u_d\\right\\} \\end{cases} \\end{aligned} \\] \\(\\max \\left\\{1-d+\\sum_{i=1}^d u_i, 0\\right\\}\\) is 1 minus number of uniforms plus the values of the uniforms (if \\(\\mathbf{d=5}, u_{1}=.5,u_{2}=.4,u_{3}=.7,u_{4}=.2,u_{5}=.6\\) then \\(\\max(1-5+2.4 = \\mathbf{-1.6},0)\\) ) This implies the maximum of the Copula is bounded by 0, or higher if the average value of the uniforms are \\(\\geq\\frac{d-1}{d}\\). \\(\\min \\left\\{u_1, \\ldots, u_d\\right\\}\\) implies the max of the Copula is bounded by the smallest marginal probability, which makes sense as that is the only one limiting the cumulative probability. 6.2 Sklar’s Theorem Any continuous multivariate CDF \\(F(x_{1}, \\dots x_{d})\\) with marginal (1D) CDF’s \\(F_{i}(x_{i}) \\ \\forall \\ i=1,\\dots,d\\) can be expressed in terms of a copula \\(C\\), as \\[ F(x_{1},\\dots x_{d}) = C(F_{1}(x_{1}), \\dots, F_{d}(x_{d})) \\] Inverse is also true, where any copula combined with marginal CDF’s can give a multivariate CDF. Copula’s model dependence separately from the marginal distributions of the RVs If \\(X \\sim F \\implies F(x) \\sim Unif(0,1) \\implies F^{-1}(Unif) \\sim F\\) If an RV follows some CDF \\(F\\), then … If you take a copula of a bunch of marginal CDF’s, you can obtain the multivariate CDF of all the marginals. The inverse is true, where you can take a multivariate CDF and come up with a copula to represent the dependency between the marginal distributions, and the marginal distributions themselves. (I think is what this is saying.) 6.2.1 Example For a continuous CDF \\(F(x_{1},\\dots,x_{d})\\) with marginals \\(F_{i}(x_{i})\\), the copula is given by: \\[ \\begin{aligned} C(F_{1}(x_{1}),\\dots,F_{d}(F_{d})) &amp;= F(x_{1},\\dots,x_{d}) \\ \\text{by Sklar&#39;s Thm}\\\\ \\\\ \\text{Let } u_{i}= F_{i}(x_{i}) &amp;\\implies x_{i} = F_{i}^{-1}(u_{i})\\\\ \\end{aligned} \\] \\[ \\therefore \\mathbf{C(u_{1},\\dots,u_{d}) = F(F_{1}^{-1}(u_{1}),\\dots,F_{d}^{-1}(u_{d}))}\\\\ \\] 6.3 Gaussian Copula We can also construct copula’s of non uniform distributions, such as this multivariate Normal CDF: \\[ \\text{Let } \\mathbf{X}\\sim N_{d}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\quad \\text{with Correlation Mat} \\ \\boldsymbol{\\rho}\\\\ \\] We can find the copula \\(C_{p}\\) of \\(\\mathbf{X}\\) using: \\[ C_{p}(u_{1},\\dots u_{d}) = \\Phi_{\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}}(\\Phi_{\\mu_{1},\\sigma_{1}^{2}}^{-1}(u_{1}), \\dots, \\Phi_{u_{d},\\sigma_{d}^{2}}^{-1}(u_{d})) \\] For multivariate distributions that have a Gaussian Copula are called meta-Gaussian distributions. These distributions themselves do not need to be Gaussian. The second plot would be flat/uniform if the two independent distributions were not correlated (\\(\\rho=0\\)). This plot shows that the probability of both being \\(1\\) or \\(0\\) is very high, but the probability that one is \\(1\\) and the other is \\(0\\) is virtually 0. This supports our goal of modelling distributions where extreme values occur together. 6.4 Creating Copula’s from Multivariate Distributions We can create Copula’s from known multivariate Distributions such as the Normal or t distributions. We copy the dependence structure of known distributions (allowing us to use different marginals for modelling) Copula from multivariate Normal CDF with correlation matrix \\(\\boldsymbol{\\rho}\\) \\[ \\begin{aligned} C_{\\rho}(u_{1},\\dots ,u_{d}) &amp;= \\Phi_{\\boldsymbol{\\rho}}(\\Phi ^{-1}(u_{1}), \\dots, \\Phi ^{-1}(u_{d}))\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Where } &amp; \\begin{cases} \\Phi_{\\rho} \\text{ is multivariate Normal CDF with correlation } \\boldsymbol{\\rho} \\\\ \\\\ \\Phi \\text{ is Standard univariate Normal CDF} \\end{cases} \\end{aligned} \\] ## Simulating from a Copula Simulating from a distribution with a copula (dependence between marginals) and marginals themselves can be done with the following steps: Generate (dependent) uniforms from the copula: \\[ (U_{1},\\dots U_{d}) \\sim C \\] This is done by generating from a multivariate normal with correlation \\(\\boldsymbol{\\rho}\\) \\[ \\mathbf{Z} = \\left[ \\begin{array}{l} Z_{1} \\\\ \\vdots \\\\ Z_{d} \\end{array} \\right] \\sim N_{d}(0,\\boldsymbol{\\rho}) \\] Calculate uniforms as their marginal CDF’s (value of uniform var is a probability \\(P(Z \\leq Z_{i})\\)) \\[ U_{i}=\\Phi(Z_{i}), \\ i=1,\\dots,d \\] Then we can use the uniforms with any other marginal This is a very involved process, that we don’t need to go into Generate target variates from marginals, using the inverse CDF method \\[ X_{i} = F_{i}^{-1}(U_{i}) \\ \\forall \\ i \\] It’s difficult for simulating uniforms from multivariate copula. Simulating from just the 2D Multivariate normal with a correlation of 75% compared to simulating from the Gaussian Copula with the same correlation between variables. 6.4.1 Copula example plots We can use two different marginals (\\(\\chi^{2}\\) in this case), which are meta-Gaussian as they have a gaussian copula. Here we see the simulated copula values for different correlation values. We can also see the differences between a Copula created using Normal and t distributions The Normal copula shows \\(u_{1},u_{2}\\) are independent with 0 correlation as there is equal probability any value is sampled from \\(u_{1}\\) and any value of \\(u_{2}\\). On the right, the t copula shows that values on the borders (\\(u_{1}=0 \\mid\\mid u_{2} =0\\)) drop to probability 0, and at the extreme values, jump very high. Peaks at extreme combinations \\(\\to\\) manifestation of tail dependence. 6.5 Elliptical Copulas The Normal and t distributions have a specific type of dependence: elliptical dependence. The ellipses describe the contours of the multivariate Normal and t distributions: The ellipses are determined by the covariance matrix of the distributions. This implies a symmetry in the dependency structure, where the strength is the same for positively and negatively correlated variables. 6.6 Archimedean Copulas A Family of copula’s whose form is given by \\[ C(u_{1},\\dots,u_{d}) = \\phi ^{-1}[\\phi(u_{1})+\\dots+\\phi(u_{d})] \\] Where \\(\\phi\\): is a continuous convex generator function maps from \\([0,1] \\to [0,\\infty]\\) \\(\\phi(0)=\\infty, \\phi(1)=0\\) This is just an example of a possible \\(\\phi\\) There are infinitely many \\(\\phi\\) but some popular choices for \\(\\phi\\) are: Name Generator \\(\\phi(t)\\) Generator Inverse \\(\\phi ^{-1}(t)\\) Parameter Clayton \\(t^{-\\theta}-1\\) \\((1+s)^{-1/\\theta}\\) \\(\\theta\\geq 0\\) Frank \\(-\\ln \\frac{e^{-\\theta t}-1}{\\theta^{-\\theta}-1}\\) \\(-\\frac{1}{\\alpha}\\ln(1+e^{-s}(e^{-\\theta}-1))\\) \\(\\theta\\geq 0\\) Gumbel \\((-\\ln t)^{\\theta}\\) \\(\\exp\\{-s ^{-1/\\theta}\\}\\) \\(\\theta\\geq 1\\) \\(\\theta\\) seems to represent how extreme the function varies between its domain, with larger \\(\\theta\\)’s implying larger values throughout the domain. For example, the Clayton generator with \\(\\theta=0.25\\) in blue, and \\(\\theta=3\\) in red. bottom=-0.2;top=4; right=1.2;left=-0.2; --- y=1/x^{0.25}-1|0&lt;=x&lt;=1|blue y=1/x^{3}-1|0&lt;=x&lt;=1|red Samples from the Archimedean copula’s: The dependency contours of each type of archimedean copula’s base distribution covariance This plot shows that we can use Archimedean copula’s to model asymmetric dependencies, but suffer limitation in \\(\\geq\\) 3 dimensions. The Archimedean Copula’s value is constant for any permutation of coordinates \\(u_{1}, \\dots, u_{d}\\) \\[ \\begin{aligned} C(u_{1}, u_{2}, \\dots, u_{d}) &amp;= \\phi ^{-1}(\\phi(u_{1})+\\phi(u_{\\mathbf{2}})+\\dots+\\phi(u_{\\mathbf{d}}))\\\\ &amp;= \\phi ^{-1}(\\phi(u_{1})+\\phi(u_{\\mathbf{d}})+\\dots+\\phi(u_{\\mathbf{2}}))\\\\ \\end{aligned} \\] All pairs of coordinates(variables) have the same dependence, which is not the case for elliptical copulas. There exist copulas that can both model asymmetric dependencies, and differences in pairwise dependence called vine copula’s. 6.7 Fitting Copula’s Given a copula and marginal distributions, the MLE method can be applied to fit multivariate distribution parameters to sample data. This could however lead to a very high number of parameters. Instead, pseudo-MLE could be used to break down the problem into the marginals and copula. \\[ \\begin{aligned} U_{i}^{(j)} = \\hat{F}_{j}(X_{i}^{(j)}) \\quad \\forall \\ i, \\dots,n \\ j = 1, \\dots, d \\end{aligned} \\] Each \\(n\\) uniforms in \\(d\\) dimensions of the copula can be created from the actual multivariate distribution. Alternatively, the empirical CDF could be used to obtain the uniforms. We can then estimate the copula using MLE on the uniforms. Guess: The uniforms are created by plugging in each asset return into their own marginal CDF. The pseudo-MLE can give the uniform marginals used to construct/fit a t-copula. "],["portfolio-theory.html", "Chapter 7 Portfolio Theory 7.1 Assumptions 7.2 Dealing with Two Assets 7.3 Efficiency frontier 7.4 Multiple asset portfolio 7.5 CAPM - Capital Asset Pricing Model", " Chapter 7 Portfolio Theory Portfolio theory deals with how an asset manager can form a portfolio that optimizes their goals, whether that be lowest risk, highest return, or some other measure of performance. How to pick stocks is a complicated science and there are many ways to go about it. 7.1 Assumptions We first make some assumptions when dealing with the theory. - Static multivariate return distribution determined by assets’ mean and covariance. - This implies a normal or elliptical distribution for returns - The investors have the same views on mean and variance - Investors also want minimum risk for maximum return - Investors measure risk by portfolio’s variance - No borrowing or short-selling restrictions - No transaction costs 7.2 Dealing with Two Assets We’ll start with a simple example with two risky assets, \\(S_{1}, S_{2}\\) We assume that net returns (as time \\(0\\to t\\) ) satisfy: \\[ \\left[\\begin{array}{c} R_{1} \\\\ R_{2} \\end{array}\\right] \\sim N \\left( \\left[ \\begin{array}{c} \\mu_{1} \\\\ \\mu_{2} \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{1}^{2} &amp; \\sigma_{12} \\\\ \\sigma_{21} &amp; \\sigma^{2}_{2} \\end{array} \\right] \\right) \\quad \\text{where } R_{i} = \\frac{S_{i}(t)-S_{i}(0)}{S_{i}(0)}, i = 1,2 \\] We can form a portfolio with \\(x_{i}\\) units of asset \\(S_{i}\\), which gives us the following equation: \\[ \\begin{aligned} V(t=0) &amp;= x_{1}\\cdot S_{1}(0)+x_{2}\\cdot S_{2}(0)\\\\ \\end{aligned} \\] We can derive the initial weights of each asset that we’ve invested into using: \\[ w_{i} = \\frac{x_{i}S_{i}(0)}{V(0)} \\] 7.2.1 Portfolio Return We can calculate the return of this two asset portfolio and show that: \\(R_{p} = w_{1}R_{1}+w_{2}R_{2}\\) \\[ \\begin{aligned} R_{p} &amp;= \\frac{V(t)-V(0)}{V(0)} = \\frac{[x_{1}S_{1}(t)+x_{2}S_{2}(t)]-[x_{1}S_{1}(0)+x_{2}S_{2}(0)]}{V(0)}\\\\ &amp;= \\frac{x_{1}[S_{1}(t)-S_{1}(0)]+x_{2}[S_{2}(t)-S_{2}(0)]}{V(0)}\\\\ &amp;= x_{1} \\underset{ \\text{Introduce }S_{1}(0) \\text{ to get} R_{1}}{ \\frac{S_{1}(t)-S_{1}(0)}{S_{1}(0)} \\frac{S_{1}(0)}{V(0)} } + x_{2} \\underset{ \\text{Introduce }S_{2}(0) \\text{ to get} R_{2} }{ \\frac{S_{2}(t)-S_{2}(0)}{S_{2}(0)} \\frac{S_{2}(0)}{V(0)} }\\\\ &amp;= R_{1} \\underbrace{ x_{1}\\frac{S_{1}(0)}{V(0)} }_{ w_{1} }+R_{2} x_{2}\\underbrace{ \\frac{S_{2}(0)}{V(0)} }_{ w_{2} }\\\\ &amp;= R_{1}w_{1}+R_{2}w_{2} \\end{aligned} \\] Which shows that the net returns of a portfolio is exactly the weight combination of the net returns of the assets within. We can also find the distribution of the portfolio returns (which may not surprise you, is just the linear combination of the individual assets returns) \\[ \\begin{aligned} R_{p} &amp;= \\underline{w}^{T}\\underline{R} = [ w_{1} \\ w_{2}] \\left[ \\begin{array}{c} R_{1} \\\\ R_{2} \\end{array} \\right] \\quad \\text{ where } \\underline{R} \\sim N_{2D}\\left( \\left[ \\begin{array} {l} \\mu_{1} \\\\ \\mu_{2} \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{1}^{2} &amp; \\sigma_{12} \\\\ \\sigma_{21} &amp; \\sigma_{2}^{2} \\end{array}\\right] \\right)\\\\ &amp;\\implies R_{p} \\sim N_{1D}(\\mu_{p},\\sigma_{p}) \\ \\text{ where}\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\mu_{p} &amp;= \\mathbb{E}(R_{p}) = \\mathbb{\\mathbf{E}}[\\underline{w}^{T}\\underline{R}]\\\\ &amp;= \\underline{w}^{T}\\mathbb{E}[\\underline{R}] = \\underline{w}^{T}\\underline{\\mu}\\\\ &amp;= \\sum_{i}w_{i}\\mu_{i}\\\\ \\\\ \\sigma_{p}^{2} &amp;= \\mathbb{V}[R_{p}] = \\mathbb{V}[\\underline{w}^{T}\\underline{R}]\\\\ &amp;= \\underline{w}^{T} \\mathbb{V}(\\underline{R})\\underline{w} = \\underline{w}^{T}\\Sigma \\underline{w}\\\\ &amp;= [ w_{1} \\ w_{2}] \\left[ \\begin{array}{cc} \\sigma_{1}^{2} &amp; \\sigma_{12} \\\\ \\sigma_{21} &amp; \\sigma_{2}^{2} \\end{array}\\right] \\left[ \\begin{array}{c} w_{1} \\\\ w_{2} \\end{array} \\right] \\\\ &amp;= w_{1}^{2}\\sigma_{1}^{2}+2w_{1}w_{2}\\sigma_{12}+w_{2}^{2}\\sigma_{2}^{2}\\\\ \\\\ \\therefore R_{p} &amp;\\sim N\\left( \\sum_{i}w_{i}p_{i}, \\quad w_{1}^{2}\\sigma_{1}^{2}+2w_{1}w_{2}\\sigma_{12}+w_{2}^{2}\\sigma_{2}^{2}\\right) \\end{aligned} \\] 7.3 Efficiency frontier A plot of mean \\(\\mu\\) by standard deviation \\(\\sigma\\) , where you can load up entirely on AAPL with \\(w_{1}=1, w_{2}=0\\) or entirely on PNC with the opposite. But how do we know the optimal amount of weight to put in each asset to minimize variance? We can simply find the \\(w\\) that minimizes the derivative of the variance w.r.t. \\(w\\) \\[ \\begin{aligned} \\text{Let }w_{1} &amp;= w \\quad \\&amp; \\ w_{2}=1-w\\\\ \\implies \\sigma_{p}^{2} &amp;= w^{2}\\sigma_{1}^{2} + 2w(1-w)\\sigma_{12} + (1-w)^{2}\\sigma_{2}^{2}\\\\ \\text{Let }\\frac{\\delta \\sigma_{p}^{2}}{\\delta w} &amp;= 0\\\\ 0 &amp;= 2w\\sigma_{1}^{2}+2(1-w)\\sigma_{12}-2w\\sigma_{12}-2(1-w)\\sigma_{2}^{2}\\\\ 0 &amp;= w(\\sigma_{1}^{2}-\\sigma_{12}) + (1-w)(\\sigma_{12}-\\sigma_{2}^{2})\\\\ 0 &amp;= w(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\sigma_{12}) + (\\sigma_{12}-\\sigma_{2}^{2})\\\\ w &amp;= \\frac{-(\\sigma_{12}-\\sigma_{2}^{2})}{(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\sigma_{12})} \\end{aligned} \\] 7.4 Multiple asset portfolio Consider \\(n\\) risky assets with returns \\(R_1, \\ldots, R_n\\) so that: \\[ \\mathbf{R}=\\left[\\begin{array}{c} R_1 \\\\ \\vdots \\\\ R_n \\end{array}\\right] \\sim N\\left(\\boldsymbol{\\mu}=\\left[\\begin{array}{c} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{array}\\right], \\boldsymbol{\\Sigma}=\\left[\\begin{array}{ccc} \\sigma_1^2 &amp; \\ldots &amp; \\sigma_{1 n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{n 1} &amp; \\cdots &amp; \\sigma_n^2 \\end{array}\\right]\\right) \\] We want to create portfolio using weights \\(\\mathbf{w}=\\left[\\begin{array}{lll}w_1 &amp; \\cdots &amp; w_n\\end{array}\\right]^{\\top}\\) such that: \\(\\sum_{i=1}^n w_i=\\mathbf{w}^{\\top} \\mathbf{1}=1\\) The distribution of returns for this portfolio would simply be: \\[ \\begin{aligned} \\mathbf{R_{p}} &amp;\\sim \\mathbf{N_{1D}(\\mu_{p}, \\sigma_{p}^{2})} \\ \\text{where }\\\\ &amp;\\mathbf{\\mu_{p} = \\underline{w}^{T}\\cdot \\underline{\\mu}}\\\\ &amp;\\mathbf{\\sigma_{p}^{2}=\\underline{w}^{T}\\cdot\\underline{\\Sigma}\\cdot\\underline{w}} \\end{aligned} \\] When fixing an expected return \\(\\mu_{p}\\), we can derive weights that minimize variance: \\[ min_{w}\\{\\mathbf{w^{T}\\Sigma w}\\}\\ s.t. \\ \\mathbf{w^{T}\\mu} = \\mu_{p}, \\mathbf{w^{T}1} = 1 \\] We can use Lagrange multipliers to define the objective function: Lagrange formula: \\(\\mathcal{L}(x,\\lambda) = f(x)+\\lambda g(x)\\) If we want to find the max/min of the function \\(f(x)\\) subject to the constraint \\(g(x) = 0\\), we can form this Lagrangian and find where it’s derivative is 0. In this case, \\(x=\\underline{w}\\) and our constraint \\(\\mathbf{w^{T}\\cdot 1} = 1\\) can be represented as \\(g(w) = \\mathbf{w^{T}\\cdot 1} -1\\) \\[ \\begin{aligned} \\mathcal{L}(\\underline{w},\\lambda) &amp;= \\underline{w}^{T}\\underline{\\Sigma}\\underline{w} - \\lambda(\\underline{w}^{T}\\cdot \\underline{1}-1)\\\\ \\text{Let }\\frac{\\delta\\mathcal{L}}{\\delta \\underline{w}}&amp;= 0\\\\ 0 &amp;= 2\\cdot\\underline{\\Sigma}\\cdot \\underline{w}-\\lambda\\cdot \\underline{1} \\\\ \\implies \\underline{w} &amp;= \\frac{\\lambda}{2}\\underline{\\Sigma}^{-1}\\cdot \\underline{1} \\end{aligned} \\] \\[ \\begin{aligned} \\text{Given } \\underline{w}^{T}\\cdot \\underline{1} &amp;= 1 \\implies \\underline{1}^{T}\\underline{w} = 1\\\\ \\\\ \\implies&amp; \\frac{\\lambda}{2}\\cdot \\underline{1}^{T}\\cdot\\underline{\\Sigma}^{-1}\\cdot \\underline{1} = \\underline{1}^{T}\\underline{w} = 1\\\\ \\implies&amp; \\lambda = \\frac{2}{\\underline{1}^{T}\\cdot\\underline{\\Sigma}^{-1}\\cdot \\underline{1}}\\\\ \\implies &amp;\\underline{w}^{*} = \\frac{\\frac{2}{\\underline{1}^{T}\\cdot\\underline{\\Sigma}^{-1}\\cdot \\underline{1}}}{2}\\underline{\\Sigma}^{-1}\\cdot \\underline{1}\\\\ &amp; \\quad \\ = \\frac{\\underline{\\Sigma}^{-1}\\cdot \\underline{1}}{\\underline{1}^{T}\\cdot\\underline{\\Sigma}^{-1}\\cdot \\underline{1}} \\end{aligned} \\] \\(\\underline{\\Sigma}^{-1}\\) represents the precision matrix, where if there is a 0 at \\((i,j)\\) then \\(i\\) and \\(j\\) are conditionally independent (given all other assets are held constant, there is no dependence/correlation between these two assets). If it exists, then it is positive definite. The numerator is a vector containing the L1 norms of each row’s (asset) precision. The denominator represents the total precision of the entire Precision matrix. The final value is the scaled weights given to each asset depending on the total precision. Instead of only having \\(n\\) risky assets, we can introduce a risk-free asset with constant return \\(R_{f}=\\mu_{f}&gt;0\\), with variance \\(\\sigma_{f} =0\\). A real life example is a government bond. With this new asset, we can determine how much to invest in the risky portfolio and the risk-free asset with weights \\(w_{p}\\) and \\((1-w_{p})\\). Return: \\(R=w_{p}R_{p}+(1-w_{p})R_{f}\\) Mean Return: \\(\\mathbb{E}[R]=w_{p}\\mathbb{E}[R_{p}]+(1-w_{p})\\mathbb{E}[R_{f}]=w_{p}\\mu_{p}+(1-w_{p})\\mu_{f}\\) Variance of returns: \\(\\mathbb{V}=\\mathbb{V}(w_{p}R_{p}+(1-w_{p}R_{f})) = w_{p}^{2}\\mathbb{V}(R_{p})=w_{p}^{2}\\cdot \\sigma_{p}^{2}\\) The best portfolio (in terms of risk and return) lie on this green line, where this line is tangent to the efficient frontier. The slope of this line is called the Sharpe ratio, which measure the excess return per unit risk (risk premium). The tangency portfolio is exactly the portfolio on the efficient frontier that also belongs to the tangent line. (It is a risky asset only portfolio with \\(\\mu_{T},\\sigma_{T}\\)) When you have a risk-free asset, the optimal investment strategy is to have a combination of the tangency portfolio and the risk-free asset. To find this tangency portfolio, we maximize the Sharpe ratio. \\[ max\\left\\{ \\frac{\\mu_{p}-\\mu_{f}}{\\sigma_{p}} \\right\\} = max_{w} \\left\\{ \\frac{\\mathbf{w^{T}\\mu}-\\mu_{f}}{\\sqrt{ \\mathbf{w^{T}\\Sigma w} }} \\right\\} \\quad \\text{subject to } \\mathbf{w^{T}1} = 1 \\] The tangency portfolio weights are given by: \\[ \\mathbf{w_{t}} = \\frac{\\mathbf{\\Sigma ^{-1}(\\mu}-\\mu_{f}\\cdot \\mathbf{\\underline{1}})}{\\mathbf{1^{T}\\Sigma ^{-1}(\\mu}-\\mu_{f}\\cdot \\mathbf{\\underline{1}})} \\] 7.5 CAPM - Capital Asset Pricing Model The CAPM theory suggests that all investors hold some form of the tangency/market portfolio. This model assumes the mean-variance analysis stretched to it’s logical consequences can explain all financial market behaviour.(Widely known to be false) The slope of the tangency portfolio is also known as the Sharpe Ratio, or Market Price of Risk. If every investor followed this mean-variance analysis, and the market is in equilibrium: &gt;idk what it means to be in equilibrium Every investor holds some portion of the same tangency portfolio The entire financial market will be made up of the same mix of risky assets Making the tangency portfolio equal to the market value-weighted index 7.5.1 Market Portfolio Using the above assumptions, the composition of the tangency portfolio is equivalent to the market and we can find the weights for each asset as: \\[ w_{i} = \\underbrace{ \\frac{S_{i}\\times O_{i}}{\\sum^{N}_{i=1}S_{i}\\times O_{i}} }_{ \\text{Market Capitalization \\%} } \\quad \\text{where } \\begin{cases} S_{i} = \\text{price of asset i} \\\\ O_{i} = \\text{shares outstanding} \\end{cases} \\] Therefore the tangency portfolio is exactly a market cap weighted index, such as the S&amp;P500 or FTSE100. 7.5.2 Capital Market Line Every mean-variance efficient portfolio \\((\\mu_{p},\\sigma_{p})\\) lies on a straight line, which is determined by \\[ \\mu_{p} = \\mu_{f} + \\frac{\\mu_{M}-\\mu_{f}}{\\sigma_{M}}, \\quad \\text{where }\\begin{cases} (\\mu_{M},\\sigma_{M}) = \\text{market portfolio} \\\\ \\mu_{f} = \\text{risk-free rate} \\end{cases} \\] 7.5.3 Security Market Line For each individual asset, CAPM implies the following risk/reward relationship (Security Market Line) \\[ \\begin{aligned} \\mu_{i} &amp;= \\mu_{f} + \\beta_{i}\\underbrace{ (\\mu_{M}-\\mu_{f}) }_{ Slope } \\quad \\text{Where } \\begin{cases} \\beta_{i} = \\frac{\\sigma_{iM}}{\\sigma^{2}_{M}} \\\\ \\sigma_{iM} = Cov(R_{i, R_{M}}) \\end{cases}\\\\ &amp;= \\mu_{f} + \\left( \\frac{\\mu_{M}-\\mu_{f}}{\\sigma_{M}} \\right) \\cdot \\sigma_{p} \\end{aligned} \\] \\(\\beta_{i}\\) is different for each asset, it captures how related the return on the given asset is with the Market return. \\(\\sigma_{M}\\) is the market risk AKA undiversifiable risk Imagine the line is in \\((\\mu,\\beta)\\)-space, with slope \\((\\mu_{M}-\\mu_{f})\\). To derive this line, we need to find a market portfolio that maximizes the Sharpe ratio (using first order derivative) \\[ \\begin{aligned} f(\\underline{w}) &amp;= \\frac{\\underline{w}^{T}\\underline{\\mu}-\\mu_{f}}{\\sqrt{ \\underline{w}^{T}\\underline{\\Sigma}\\underline{w} }} = \\frac{\\underline{w}^{T}\\cdot (\\underline{\\mu}-\\underline{1}\\mu_{f})}{\\sqrt{ \\underline{w}^{T}\\underline{\\Sigma}\\underline{w} }} \\quad \\text{maximized when } \\frac{\\delta f}{d\\underline{w}} = 0\\\\ \\implies 0 &amp;= \\frac{(\\underline{\\mu}-\\underline{1}\\mu_{f})\\cdot\\sqrt{ \\underline{w}^{T}\\underline{\\Sigma}\\underline{w} } - \\frac{1}{2}{(\\overbrace{ \\underline{w}^{T}\\underline{\\Sigma}\\underline{w}}^{ \\sigma_{m}^{2} }})^{-1/2}\\cdot 2\\underline{\\Sigma}\\underline{w}\\cdot (\\overbrace{ \\underline{w}^{T}\\cdot (\\underline{\\mu} }^{ \\mu_{M} }-\\underline{1}\\mu_{f})}{\\underline{w}^{T}\\underline{\\Sigma}\\underline{w}}\\\\ &amp;= \\frac{(\\underline{\\mu}-\\underline{1}\\mu_{f})\\sigma_{m}-\\frac{1}{\\sigma_{M}}\\underline{\\Sigma}\\underline{w}(\\mu_{M}-\\mu_{f})}{\\sigma_{M}^{2}}\\\\ \\implies (\\underline{\\mu}-\\underline{1}\\mu_{f}) &amp;= \\frac{1}{\\sigma_{M}^{2}\\underline{\\Sigma}\\underline{w}(\\mu_{M}-\\mu_{f})}\\\\ \\implies \\left[ \\begin{array}{c} \\mu_{1}-\\mu_{f} \\\\ \\vdots \\\\ \\mu_{n}-\\mu_{f} \\end{array} \\right] &amp;= \\frac{\\mu_{M}-\\mu_{f}}{\\sigma_{M}^{2}}\\cdot \\left[ \\begin{array}{c} Cov(R_{1},R_{M}) \\\\ \\vdots \\\\ Cov(R_{n},R_{M}) \\end{array} \\right] \\\\ &amp;= \\frac{(\\mu_{M}-\\mu_{f})}{\\sigma_{M}^{2}}\\cdot \\left[ \\begin{array}{c} \\sigma_{1,M} \\\\ \\vdots \\\\ \\sigma_{n,M} \\end{array} \\right] \\end{aligned} \\] This is because \\(Cov(\\underline{R}, R_{M}) = Cov(\\underline{R},\\underline{w}^{T}\\cdot \\underline{R}) = \\underline{w}^{T}\\underbrace{ Cov(\\underline{R},\\underline{R}) }_{ \\underline{\\Sigma} } = \\underline{w}^{T}\\underline{\\Sigma}\\) 7.5.3.1 Ex: Find market portfolio weights and SML with \\(N\\) i.i.d assets Each asset \\(\\sim N(\\mu,\\sigma^{2})\\) returns and the risk-free return \\(\\mu_{f}&lt;\\mu\\). \\[ \\left[ \\begin{array}{c} R_{1} \\\\ \\vdots \\\\ R_{N} \\end{array} \\right] \\sim N_{N \\ Dim}(\\underline{\\mu} = \\mu \\cdot \\underline{1}, \\underline{\\Sigma} = \\sigma^{2}\\cdot \\underline{I}) \\] So we know: \\[ \\forall \\ \\underline{w} \\quad s.t. \\ \\underline{w}^{T}\\cdot \\underline{1} = 1 \\implies \\underline{w}^{T}\\cdot\\mu\\cdot 1 = \\mu \\] Which implies that the market portfolio is equal to the minimum variance portfolio. \\[ \\begin{aligned} \\underline{w}^{\\star} &amp;= \\frac{\\underline{\\Sigma}^{-1}\\cdot \\underline{1}}{\\underline{1}^{T}\\underline{\\Sigma}^{-1}\\cdot \\underline{1}} = \\frac{\\frac{1}{\\sigma^{2}}\\underline{I}\\cdot \\underline{1}}{\\frac{1}{\\sigma^{2}} \\underline{1}^{T}\\underline{I}\\cdot \\underline{1}} = \\frac{\\underline{1}}{N}\\\\ \\implies \\underline{w} &amp;= \\frac{\\underline{1}}{N} = \\left[ \\begin{array}{c} \\frac{1}{N} \\\\ \\vdots \\\\ \\frac{1}{N} \\end{array} \\right] \\implies w_{i} = \\frac{1}{N} \\ \\forall i = 1,\\dots,N\\\\ \\implies \\text{min variance } &amp;= \\underline{w}^{T}\\underline{\\Sigma}\\underline{w}\\\\ &amp;= \\left( \\frac{1}{N} \\right)^{2}\\cdot \\underline{1}^{T}(\\sigma^{2}\\cdot \\underline{I})\\cdot \\underline{1}\\\\ &amp;= \\frac{\\sigma^{2}}{N} \\end{aligned} \\] 7.5.4 Security Characteristic Line The \\(\\beta_{i}\\)’s are found empirically, by regressing (\\(R_{i}-R_{f}\\)) on (\\(R_{M}-R_{f}\\)) - Where \\(R_{M}\\) is the market return (Proxy by large market index, ex S&amp;P500) - \\(R_{f}\\) is the risk free rate (proxy by T-bill) \\[ (R_{i,t} - R_{f,t}) = \\beta_{i}(R_{M,t}-R_{f,t})+\\epsilon_{t}, \\quad \\text{ where } \\epsilon_{t} \\sim N(0,\\sigma^{2}_{\\epsilon,i}) \\] By fitting this, we can then extract the \\(\\beta_{i}\\)’s! However the mean expected return is much different than the actual return. Therefore the CAPM model doesn’t work very well, or hold. An iteration of this model includes an intercept, \\(\\alpha\\) \\[ (R_{i,t} - R_{f,t}) = \\alpha_{i} + \\beta_{i}(R_{M,t}-R_{f,t})+\\epsilon_{t} \\] Finding the mean and variance: \\[ \\begin{aligned} \\mu_{i} = \\mathbb{E}[R_{i}] &amp;= \\mathbb{E}[R_{f,t}+\\alpha_{i}+\\beta_{i}(R_{M,t}-R_{f,t})+\\epsilon_{t}]\\\\ &amp;= \\underbrace{ \\mathbb{E}[R_{f}] }_{ R_{f} } + \\alpha_{i} + \\beta_{i}\\underbrace{ \\mathbb{E}[R_{m}-R_{f}] }_{ \\mu_{m - R_{f}} } + \\underbrace{ \\mathbb{E}[\\epsilon_{t}] }_{ =0 }\\\\ &amp;= R_{f} + \\alpha_{i} + \\beta_{i}(\\mu_{m}-R_{f})\\\\ \\\\ \\sigma^{2}_{i} = \\mathbb{V}[R_{i}] &amp;= \\mathbb{V}[\\underbrace{ R_{f}+\\alpha_{i} }_{ Var = 0 }+\\beta_{i}(R_{m}-R_{f})+\\epsilon_{i})]\\\\ &amp;= \\mathbb{V}[\\beta_{i}(R_{m}-\\underbrace{ R_{f} }_{ Var = 0 }] + \\mathbb{V}[\\epsilon_{i}]\\\\ &amp;= \\beta_{i}^{2} \\cdot \\mathbb{V}[R_{M}] + \\sigma^{2}_{\\epsilon, i}\\\\ &amp;= \\beta^{2}_{i}\\cdot \\sigma^{2}_{m}+\\sigma^{2}_{\\epsilon,i} \\end{aligned} \\] Time doesn’t matter for the risk free or market returns 7.5.4.1 Alpha &amp; Beta An asset’s beta \\(\\beta_{i}\\) can be seen as a measure of both risk &amp; reward \\[ CAPM \\to \\mu_{f} = \\mu_{f} + \\beta_{i}(\\mu_{M}-\\mu_{f}) \\implies \\mu_{i} = \\mu_{f} + \\beta_{i}\\underbrace{ \\left(\\frac{\\mu_{M}-\\mu_{f}}{\\sigma_{M}} \\right)}_{ \\text{Sharpe Ratio} }\\sigma_{M} \\] \\(\\beta_{i}\\) measures extent of the return of a given asset is related to the market If you believe in CAPM, then the bigger beta is, the more risk you will have but also the more reward you will have. \\(\\alpha_{i}\\) measure how much the asset outperforms the market consistently, the amount returned on top of the \\(\\beta_{i}\\) 7.5.5 Legacy of CAPM CAPM is wrong. A lot of it’s assumptions do not hold in reality. However, it had immense practical impact on investing, specifically in terms of - Diversification: concept of decreasing risk by spreading portfolio over different assets - Index investing: justification for common investing strategy of tracking some broad index with mutual funds or ETF’s - Benchmarking: Measuring performance of investment relative to market / index 7.5.6 Performance Evaluation CAPM says the best portfolio you can create is the tangency/market portfolio. This implies the best you can do is get the broadest index and combine it with a T-bill. Aside: In the past, you needed to use a mutual fund because exchange traded funds didn’t exist. Thats how most people invested, even though they cost high fees as there was a lot of human involvement. There are several ways to measure an asset’s performance, based on CAPM Sharpe ratio: (excess return per unit risk) \\[ S_i=\\frac{\\mu_i-\\mu_f}{\\sigma_i} \\] If you combine different assets, you can reduce the \\(\\sigma_{i}\\) of the portfolio, so it’s not objective because two assets may be really risky, but negatively correlated reducing \\(\\sigma_{i}\\). It doesn’t however measure how correlated a portfolio is to the market. Treynor index: (excess return per unit non-diversifiable risk) \\[ \\quad T_i=\\frac{\\mu_i-\\mu_f}{\\beta_i} \\] You would look at this to find the best return Jensen’s alpha: (excess return on top of the return explained by the market) \\[ \\quad \\alpha_i=\\hat{\\alpha}_i \\] Usually the most important measure a portfolio manager tries to use to convince people to invest in them. "],["factor-models.html", "Chapter 8 Factor Models 8.1 Three Factor models 8.2 Factor Model Assumptions 8.3 Time Series Regression Models 8.4 Statistical Factor Models 8.5 Principle Component Analysis (PCA) 8.6 Factor Analysis", " Chapter 8 Factor Models 8.1 Three Factor models The CAPM model assumes the market is a single factor that drives asset returns. - In practice, CAPM does not adequately describe real-world returns We can improve this model by including more factors in the regression. There are three types of factor models we will look at: Macroeconomic: Factors are observable economic and financial time series data (eg return of the S&amp;P 500) Fundamental: Created explicitly or implicitly from observable asset characteristics Statistical (Latent variable model): Factors are unobservable and extracted from asset returns All three types follow some form of \\[ \\begin{aligned} R_{i}(t) = \\beta_{i,0} + \\beta_{i,1}F_{1}(t)+\\dots+\\beta_{i,p}F_{p}(t)+\\epsilon_{i}(t), \\quad \\forall \\begin{cases} i=1,\\dots, N \\\\ t \\in \\mathbb{R} \\end{cases} \\end{aligned} \\] where: \\(R_{i}(t)\\) is return on the \\(i^{th}\\) asset at time t \\(F_{j}(t)\\) is the \\(j^{th}\\) common factor at time t \\(\\beta_{i,j}\\) is the factor loading/beta of \\(i^{th}\\) asset on the \\(j^{th}\\) factor \\(\\epsilon_{i}(t)\\) is the idiosyncratic/unique return of asset \\(i^{th}\\) We want the errors to be independent from the factors, \\[ Cov[\\epsilon(t), F(t)] = 0 \\] In matrix form, \\[ \\begin{aligned} &amp; \\Leftrightarrow \\\\ &amp; {\\left[\\begin{array}{c} R_1(t) \\\\ \\vdots \\\\ R_N(t) \\end{array}\\right] }=\\left[\\begin{array}{c} \\beta_{1,0} \\\\ \\vdots \\\\ \\beta_{N, 0} \\end{array}\\right]+\\left[\\begin{array}{ccc} \\beta_{1,1} &amp; \\cdots &amp; \\beta_{1, p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\beta_{N, 1} &amp; \\cdots &amp; \\beta_{N, p} \\end{array}\\right]\\left[\\begin{array}{c} F_1(t) \\\\ \\vdots \\\\ F_p(t) \\end{array}\\right]+\\left[\\begin{array}{c} \\varepsilon_1(t) \\\\ \\vdots \\\\ \\varepsilon_p(t) \\end{array}\\right] \\\\ &amp; \\Leftrightarrow \\\\ &amp; \\mathbf{R}(t)=\\boldsymbol{\\beta}_0+\\boldsymbol{\\beta}^{\\top} \\mathbf{F}(t)+\\boldsymbol{\\varepsilon}(t) \\end{aligned} \\] The factors \\(F_{j}(t)\\) are stationary with moments: \\[ \\begin{aligned} R(t) &amp;= \\beta_{0} + \\beta^{T}F(t) + \\epsilon(t)\\\\ \\\\ \\mu_{r} &amp;= \\mathbb{E}(R(t)) = \\mathbb{E}[\\beta_{0} + \\beta^{T}F(t) + \\epsilon(t)]\\\\ &amp;= \\beta_{0}+\\beta^{T}\\underbrace{ \\mathbb{E}[F(t)] }_{ \\mu_{F} } + \\underbrace{ \\mathbb{E}[\\epsilon(t)] }_{ 0 }\\\\ &amp;= \\beta_{0} + \\beta^{T}\\mu_{F}\\\\ \\\\ \\Sigma_{R} &amp;= \\mathbb{V}[\\beta_{0}+\\beta^{T}F(t) +\\epsilon(t)]\\\\ &amp;\\underbrace{= \\mathbb{V}[\\beta^{T}F(t)] + \\mathbb{V}[\\epsilon(t)]}_{ \\text{As } F(t)\\text{ indep of } \\epsilon_(t)}\\\\ &amp;= \\beta^{T}\\underbrace{ \\mathbb{V}[F(t)] }_{ \\Sigma_{F} }\\beta+\\Sigma_{\\epsilon}\\\\ &amp;= \\beta^{T}\\Sigma_{F}\\beta+\\underbrace{ \\Sigma_{\\epsilon} }_{ \\text{Diagonal} } \\end{aligned} \\] We can also find the moments of the portfolio with weights \\(w = [w_{1}, \\dots w_{N}]^{T}\\) \\[ \\begin{aligned} R_{port} = w^{T}R \\implies \\begin{cases} \\mu_{port} = \\mathbb{E}[R_{port}] = w^{T}\\mathbb{E}[R] = w^{T}(\\beta_{0}+\\beta_{\\mu_{F}}) \\\\ \\sigma^{2}_{port} = \\mathbb{V}[R_{port} = \\mathbb{V}[w^{T}R] = w^{T}\\mathbb{V}[R]w = w^{T}(\\beta^{T}\\Sigma_{F}\\beta+\\Sigma_{\\epsilon})w \\end{cases} \\end{aligned} \\] 8.2 Factor Model Assumptions Factors \\(F_j(t)\\) are stationary, with moments: \\[ E[\\mathbf{F}(t)]=\\boldsymbol{\\mu}_F \\quad \\&amp; \\quad \\operatorname{Var}[\\mathbf{F}(t)]=\\boldsymbol{\\Sigma}_F \\] Asset-specific errors \\(\\varepsilon_i(t)\\) are uncorrelated with common factors: \\[ \\operatorname{Cov}[\\mathbf{\\varepsilon}(t), \\mathbf{F}(t)]=\\mathbf{0} \\] Errors are serially &amp; contemporaneously uncorrelated across assets \\[ \\operatorname{Var}[\\boldsymbol{\\varepsilon}(t)]=\\operatorname{diag}\\left[\\left\\{\\sigma_{\\varepsilon_i}^2\\right\\}_{i=1, \\ldots, N}\\right]=\\boldsymbol{\\Sigma}_{\\varepsilon} \\quad \\&amp; \\operatorname{Cov}[\\boldsymbol{\\varepsilon}(t), \\boldsymbol{\\varepsilon}(s)]=\\mathbf{0} \\] 8.3 Time Series Regression Models Consider model for which factor values are known (e.g. macro/fundamental model) Estimate betas &amp; risks (variances) one asset at a time, using time series regression For each fixed \\(i=1,\\dots N\\), fit regression model: \\[ R_{i}(t) = = \\beta_{i,0} + \\beta_{i,1}F_{1}(t)+ \\dots + \\beta_{i,p}F_{p}(t) + \\epsilon_{i}(t) \\] Most models will always include some proxy for the overall economy (eg the market) Instead of Fama-French, there is BARRA which uses the cross section of returns? Still regression tho 8.3.1 Fama-French 3 Factor Model Additionally to the market, they looked at two other factors that are consistent over many datasets and time periods. Small Minus Big: (SMB) One factor tries to capture the size(market cap) of the company/stock, which empirically showed that size played a role in its return Regressed returns on how companies of a certain size/group did They took the smallest and biggest companies, and looked at their avg return over some period, creating a difference to use to separate the groups of companies High Minus Low (HML): The second factor looks at whether a company is a value one or not. Measured using book-to-market ratio. These two factors were found to have statistically significant coefficients in multiple regression. People frequently use the factor model to estimate the return covariance matrix. They can use the sample covariance matrix of the factors and apply the beta vector in order to obtain the return covariance matrix \\[ \\operatorname{Var}(\\mathbf{R})=\\hat{\\boldsymbol{\\Sigma}}_R=\\hat{\\boldsymbol{\\beta}}^{\\top} \\hat{\\boldsymbol{\\Sigma}}_F \\hat{\\boldsymbol{\\beta}}+\\hat{\\boldsymbol{\\Sigma}}_{\\varepsilon} \\] where: \\[ \\begin{aligned} &amp; \\hat{\\boldsymbol{\\beta}}=\\text { beta coefficient matrix (from regressions) } \\\\ &amp; \\hat{\\boldsymbol{\\Sigma}}_F=\\text { factor sample covariance matrix } \\\\ &amp; \\hat{\\boldsymbol{\\Sigma}}_{\\varepsilon}=\\text { diagonal error variance matrix (from residuals) } \\end{aligned} \\] This gives more stable estimates than sample covariance. 8.4 Statistical Factor Models In this model, the factors are unknown (latent) and unobserved. This implies that we need to estimate both \\(\\beta\\) and \\(F\\). Unusual constraint: Because the factors are uncorrelated (not necessarily i.i.d.) \\[ \\Sigma_{F} = Cov(F)= I \\sigma_F \\quad \\&amp; \\quad \\mu_{F}=\\mathbb{E}(F) = 0 \\] This gives us the resulting moments of the returns: \\[ \\begin{aligned} \\mu_{R} &amp;= \\mathbb{E}[R] = \\mathbb{E}[\\beta_{0}+\\beta^TF + \\epsilon] = \\beta_{0}+\\beta^{T}\\underbrace{ \\mathbb{E}[F] }_{ 0 }+\\underbrace{ \\mathbb{E}[\\epsilon] }_{ 0 } = \\beta_{0}\\\\ \\\\ \\Sigma_{R} &amp;= \\mathbb{V}[R] = \\mathbb{V}[\\boldsymbol{\\beta}_{0}+\\boldsymbol{\\beta}^{T}F+\\boldsymbol{\\epsilon}] = \\boldsymbol{\\beta}^{T}\\mathbb{V}[F]\\boldsymbol{\\beta}+\\boldsymbol{\\Sigma_{\\epsilon} = \\beta^{T}(I\\cdot \\sigma_{F})\\beta+\\Sigma_{\\epsilon}} \\end{aligned} \\] 8.5 Principle Component Analysis (PCA) This technique helps reduce the dimensionality of a problem. Consider a factor model without errors: \\[ \\mathbf{R}(t) = \\boldsymbol{\\beta}_{0}+\\boldsymbol{\\beta}^{T}\\mathbf{F}(t) \\implies \\boldsymbol{\\Sigma_{R}=\\beta^{T}\\Sigma_{F}\\beta} \\] Given a set of N assets, we can construct a set of variables (components) capturing most of the variability. PCA is a linear transformation The goal is to capture as much variation as possible. When we apply PCA to the statistical model, we find \\(n\\) factors set as the PCA components. These components are uncorrelated, and have maximum variance. \\[ F_{1},\\dots,F_{n} \\text{ are factors which are the Principle Components} \\] \\[ \\begin{aligned} F_{1} &amp;= \\gamma_{1}^{T}\\mathbf{R} = \\gamma_{11}R_{1}+\\dots+\\gamma_{1n}R_{n}\\\\ \\vdots\\\\ F_{n} &amp;= \\gamma_{n}^{T}\\mathbf{R} = \\gamma_{n1}R_{1}+\\dots+\\gamma_{nn}R_{n}\\\\ \\end{aligned} \\] Where each factor: \\[ \\begin{aligned} F_{i} &amp;= \\gamma_{i}^{T}\\mathbf{X} \\quad \\text{Maximizes } \\ Var(F_{i})=\\boldsymbol{\\gamma_{i}^{T}\\Sigma_{R}\\gamma_{i}} \\quad s.t. \\boldsymbol{\\gamma_{i}^{T}\\gamma_{i}}=1\\\\ \\end{aligned} \\] \\[ \\begin{aligned} Cov(F_{i},F_{j}) &amp;= \\boldsymbol{\\gamma_{j}^{T}\\Sigma_{R}\\gamma_{i}} = 0\\\\ \\\\ \\text{Where }&amp; i,j\\in 1,\\dots,n, \\quad i &gt; j \\end{aligned} \\] Supposedly does not imply that \\(\\gamma_{j}^{T}\\gamma_{i} = 0\\) which may mean they are not orthogonal? But PCA components should be orthogonal? We can find the \\(Cov(F)\\) and beta (loading factor) of \\(R_i\\) on \\(F_j\\): \\[ \\begin{aligned} \\mathbb{V}(\\mathbf{F})&amp;= \\mathbb{V}(\\mathbf{P}^{T}\\mathbf{R})\\\\ &amp;= \\mathbf{P}^{T}\\mathbb{V}(\\mathbf{R})\\mathbf{P}\\\\ &amp;= \\mathbf{P}^{T}\\boldsymbol{\\Sigma}_{R}\\mathbf{P}\\\\ &amp;\\text{By Eigen-decomposition}\\\\ &amp;= \\mathbf{P^{T}(P\\Lambda P^{T})P}\\\\ &amp;\\text{Where } \\mathbf{P^{T}P} = 1\\\\ &amp;= \\boldsymbol{\\Lambda} \\end{aligned} \\] It’s kind of circular logic. Also, \\(\\Lambda\\) is sorted with the eigenvalue associated with the eigenvector that captures the most variance is first. Fact: Total variance of all PC’s is equal to original variable variance. \\[ \\begin{aligned} \\text{ PC Total Variance = Population Total Variance } &amp;\\iff\\\\ tr(\\Lambda) = tr(\\Sigma) &amp;\\iff\\\\ \\lambda_{1}+\\dots+\\lambda_{P} = \\sigma_{1}^{2}+\\dots+\\sigma^{2}_{N} \\end{aligned} \\] The proportion of total variance that is explained by each PC is: \\[ Var(PC_{j}) =\\frac{\\lambda_{j}}{\\lambda_{1}+\\lambda_{2}+\\dots+\\lambda_{N}} \\quad \\text{Where } j=1,\\dots,N \\] 8.5.1 Selecting Components The important part of PCA is how many components to select to capture a suitable amount of variability. You don’t need many components if the data is very correlated, and a few components may represent most of the variance in the data. We can also use a scree plot and pick the number of components before some “elbow” point. PCA run on the correlation matrix (which is the standardized covariance matrix, which is preferred when there is very large range of variances between variables) When we look at the first component, we can see it is almost an equal weighting of all the assets. We can use PCA to identify components that explain overall variation of the data, although it is not guaranteed to give meaning components. For a proper data-generating model, we must use Factor Analysis. 8.6 Factor Analysis He didn’t go into much detail, just went over how the factor analysis works at a high level and how you can’t really try to interpret the factors. If the variance of the factors is simply the identity matrix, the variance of the returns is just the cross products of the factor variables plus some error variance. "],["risk-measures.html", "Chapter 9 Risk Measures 9.1 Formulas 9.2 Value At Risk (VAR) 9.3 VaR hides tail risk 9.4 Risk Measure Properties 9.5 Entropic VaR 9.6 Calculating Risk Measures", " Chapter 9 Risk Measures We will primarily focus on market risk. Financial risk describes the possibility of an investment losing money. It can have several sources, the main ones being: Market risk: due to changes in market prices Credit risk: counterparty doesn’t honor obligations Liquidity risk: lack of asset tradability Operational risk: from organization’s internal activities E.g., legal, fraud, or human error risk Risk management is the process of identifying, measuring, and controlling risk The first thing we want to do is quantify the risk somehow. To operationalize risk management, we use risk measures to define some capital requirement that the organization should hold in a liquid asset/cash. The first thing that comes to mind is measuring risk by the variance of returns. But it is not a good risk measure. In the following example, the variances are the same, but clearly the risk is different. WIth the left distribution, the long tail goes to the left which means your returns could become large negative values. (Reasonable to assume it is “more risky”) For the right distribution, the average return is negative although there is no chance of very large negative returns. (Implies that it is “less risky”) The point of risk mgmt: We want to know the amount of money we should set aside to cover expected losses. 9.1 Formulas Assuming \\(Port \\sim N(\\mu, \\sigma^{2})\\) \\[ \\begin{aligned} VaR_{\\alpha} &amp;= \\mu +z_{\\alpha}\\cdot \\sigma\\\\ ES_{\\alpha} &amp;= \\mu + \\frac{\\phi(z_{\\alpha})}{\\alpha} \\cdot \\sigma\\\\ EVaR_{\\alpha} &amp;= \\mu + \\sigma \\cdot \\sqrt{-2\\ln(\\alpha)} \\end{aligned} \\] 9.2 Value At Risk (VAR) Was really common, although recently it has decreased in use (discredited) although there are variations still in use. Let \\(\\mathrm{RV}\\) \\(L\\) be the loss of an investment over some time period \\(T\\) Loss is negative of gain/revenue \\((R): L=-R\\) We look at losses because we only capture the negative losses from this asset Value at Risk at confidence level \\((1-\\alpha)\\) &amp; time horizon \\(T\\) is defined as the: \\((1-\\alpha)\\)-quantile of \\(L\\) for some \\(\\alpha \\in(0,1)\\) \\[ \\begin{aligned} \\operatorname{VaR}(\\alpha)=\\inf \\{x: P(L \\leq x) \\geq 1-\\alpha\\}=\\inf \\{x: P(L&gt;x) \\leq \\alpha\\} \\end{aligned} \\] Infimum (smallest value of x) is only used for discrete distributions such that the the probability of \\(L\\) will not exceed some value \\(x\\) and we want this to happen \\((1-\\alpha)\\) of the time. (Usually want it to be less than \\(5\\%\\)) For continuous RV with \\(\\operatorname{CDF} F_L\\) , we don’t need the infimum anymore and can just do: \\(\\Rightarrow \\operatorname{VaR}(\\alpha)=F_L^{-1}(1-\\alpha)\\) Basically the quantile of the distribution VaR represents amount that covers losses with probability \\((1-\\alpha)\\) We want the quantile at which the probability is at \\((1-\\alpha)\\) The VaR(\\(\\alpha\\)) value indicates what value of \\(x\\) we need to hold to cover our losses \\((1-\\alpha) = 95\\%\\) of the time. 9.2.1 Example Consider asset with \\(N(\\mu=.03, \\sigma^{2}=.04)\\) annual log-returns. Find the 95% confidence level annual VaR for a \\(S_{0} =\\)$1000 investment in this asset. Let \\(R=S_{T}-S_{0}\\) represent revenue (-losses) We want to find the VaR(\\(\\alpha\\)) s.t \\[ \\begin{aligned} P(L &gt; VaR) &amp;= 5\\%\\\\ &amp;= P(-R &gt; VaR) \\\\ &amp;= P(R &lt; -VaR)\\\\ \\\\ X &amp;\\sim N(0.03, \\sigma^{2}=0.04)\\\\ \\implies S_{T} &amp;= S_{0}\\cdot e^{X}\\\\ \\\\ P(S_{T}-S_{0} &lt; -VaR) &amp;= P(S_{T} &lt; S_{0}-VaR)\\\\ &amp;= P(S_{0}e^{X} &lt; S_{0}\\cdot VaR)\\\\ &amp;= P\\left( X &lt; \\log\\left( \\frac{S_{0}-VaR}{S_{0}} \\right)\\right)\\\\ &amp;= P\\left( \\underbrace{ \\frac{X-0.03}{.2} }_{ Z\\sim N(0,1) } &lt; \\frac{\\log\\left( 1-\\frac{VaR}{S_{0}} \\right)-0.03}{.2} \\right) = 0.05\\\\ &amp;= P\\left( Z &lt; \\underbrace{ \\frac{\\log\\left( 1-\\frac{VaR}{S_{0}} \\right)-0.03}{.2} }_{ z } \\right) = 0.05\\\\ \\implies z &amp;= -1.645\\\\ \\frac{\\log\\left( 1-\\frac{VaR}{S_{0}} \\right)-0.03}{.2} &amp;= -1.645\\\\ \\\\ \\mathbf{VaR} &amp;= (1-\\exp\\{-1.645\\cdot 0.20+0.03\\}) \\cdot S_{0}\\\\ \\mathbf{VaR} &amp;= (1-\\exp\\{-0.299\\})\\cdot 1000\\\\ &amp;= \\mathbf{258.44} \\end{aligned} \\] VaR is basic and widely used as a risk measure to simply indicate “What is the most I can lose on this investment?” It has been widely used partly because of the Basel framework which is some risk management standard for banks and banks implemented Basel to comply. However it has some glaring issues. No one trusts it as a proper risk management tool. Flaws: - VaR hides tail risk (hides risk of very extreme events) although there is a way to remedy it - It discourages diversification The additional resources talk about why VaR is not a good measure of risk: [[../../Lectures/Risk Mismanagement.pdf]] [[../../Lectures/Against VaR.pdf]] 9.3 VaR hides tail risk Var doesn’t care what happens beyond the cut off, just the actual z-score (or whatever score from some distribution). If we push some density of losses very far (hiding a small probability of really bad events), we can actually concentrate the density of profits higher. 9.3.0.1 Mitigation: Cond VaR or Expected Shortfall AKA CVar or ES This gives us the expected losses that can occur given that we are beyond \\(VaR_{\\alpha}\\) Imagine we cut off the loss distribution beyond the VaR(\\(\\alpha\\))\\(=x\\) point, and we have another little distribution of losses which are way larger/further from the bulk of the distribution. We take the mean/average/expected value of this distribution which tells us “hey in the 5% scenario our losses are so huge, what can we expect to lose?” 9.3.0.2 Example If \\(R\\sim N(0,1)\\), find ES at confidence level \\(\\alpha\\) \\[ \\begin{aligned} R\\sim N(0,1) &amp;\\implies L \\sim N(0,1)\\\\ \\text{Let } Z_{\\alpha} &amp;\\text{denote the } \\alpha \\text{-quantile of the std normal}\\\\ \\\\ ES_{\\alpha} &amp;= \\mathbb{E}[L|L&gt;Z_{\\alpha}] = \\int ^{\\infty}_{Z_{\\alpha}} x \\ \\phi(x| L&gt; z_{\\alpha}) \\, dx\\\\ &amp;= \\int ^{\\infty}_{Z_{\\alpha}} x \\frac{\\phi(x)}{\\underbrace{ P(L &gt; Z_{\\alpha}) }_{ \\alpha }}\\, dx \\\\ &amp;= \\frac{1}{\\alpha}\\int ^{\\infty}_{Z_{\\alpha}} x \\frac{1}{2\\sqrt{ \\pi }}e^{-x^{2}/2}\\, dx \\\\ &amp;= \\frac{1}{\\alpha} \\int ^{\\infty}_{Z_{\\alpha}}\\frac{1}{2\\pi} \\lfloor-e^{-x^{2}/2}\\rfloor \\, dx \\\\ &amp;= \\frac{1}{\\alpha} \\frac{1}{2\\pi} \\left[-e^{-x^{2}/2}\\right]^{\\infty}_{x=z_{\\alpha}}\\\\ \\implies ES_{\\alpha} &amp;= CVaR_{\\alpha} = \\frac{1}{\\alpha}\\frac{1}{\\sqrt{ 2\\pi }}e^{-z_{\\alpha}^{2}/2}\\\\ &amp;= \\frac{1}{\\alpha} \\phi(z_{\\alpha}) \\end{aligned} \\] This means that generally for \\(L\\sim N(\\mu, \\sigma^{2})\\), \\[ \\begin{aligned} ES_{\\alpha} &amp;= \\mu + \\frac{\\phi(z_{\\alpha})}{\\alpha} \\cdot \\sigma\\\\ VaR_{\\alpha} &amp;= \\mu +z_{\\alpha}\\cdot \\sigma \\end{aligned} \\] and for small \\(\\alpha\\), \\(\\frac{\\phi(z_{\\alpha})}{\\alpha} &gt; z_{a}\\) which implies \\(ES_{\\alpha}\\geq VaR_{\\alpha}\\) 9.4 Risk Measure Properties We now look at formal set of requirements for risk measures (5 properties) We let \\(\\rho(L)\\) represent the risk measure, which takes in a distribution of a some returns and outputs a number indicating the capital requirements (money to be kept aside) should be. We let \\(L\\) is a random variable and let \\(\\rho\\) is a functional distribution of \\(L\\). For this measure \\(\\rho\\) to reasonably quantify risk, it must: be normalized \\(\\rho(0)=0\\) (risk of holding no assets is 0) Translation invariance: \\(\\rho(L+c) = \\rho(L) + c \\ \\forall c \\in \\mathbb{R}\\) adding a loss \\(c\\) to the portfolio increases risk by exactly \\(c\\) Positive Homogeneity: \\(\\rho(bL) = b\\rho(l)\\) scaling portfolio returns also will scale risk Monotonicity: \\(L_{1}\\geq L_{2} \\implies \\rho(L_{1}) \\geq \\rho(L_{2})\\) The ordering of the random variables is almost surely \\(P(L_{1}\\geq L_{2})=1\\) Sub-additivity: \\(\\rho(L_{1}+L_{2}) \\leq \\rho(L_{1}+L_{2})\\) Only when the two losses are perfectly correlated, then equal The risk of two combined portfolios cannot exceed the sum of the two portfolio risks If a risk measure has all 5 properties, then it is called a coherent risk measure. 9.4.1 Example that VAR and CVar satisfy prop 2 &amp; 3 We first show that VAR and CVaR satisfy the second and third property of translation invariance and positive homogeneity. \\[ \\begin{aligned} VaR_{\\alpha}(L) &amp;= inf\\{x:P(L&gt;x) \\leq \\alpha\\}\\\\ \\\\ \\text{Let}\\ \\ L&#39; &amp;= bL+c\\\\ \\implies VaR(L&#39;) &amp;= inf\\{x&#39;: P(L&#39; &gt;x&#39;) \\leq \\alpha\\}\\\\ &amp;= inf\\{x&#39;:\\mathbb{P}(bL+c &gt; x&#39;) \\leq \\alpha\\}\\\\ &amp;= inf\\left\\{ x&#39;: \\mathbb{P}\\left( L&gt; \\frac{x&#39;-c}{b} \\leq \\alpha \\right) \\right\\}\\\\ &amp;= inf\\{bx+c : \\mathbb{P}(L &gt; x) \\leq \\alpha\\}\\\\ &amp;=b\\cdot inf\\{x: \\mathbb{P}(L &gt; x) \\leq \\alpha\\}+c\\\\ &amp;= b VaR(L)+c \\end{aligned} \\] \\[ \\begin{aligned} CVaR_{\\alpha}(L) &amp;= \\frac{1}{\\alpha}\\int ^{\\alpha}_{0} VaR_{u}(L) \\, du\\\\ \\implies CVaR_{\\alpha}(L&#39;) &amp;= \\frac{1}{\\alpha}\\int ^{\\alpha}_{0}(bVaR_{u}(L)+c) \\, du\\\\ &amp;= b\\left( \\frac{1}{\\alpha} \\int ^{\\alpha}_{0}VaR_{u}(L) \\, du\\right) +c\\\\ &amp;= b \\cdot CVaR_{\\alpha}(L)+c \\end{aligned} \\] We will show that VaR is not sub-additive Consider two risky zero-coupon bonds priced at $95 per $100 face value. If each one has 4% independent default probability, show that \\(VaR_{5\\%}\\) is not sub-additive \\[ \\begin{aligned} \\text{Distribution of }L_{1/2} &amp;= \\begin{cases} -5 &amp; w.p. 96\\% \\\\ +95 &amp; w.p. 4\\% \\end{cases}\\\\ \\text{By definitioin, VaR} &amp; = inf\\{x: \\mathbb{P}(L &gt;x)&lt;5\\%\\}\\\\ &amp;= inf(x: \\mathbb{P}(L \\leq x)\\geq 95\\%)\\\\ \\implies VaR_{5\\%}(L_{1/2}) &amp;= -5 \\\\ \\end{aligned} \\] As -5 is the smallest value that satisfies \\(\\mathbb{P}(L\\leq x) \\geq 95\\%\\) as \\(\\mathbb{P}(L \\leq -5) = 96\\%\\) \\[ \\begin{aligned} \\text{Distribution of }L_{1} + L_{2} &amp;= \\begin{cases} -5-5 = -10 \\quad w.p. 96\\%^{2}=92.16\\% &amp; \\text{when both don&#39;t default}\\\\ -5+95 = 90 \\quad w.p. 96\\% \\cdot 4\\% = 7.68\\% &amp; \\text{when only one defaults} \\\\ 95+95=190 \\quad w.p. 4\\%^{2}=.16\\% &amp; \\text{both default} \\end{cases}\\\\ VaR_{5\\%} (L_{1}+L_{2}) &amp;= inf\\{x:\\mathbb{P}(L_{1}+L_{2} &lt; x) \\geq 95\\%\\} = 90 \\end{aligned} \\] \\[ \\begin{aligned} \\implies VaR_{5\\%}(L_{1}+L_{2}) = 90 &gt; VaR_{5\\%}(L_{1}) + VaR_{5\\%}(L_{2}) = -5 -5 = 10 \\end{aligned} \\] Which suggests holding both bonds is more risky than holding them separately. We can show that CVaR is sub-additive. \\[ \\begin{aligned} CVaR_{5\\%}(L_{1}) &amp;= \\frac{1}{\\alpha}\\int ^{\\alpha}_{0}VaR_{u}(L_{1}) \\, du\\\\ &amp;= \\frac{1}{5\\%} (95\\cdot 4\\% + (-5)\\cdot 1\\%) = 73\\%\\\\ CVaR_{5\\%}(L_{1}+L_{2}) &amp;= \\frac{1}{5\\%}\\int ^{5\\%}_{0}VaR_{u}(L_{1}+L_{2}) \\, du\\\\ &amp;= \\frac{1}{5\\%}[190\\cdot.16\\%+90\\cdot 4.84\\%] = 93.2\\\\ \\\\ \\implies CVaR_{5\\%}(L_{1}+L_{2}) &amp;= 93.2 \\leq CVaR_{5\\%}(L_{1})+CVaR_{5\\%}(L_{2})\\\\ &amp;= 2\\cdot 73 = 146 \\end{aligned} \\] 9.5 Entropic VaR A coherent alternative to VaR based on the Chernoff bound for tail probability For positive RV \\(X\\), the Chernoff inequality gives: \\[ P(X\\geq c) \\leq \\frac{\\mathbb{E}(x)}{c} \\quad \\forall c &gt; 0 \\] For loss RV \\(L\\) with mgf \\(M_L(z)=E\\left[e^{z L}\\right]&lt;\\infty, \\forall z&gt;0\\) we have \\(P(L \\geq c) \\Leftrightarrow P\\left(e^{z L} \\geq e^{z c}\\right) \\leq M_L(z) e^{-z c}\\) Limiting bound to \\(M_L(z) e^{-z c} \\leq \\alpha\\) and solving for \\(c\\) we get \\(c=z^{-1} \\ln \\left(M_L(z) / \\alpha\\right)\\) EVaR defined as: \\(\\mathrm{EVaR}_\\alpha=\\inf _{z&gt;0}\\left\\{z^{-1} \\ln \\left(M_L(z) / \\alpha\\right)\\right\\}\\) Can define a risk measure we can use the MGF instead of the PDF or CDF which in some cases is convenient. We can just minimize the MGF 9.5.1 EVaR for a Normal Distribution \\[ \\begin{aligned} \\text{Let } L &amp;\\sim N(\\mu, \\sigma^{2})\\\\ M_{L}(z) &amp;= e^{\\mu z+1/2 \\sigma^{2}z^{2}} = \\mathbb{E}[e^{z\\cdot L}]\\quad \\forall z\\\\ \\\\ \\text{Know: }EVaR_{\\alpha} &amp;= inf_{z&gt;0} \\left\\{ z^{-1} \\ln(M_{L}(z) / \\alpha) \\right\\}\\\\ \\text{Let } f(z) &amp;= z^{-1} \\ln(M_{L}(z) / \\alpha)\\\\ &amp;= z^{-1} \\ln(e^{\\mu z+1/2 \\sigma^{2}z^{2}} / \\alpha)\\\\ &amp;= \\frac{1}{z}\\left[ \\mu z+\\frac{\\sigma^{2}z^{2}}{2}-\\ln \\alpha \\right]\\\\ &amp;= \\mu + \\frac{z\\sigma^{2}}{2} - \\frac{\\ln(\\alpha)}{z}\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{So to find }&amp; \\text{the smallest $z$, we take the derivative:}\\\\ \\implies f&#39;(z) &amp;= \\frac{\\sigma^{2}}{2}+\\frac{\\ln(\\alpha)}{z^{2}}\\\\ \\implies z &amp;= \\frac{\\sqrt{-2\\ln(\\alpha)}}{\\sigma}\\\\ \\\\ \\implies EVaR_{\\alpha}(L) &amp;= \\mu + \\frac{\\sigma^{2}}{2}\\cdot \\frac{\\sqrt{-2\\ln(\\alpha)}}{\\sigma} - \\frac{\\ln(\\alpha)}{\\frac{\\sqrt{-2\\ln(\\alpha)}}{\\sigma}}\\\\ &amp;= \\mu + \\sigma \\cdot \\sqrt{-\\frac{\\ln(\\alpha)}{2}} + \\sigma \\frac{\\sqrt{(-\\ln(\\alpha))^{2} }}{\\sqrt{-2\\ln(\\alpha)}}\\\\ &amp;= \\mu + \\sigma \\cdot \\sqrt{-\\frac{\\ln(\\alpha)}{2}} + \\sigma \\underbrace{ \\frac{\\sqrt{(\\ln(\\alpha ^{-1}))^{2} }}{\\sqrt{2\\ln(\\alpha ^{-1})}} }_{= \\sqrt{ \\frac{\\ln(\\alpha ^{-1})}{2} } = \\sqrt{ -\\frac{\\ln(\\alpha)}{2} }}\\\\ &amp;= \\mu + 2\\sigma \\cdot \\sqrt{-\\frac{\\ln(\\alpha)}{2}}\\\\ &amp;= \\mu + \\sigma \\cdot \\sqrt{-2\\ln(\\alpha)}\\\\ \\end{aligned} \\] 9.6 Calculating Risk Measures Parametric Modelling Not used because theres usually no closed form distribution for the losses Historical Simulation Not favoured because past performance does not predict future performance Monte Carlo Simulation 85% of large banks use historical simulation, while the remaining use MC simulation Usually done by sampling from some estimated parametric model for individual assets Other ways to test are stress-testing (the worst case scenario) and use the Extreme Value Theorem Rest of the slides were not gone over in detail, just showed how the CVaR performed better than VaR (gave lower z-score -&gt; allocation more money for risk purposes), how parametric modelling can’t really be done, how time series models are independent over time but can correlate between assets and show volatility clustering, how RiskMetrics is a time series model but didnt get into it, and the fact GARCH models are used. "],["betting-strategies.html", "Chapter 10 Betting Strategies 10.1 Problem Setup 10.2 Kelly Criterion", " Chapter 10 Betting Strategies There are betting/gambling games where a player has an edge, which just says your expected payoff \\(&gt;0\\). Consider roulette, where the “house” has an edge: - P(win) = 1/37 - odds: 35:1 &gt;In this scenario, the house has a slight edge as the odds don’t directly reflect the actual probability. Consider blackjack, in the 60’s, Ed Thorp, a mathematician figured out how to beat the house. Unfortunately, casino’s have strategies against card counting, like using multiple decks, burning cards, dealing only some of a deck of cards, etc. Given a sequence of bets, we can use a kelly criterion to determine how to bet optimally. 10.1 Problem Setup Consider a sequence of independent and identical gambles. \\(P(win) = \\frac{1}{2}&lt;p&lt;1 \\implies P(lose) = q = 1-p\\) For each $1 bet: - Get $1 if you win - Lose $1 if you lose. By starting with an initial wealth \\(V_{0}\\) what is the best strategy for placing bets? 10.1.1 Scenario 1: Bet fixed amount Bet a fixed amount \\(X\\) at every step. We ignore ruin (\\(V_{t}\\leq 0\\) for some \\(t&gt;0\\)) for now The expected wealth \\(V_{n}\\) after \\(n\\) steps: \\[ \\begin{aligned} \\text{Let } I_{i} &amp;= \\begin{cases} 1 &amp; \\text{w.p. } p \\\\ 0 &amp; o/w \\end{cases}\\\\ V_{1} &amp;= V_{0}+(2I_{1}-1)X\\\\ V_{2} &amp;= V_{1}+(2I_{2}-1)X\\\\ &amp;= V_{0}+(2I_{1}-1)X+(2I_{2}-1)X\\\\ &amp;\\vdots\\\\ V_{n} &amp;= V_{0} + X\\left( 2\\underbrace{ \\sum^{n}_{i=1}I_{i} }_{ \\sim \\text{Binomial(n,p)} }-n \\right)\\\\ \\implies \\mathbb{E}(V_{n}) &amp;= \\mathbb{E}\\left( V_{0}+X\\left( 2\\sum^{n}_{i=1}I_{i}-n \\right) \\right)\\\\ &amp;= V_{0}+X\\left( 2\\mathbb{E}\\left( \\sum^{n}_{i=1}I_{i} \\right) -n\\right)\\\\ &amp;= V_{0}+X(2np-n)\\\\ &amp;= V_{0}+nX(2p-1)\\\\ &amp;\\text{Since } \\frac{1}{2}\\leq p&lt;1 \\implies 2p-1 &gt;0 \\end{aligned} \\] With this probability and bet \\(X\\) per step, then we can expect to have some positive wealth at the \\(n^{th}\\) step which increases linearly, and the variance increases quadratically, but the probability of ruin is very small as \\(p&gt;\\frac{1}{2}\\) We didn’t consider the probability of ruin, as if \\(V_{0}\\) is very large and \\(X\\) is very small, the probability of ruin is very small. 10.1.1.1 Hidden probability of ruin By betting $1 at each step with \\(V_{0}\\) starting wealth, the probability of eventual ruin at some time step \\(n\\) is: \\[ \\begin{aligned} \\text{Let } \\pi_{i} &amp;= \\mathbb{P}(\\text{eventual ruin for }V_{0}=i) \\quad \\forall\\ i\\geq 1 \\\\ \\text{Let } \\pi_{0} &amp;= 1 \\quad \\text{if you start with no money, you are already ruined}\\\\ \\\\ \\implies \\pi_{i} &amp;= \\pi_{i+1}\\cdot p + \\pi_{i-1}q \\quad \\forall i \\geq 1 \\end{aligned} \\] Which is a second order linear recurrence relation. This can solved like a differential equation. We will assume a solution of the form: \\(\\pi_{i} = y^{i}\\) \\[ \\begin{aligned} \\implies y^{i} &amp;= py^{i+1}+qy^{i-1}\\\\ y &amp;= py^{2}+q\\\\ =&gt; py^{2}-y+q &amp;= 0\\\\ \\end{aligned} \\] Which leaves a quadratic equation: \\[ \\begin{aligned} y &amp;= \\frac{-b\\pm \\sqrt{ b^{2} -4ac}}{2a}\\\\ &amp;= \\frac{-(-1)\\pm \\sqrt{ (-1)^{2}-4pq }}{2p}\\\\ &amp;= \\frac{1 \\pm \\sqrt{ 1-4p(1-p) }}{2p}\\\\ &amp;= \\frac{1\\pm \\sqrt{ (2p-1)^{2} }}{2p}\\\\ \\implies y &amp;= \\begin{cases} \\frac{1+2p-1}{2p} = \\frac{2p}{2p}=1 &amp; \\text{reject} \\\\ \\frac{1-2p+1}{2p} = \\frac{1-p}{p} = \\frac{q}{p} \\end{cases}\\\\ \\implies \\pi_{i} &amp;= y^{i} = \\left( \\frac{q}{p} \\right)^{i} \\quad \\forall i \\geq 0 \\end{aligned} \\] Intuitively, if \\(q&lt;p\\) then the probability of ruin decreases geometrically fast. If they’re equal, then the chances of ruin are also practically 0. 10.1.2 Scenario 2: Bet entire wealth What is \\(\\mathbb{E}(V_{n})\\) after \\(n\\) steps? \\[ V_{n} = \\begin{cases} V_{0}\\cdot 2^{n} &amp; w.p. p^{n} \\\\ 0 &amp; o/w \\end{cases} \\] \\[ \\begin{aligned} \\mathbb{E}(V_{n}) &amp;= 2^{n}V_{0}p^{n}+0(1-p^{n})\\\\ &amp;= V_{0}\\cdot (2p)^{n} \\to \\infty\\\\ &amp;\\text{Because } p &gt; \\frac{1}{2}\\implies 2p &gt; 1 \\end{aligned} \\] We’ve achieved exponential growth rate by betting everything every step, but should only be used if \\(p=1\\) or you are very very desperate. We are combining very extreme events which means the expected value is kind of meaningless. 10.1.3 Scenario 3: Exp growth w/o ruin Betting a fixed fraction of wealth \\(f\\) at each step. \\[ \\begin{aligned} V_{1} &amp;= \\begin{cases} V_{0}\\cdot (1+f) &amp; \\text{win} \\\\ V_{0}\\cdot (1-f) &amp; \\text{lose} \\end{cases}\\\\ \\implies V_{i} &amp;= \\begin{cases} V_{i-1}(1+f) &amp; \\text{win with } p\\\\ V_{i-1}(1-f) &amp; \\text{lose with }q \\end{cases}\\\\ V_{n} &amp;= V_{n-1} \\cdot (1+f)^{I_{n}}\\cdot(1-f)^{1-I_{n}}\\\\ &amp;= V_{0}\\cdot(1+f)^{\\sum^{n}_{i=1} I_{i}}\\cdot (1-f)^{n-\\sum^{n}_{i=1}I_{i}}\\\\ &amp;\\text{Let }w = \\sum^{n}_{i=1}I_{i}\\\\ &amp;= V_{0}(1+f)^{w}(1-f)^{n-w} \\quad \\text{where } W\\sim Bin(n,p) \\end{aligned} \\] So the expected value becomes: \\[ \\begin{aligned} \\mathbb{E}(V_{n}) &amp;= \\mathbb{E}(V_{0}(1+f)^{w}(1-f)^{n-w})\\\\ &amp;= V_{0}\\cdot (1-f)^{n}\\mathbb{E}\\left( \\frac{1+f}{1-f}^{w} \\right)\\\\ \\\\ &amp;\\text{From Prob Gen Fnc (Laplace Trans) of Bin(n,p)}\\\\ &amp;G_{w}(z)= \\mathbb{E}(z^{w}) = (q+pz)^{n} \\\\ \\\\ \\implies \\mathbb{E}(V_{n}) &amp;= V_{0}\\cdot (1-f)^{n}\\underbrace{ \\mathbb{E}\\left( \\frac{1+f}{1-f}^{w} \\right) }_{ G_{w}\\left( \\frac{1+f}{1-f} \\right) }\\\\ &amp;= V_{0}\\cdot (1-f)^{n}\\left( q+p\\left( \\frac{1+f}{1-f} \\right) \\right)^{n}\\\\ &amp;= V_{0}\\cdot \\left( q(1-f)+p(1+f) \\right)^{n}\\\\ &amp;= V_{0}\\cdot(1-qf+pf)^{n}\\\\ &amp;= V_{0}(1+f\\cdot(p-q))^{n}\\\\ &amp;= V_{0}(1+f\\underbrace{ (2p-1) }_{ &gt;0 })^{n} \\end{aligned} \\] Which gives geometric growth and has very low bankruptcy probability. 10.2 Kelly Criterion We can use this criterion to determine the optimal fraction \\(f\\). Definition: The fraction \\(f\\) is the amount that maximizes the expected log returns. The log can be interpreted as because you are compounding your wealth, you don’t just want to maximize the next step but the entire sequence of bets. By maximizing the expected log-return \\(\\leftrightarrow\\) maximizing expected log of \\(V_{n}\\) \\(\\leftrightarrow\\) maximizing geometric avg of returns. By jensens inequality, \\(E\\left[\\log \\left(V_n\\right)\\right] \\neq \\log \\left(E\\left[V_n\\right]\\right)\\) 10.2.1 Finding the Optimal Value Finding the optimal value of fraction \\(f^{*}\\) according to the Kelly criterion \\[ \\begin{aligned} \\mathbb{E}\\left( \\log\\left( \\frac{V_{n}}{V_{0}} \\right) \\right) &amp;= \\mathbb{E}\\left( \\log\\left( \\frac{V_{0}\\cdot(1+f)^{w}\\cdot(1-f)^{n-w}}{V_{0}} \\right) \\right)\\\\ &amp;= \\mathbb{E}[w\\log(1+f))+(n-w)\\log(1-f)]\\\\ &amp;= \\log(1+f)\\mathbb{E}(w) + \\log(1-f)(n-\\mathbb{E}(w))\\\\ &amp;= \\log(1+f)np+\\log(1-f)(\\underbrace{ n-np }_{ n(1-p)=nq })\\\\ &amp;= \\underbrace{ \\log(1+f)np+\\log(1-f)nq }_{ G(f) }\\\\ \\end{aligned} \\] We want to maximize \\(G(f)\\) with respect to the fraction \\(f\\). We can do this by taking the derivative: \\[ \\begin{aligned} \\frac{\\delta G(f)}{\\delta f} &amp;= \\frac{\\delta}{\\delta f}(\\log(1+f)np+\\log(1-f)nq)\\\\ \\text{Set } 0 &amp;= n\\left( \\frac{1}{1+f}p+\\frac{1}{1-f}(-1)q \\right)\\\\ \\implies \\frac{p}{1+f}&amp;=\\frac{p}{1-f}\\\\ p(1-f)&amp;=q(1+f)\\\\ p-q&amp;=f\\cdot(p+q)\\\\ \\end{aligned} \\] \\[ \\mathbf{\\implies f^{*} = p-q = 2p-1} \\] So the fraction you bet depends on the edge you have on this bet. 10.2.2 Geometric average Assuming a fixed fraction \\(f\\) of wealth is bet at each time step. What is the geometric avg of the returns as \\(n\\to \\infty\\)? \\[ \\begin{aligned} \\text{Let growth rate } r_{i} &amp;\\text{ be }:\\\\ \\text{For } i-1 \\to i: r_{i} &amp;= \\frac{V_{i}}{V_{i-1}}\\\\ \\text{Geometric Average: }&amp;: (r_{1} \\times r_{2}\\dots \\times r_{n})^{1/n} = \\left[ \\prod^{n}_{i=1}(1+f)^{I_{i}}(1-f)^{1-I_{i}} \\right] ^{1/n}\\\\ &amp;= (1+f)^{\\underbrace{ \\sum^{n}_{i=1}I_{i}/n }_{ W_{n} }} \\times (1-f)^{1/n \\sum^{n}_{i=1}(1-I_{i})}\\\\ &amp;= (1+f)^{W_{n}/n}\\times (1-f)^{1-W_{n}/n} \\quad \\text{where } W_{n}\\sim Binom(n,p) \\end{aligned} \\] So as \\(n\\to \\infty\\), \\(\\frac{W_{n}}{n}\\to p\\) from LLN, which implies the geometric average converges to: \\[ (1+f)^{p}(1-f)^{1-p} \\] from the continuous mapping theorem. This is also the expression where if you take the log, you get \\(\\underbrace{ \\log(1+f)np+\\log(1-f)nq }_{ G(f) }\\) from the [[#Finding the Optimal Value]] section. 10.2.3 Simple example Consider general sequence of bets, where \\(1 bet \\implies +\\$a\\) if win and \\(−\\$b\\) if lose For previous examples \\(a = b = 1\\) \\(P(win)=p\\) and \\(P(lose)=q\\) Bet is favorable, i.e. \\(p\\cdot a&gt; q\\cdot b\\) Kelly criterion optimal fraction to bet is \\[ f^{*} = \\frac{pa-qb}{ab} \\] So \\[ \\begin{aligned} \\text{Given }&amp; a = 1, b = 1\\\\ \\\\ f^{*} = &amp;\\frac{1p-1q}{1\\cdot1} = p-q = .55-.45 = .10 \\end{aligned} \\] 10.2.4 Kelly Criterion in Investing Consider a situation where you have: - risk free asset with return \\(r_{i}\\) - One risky asset with return \\(R_{i}\\), with \\(\\mathbb{E}(R) = \\mu, \\mathbb{V}(R) = \\sigma^{2}\\) Assume we invest fraction \\(f\\) in the risky asset, and \\(1-f\\) in the risk free asset. We first need to show that: \\[ \\mathbb{E}\\left[ \\log\\left( \\frac{V_{n}}{V_{0}} \\right) \\right] \\approx \\log(1+r_{f}) +f\\left( \\frac{\\mu-r_{f}}{1+r_{f}} \\right)-f^{2}\\left( \\frac{\\sigma^{2}+(\\mu-r_{f})^{2}}{2(1+r_{f})^{2}} \\right) \\] \\[ \\begin{aligned} \\text{Know: }V_{1} &amp;= V_{0}\\cdot f\\cdot(1+R) + V_{0}(1-f)(1+r_{f})\\\\ &amp;= V_{0}[1+fR+(1-f)r_{f}]\\\\ &amp;= V_{0}[1+r_{f}+f\\underbrace{ (R-r_{f}) }_{ \\text{Excess ret} }]\\\\ \\implies \\frac{V_{1}}{V_{0}} &amp;= \\log(1+r_{f}+f(R-r_{f}))\\\\ \\\\ \\implies \\log \\left( \\frac{V_{n}}{V_{0}} \\right) &amp;= \\log \\left( \\prod^{n}_{t=1} \\frac{V_{t}}{V_{t-1}} \\right) \\\\ &amp;= \\sum^{n}_{t=1} \\log(1+r_{f}+f(R_{t}-r_{f}))\\\\ \\\\ \\implies \\mathbb{E}\\left[ \\log\\left( \\frac{V_{n}}{V_{0}} \\right) \\right] &amp;= \\sum^{n}_{t=1}\\mathbb{E}[\\log(1+r_{f}+f(R_{t}-r_{f}))]\\\\ &amp; \\text{Assuming i.i.d. returns}\\\\ &amp;= n \\mathbb{E}[\\log(1+r_{f}+f(R-r_{f}))] \\end{aligned} \\] We can simplify this using a taylor approx of the function \\(\\log(1+r_{f}+f(R-r_{f}))\\) around \\(1+r_{f}\\): \\[ \\begin{aligned} \\underbrace{ \\log }_{ g } \\left( \\underbrace{ 1+r_{f} }_{ x_{0} }+\\underbrace{ f(R_{t}-r_{f} }_{ \\delta }) \\right) &amp;\\approx \\underbrace{ \\log(1+r_{f}) }_{ g(x_{0}) } + \\underbrace{ \\frac{1}{1+r_{f}}f(R_{t}-r_{f}) }_{ g&#39;(x_{0}) }+\\underbrace{ \\frac{1}{2}\\left( \\frac{f^{2}}{(1+r_{f})^{2}}\\right)\\delta^{2} }_{ g&#39;&#39;(x_{0}) } \\end{aligned} \\] \\[ \\begin{aligned} \\mathbb{E}[g(x_{0}-\\delta)] \\approx \\log(1+r_{f}) &amp;+ \\frac{1}{1+r_{f}}f\\underbrace{ \\mathbb{E}(R_{t}-r_{f}) }_{ \\mu-r_{f} }+\\frac{1}{2}\\left( \\frac{f^{2}}{(1+r_{f})^{2}}\\right)\\mathbb{E}[(R_{t}-r_{f})^{2}]\\\\ \\text{Where } \\mathbb{E}[(R_{t}-r_{f})^{2}] &amp;= (\\sigma^{2}+\\mu^{2}+r_{f}^{2}-2\\mu r_{f}) \\end{aligned} \\] We can now show that \\(f^{*} \\approx (1+r_{f}) \\frac{(\\mu-r_{f})}{\\sigma^{2}}\\) \\[ \\begin{aligned} \\text{Let } G(f) &amp;= \\log(1+r_{f})+\\frac{f}{1+r_{f}}(\\mu-r_{f})-\\frac{f^{2}}{2(1+r_{f})^{2}}(\\sigma^{2}+\\mu^{2}+r_{f}^{2}-2\\mu r_{f}) \\end{aligned} \\] \\[ \\mathbf{\\implies f^{*} \\approx (1+r_{f}) \\frac{\\mu-r_{f}}{\\sigma^{2}}} \\] 10.2.4.1 Example in investing 10.2.5 Theoretical properties In the long term \\(n\\to \\infty\\), with probability 1, a strategy based on a Kelly criterion will: - Maximize the limiting exponential growth rate of wealth - Maximizes median of final wealth - Half of distribution is above median &amp; half below it - Minimizes the expected time required to reach a specified goal for the wealth 10.2.6 Criticisms It tends to be risky It works well if you have clear information (what exactly your edge is) and that you have an infinite sequence of gambles In practice, people will take half of the kelly criterion, as a precaution against high volatility Most people retire at some point and therefore do not have infinite gambles "],["statistical-arbitrage.html", "Chapter 11 Statistical Arbitrage 11.1 Pairs Trading 11.2 Financial Indices 11.3 Index Arbitrage 11.4 Volatility Arbitrage 11.5 High Frequency Trading", " Chapter 11 Statistical Arbitrage Mathematical definition of a “free lunch”. In the real world, there is never 0 risk. Statistical arbitrage uses “statistical mispricing” of assets to make money. This involves long and short positions simultaneously and are typically short-term and market-neutral. Examples: Pairs Trading Index Arbitrage Volatility Arbitrage Algorithmic and High Frequency Trading 11.1 Pairs Trading Developed in the 80’s by Morgan Stanley quants, who made ~50 Million in profits in 1987. The idea: Find pairs of stocks that tend to move together, and once they diverge, you expect them to come back together. If they cross some threshold of divergence, buy the low priced one and sell the higher priced one until they converge. Problem: If the prices don’t converge. Example of two pairs: We use log asset prices because it’s easier to model with Brownian motion. Using this chart, we can decide the threshold at which to trade the two stocks: Key: Buy and sell the same dollar amount. Buy \\(\\frac{1}{P2_{0}}\\) where \\(P2_{o}\\) is the opening price of stock 2 Per $1 invested, the profit will be: \\[ \\frac{1}{P2_{o}}\\times P2_{c}-\\frac{1}{P1_{o}}\\times P1_{c} = \\frac{28.07}{27.15}-\\frac{30.23}{32.91} = .11532 \\] The profitability is determined by the behaviour of the log asset-price ratio. \\[ \\begin{gathered} \\text{Profitable if: } \\frac{P1_{c}}{P1_{o}} - \\frac{P2_{c}}{P2_{o}} \\ne 0\\\\ \\implies \\frac{P1_{c}}{P1_{o}} &lt; \\text{ or }&gt; \\frac{P2_{c}}{P2_{o}} \\end{gathered} \\] which is equivalent to asking \\[ \\begin{aligned} \\frac{P2_{o}}{P1_{o}} &amp;&lt; \\text{or} &gt; \\frac{P2_{c}}{P1_{c}}\\\\ &amp;\\iff\\\\ \\log(\\frac{P2_{o}}{P1_{o}}) &amp;&lt; \\text{or} &gt; \\log(\\frac{P2_{c}}{P1_{c}}) \\end{aligned} \\] &gt;Profitability is determined by log-ratio of prices. We say \\(&lt; \\text{ or } &gt;\\) just because we only care that there exists a difference, but not which one exactly is the larger of the two. We can also show market-neutrality: \\[ \\begin{aligned} CAPM &amp;\\implies \\begin{cases} R1 = \\beta\\cdot R_{M} + \\epsilon_{1} \\\\ R2 = \\beta\\cdot R_{M} + \\epsilon_{2} \\\\ \\end{cases} \\implies \\begin{cases} \\frac{P1_{c}}{P1_{o}} = 1+R1 = 1+\\beta\\cdot R_{M} + \\epsilon_{1} \\\\ \\frac{P2_{c}}{P2_{o}} = 1+R2 = 1+\\beta\\cdot R_{M} + \\epsilon_{2} \\\\ \\end{cases}\\\\ &amp;\\implies \\frac{P1_{c}}{P1_{o}} - \\frac{P2_{c}}{P1_{o}} = \\frac{P2_{c}}{P2_{o}} = (1+\\beta\\cdot R_{M} + \\epsilon_{1}) - (1+\\beta\\cdot R_{M} + \\epsilon_{2}) = \\epsilon_{1}-\\epsilon_{2}\\\\ \\end{aligned} \\] Where \\(\\epsilon_{1}-\\epsilon_{2}\\) is independent of market return \\(R_{M}\\). 11.1.1 What could go wrong When the prices don’t converge, and the log ratio of prices don’t go back to the mean. (Mean-reverting) 11.1.2 Other things to consider Determine which pairs to trade When to open a position (the threshold) What amounts to buy/sell When to close trade When to bail out of trade Need to create a statistical/mathematical model to help make decisions on these items to consider. 11.1.2.1 1. Determining which pairs to trade For a market with \\(N\\) assets, there are max \\(_{n}C_{2}\\) pairs that could be made which is in the order of \\(n^{2}\\). We only want pairs who’s log-ratio has strong mean reversion. This is not the same as simply having a constant mean. It depends on the dynamics of log-ratio process. For example, ARMA models have mean reversion properties but random walk processes/brownian motion do not have the mean reversion property. 11.1.2.2 Example 1: Log Let \\(X_{t} = \\log(\\frac{P1_{t}}{P2_{t}}) \\sim^{iid} N(0,\\sigma^{2}), \\ \\forall t=1,2,\\dots\\) If \\(X_{0} = 2\\sigma\\), what’s the expected time until \\(X_{T}\\leq 0\\)? On any day \\(t\\), \\(\\mathbb{P}(X_{t}\\leq 0) = 0.5\\) by symmetry \\[ \\begin{aligned} \\text{Let } T &amp;= \\# \\text{ days until } X_{t} \\leq 0 \\text{ for the first time}\\\\ &amp;= \\text{(hitting time)}\\\\ &amp;= \\# \\text{ trials until 1st success } x_{t} \\leq 0\\\\ \\implies T &amp;\\sim \\text{Geometric}\\left( p=\\frac{1}{2} \\right)\\\\ PMF \\ \\ p_{T}(t) &amp;= \\left( \\frac{1}{2} \\right)^{t} \\quad \\forall t \\geq 1\\\\ \\mathbb{E}(T) &amp;= \\frac{1}{p} = 2 \\end{aligned} \\] 11.1.2.3 Example 2: BM Let \\(X_{t} = \\log(\\frac{P1_{t}}{P2_{t}}) \\sim\\) Brownian Motion (BM) (continuous time Random Walk) For any \\(X_{0}=c &gt; 0\\), show that the expected time until \\(X_{T}\\leq 0\\) is infinite. \\[ \\begin{aligned} \\text{Let } T_{c} &amp;= \\{\\text{first time std BM w/} W_{0} =0 \\text{ hits level c}\\}\\\\ \\text{Let }M_{t} &amp;= \\max\\left\\{ W_{u} ; 0 \\leq u \\leq t \\right\\} \\implies M_{t} \\sim |W_{t}| \\end{aligned} \\] Which tells you that the distribution of the \\(M_{t}\\) follows the same distribution of the absolute value of the Brownian Motion. This means: \\[ \\begin{aligned} \\mathbb{P}(T_{c}\\leq t) = \\mathbb{P}(M_{t}\\geq c) = \\mathbb{P}(|\\underbrace{ W_{t} }_{ \\sim N_(0,t) }| \\geq c) = 2 \\cdot \\Phi\\left( -\\frac{c}{\\sqrt{ t }} \\right) \\end{aligned} \\] Where \\(\\Phi\\) is the CDF of the std normal. As we look at the two tails of the normal after standardizing, but we take the area under the lower tail (-c). \\[ \\begin{aligned} \\implies \\text{PDF of }&amp; T_{c} \\text{ is given by } f(t)\\\\ f(t) &amp;= \\frac{\\delta}{\\delta t} \\underbrace{ \\mathbb{P}(T_{c}\\leq t) }_{ F(t) } = \\frac{\\delta}{\\delta t}\\left[ 2\\cdot \\Phi\\left( -\\frac{c}{\\sqrt{ t }} \\right) \\right]\\\\ &amp;= 2 \\frac{\\phi\\left( -\\frac{c}{\\sqrt{ t }} \\right)\\delta}{\\delta t}\\left( -\\frac{c}{\\sqrt{ t }} \\right)\\\\ &amp;= 2\\phi\\left( -\\frac{c}{\\sqrt{ t }} \\right)\\cdot\\left( -\\left( -\\frac{1}{2} \\frac{c}{\\sqrt{ t^{3} }}\\right) \\right)\\\\ &amp;= \\frac{1}{\\sqrt{ 2 \\pi }}e^{-1/2 \\cdot c^{2}/t}\\cdot \\frac{c}{\\sqrt{ t^{3} }} \\end{aligned} \\] So the expected value of hitting time is: \\[ \\begin{aligned} \\mathbb{E}(T_{c}) &amp;= \\int ^{\\infty}_{0} t\\cdot f(t) \\, dt\\\\ &amp;= \\int ^{\\infty}_{0}t \\frac{c}{\\sqrt{ 2\\pi t^{3} }}e^{-1/2 \\cdot c^{2}/2} \\, dt\\\\ &amp;= \\frac{c}{\\sqrt{ 2\\pi }} \\int^{\\infty}_{0} \\frac{1}{\\sqrt{ t }}e^{-c^{2}/2t} \\, dt\\\\ &amp;\\geq \\frac{c&#39;}{\\sqrt{ 2\\pi }}\\int^{\\alpha}_{0} \\frac{1}{\\sqrt{ t }} \\, dt + \\int ^{\\infty}_{\\alpha} \\frac{1}{\\sqrt{ t }}e^{-c^{2}/2t} \\, dt \\to \\infty\\\\ \\\\ \\implies \\mathbb{E}[T_{c}] &amp;= \\infty \\end{aligned} \\] A stationary process ensures mean reversion. Thats why we use stationary processes to model the log-ratio. 11.1.3 Mean Reversion Mean reversion suggests log-ratio process are stationary. Stationary processes are guaranteed to converge back to their mean within a reasonable time. Marginal distribution has constant mean, variance, and covariance between two stationary variables only depends on the distance between times Auto-Correlation Function (ACF) \\(\\rho(h), \\forall h=0,1, \\ldots\\) describes (linear) dependence at lag \\(h=|t-s|\\) \\[ \\begin{aligned} \\mathbb{E}[X_{t}] &amp;= \\mu \\quad \\forall t\\\\ \\mathbb{V}[X_{t}] &amp;= \\sigma &lt; \\infty \\quad \\forall t\\\\ Cov(X_{t},X_{s}) &amp;= Cov(X_{t+r},X_{s+r}) \\quad \\forall r,s,t \\end{aligned} \\] 11.1.4 Integrated Series In general, asset log-prices which are random walks, are not stationary. They are unpredictable. However, the log returns \\(r_{t} = \\log(\\frac{S_{t}}{S_{t-1}})\\) follow a stationary process, whereas the asset log-prices \\(\\log(S_{t}) = \\log(S_{0}) + \\sum^{t}_{i=1}r_i\\) are random walks. A variable that is not stationary but it’s differences are, is called an integrated series. \\(\\left\\{ X_{t} \\right\\}\\) is not stationary but \\(\\left\\{ \\nabla X_{t} = X_{t} - X_{t-1}\\right\\}\\) is. 11.1.5 Cointegration Consider two integrated series \\(\\left\\{ X_{t}, Y_{t} \\right\\}\\) which behave as random walks, but if they seem to have some constant (stationary) relationship when linearly combined then they are called cointegrated. \\[ \\exists \\ \\alpha\\ s.t.\\ X_{t} + \\alpha Y_{t} \\sim Stationary \\] 11.1.5.1 Ex: Yield Rate The short, medium, and long term rates are considered cointegrated as they individually follow a random walk, but they do it together. 11.1.5.2 Ex: Math Let \\(\\left\\{ W_{t} \\right\\}\\) be a random walk, and \\[ \\begin{cases} X_{t} = W_{t} + \\epsilon_{t} \\\\ Y_{t} = W_{t} + \\nu_{t} \\end{cases} \\] where \\(\\epsilon, \\nu \\sim^{iid} N(0,\\sigma^{2})\\) We can show \\(\\left\\{ X_{t},Y_{t} \\right\\}\\) are cointegrated. (They are not stationary, but their 1st order different is stationary) \\[ \\mathbb{V}(X_{t}) = \\mathbb{V}[W_{t}+\\epsilon_{t}] = \\overbrace{ \\mathbb{V}(W_{t}) }^{ t \\sigma_{w}^{2} } + \\overbrace{ \\mathbb{V}(\\epsilon_{t}) }^{ \\sigma^{2} } = t\\sigma_{w}^{2}+\\sigma^{2} \\implies \\text{not stationary} \\] Which is the same case for \\(Y_{t}\\). Looking at the first order differences: \\[ \\begin{gathered} \\nabla X_{t} = X_{t}-X_{t-1} = W_{t}+\\epsilon_{t} -W_{t-1}+\\epsilon_{t-1} = (W_{t}-W_{t-1})+\\epsilon_{t}+\\epsilon_{t-1}\\\\ \\implies \\mathbb{V}(\\nabla X_{t}) = \\sigma^{2} \\text{stationary} \\end{gathered} \\] To show cointegration, we much show \\(X_{t}-Y_{t}\\) is stationary. \\[ \\begin{gathered} X_{t}-Y_{t} = (W_{t}+\\epsilon_{t})-(W_{t}+\\nu_{t}) = \\epsilon_{t} + \\nu_{t} \\text{sum of iid sequences}\\\\ \\implies \\begin{cases} \\mathbb{E}(X_{t}-Y_{t}) = 0 \\\\ \\mathbb{V}(X_{t}-Y_{t}) = 2\\sigma^{2} \\\\ Cov(X_{t}-Y_{t}, X_{t+h}-Y_{t+h}) = \\rho |h| ?? \\end{cases} \\end{gathered} \\] For pairs trading, we want assets which are cointegrated (their log difference is stationary) 11.1.6 Stationarity Time series is not always a good indicator of a random walk. We can use a ACF to determine it. Linear decline -&gt; random walk 11.1.6.1 Tests There also exists stationarity tests: Hypothesis test for \\(\\left\\{\\begin{array}{l}H_0 \\text { : series is integrated } \\\\ H_1 \\text { : series is stationary }\\end{array}\\right.\\) - Idea: fit \\(X_t=\\beta X_{t-1}+\\varepsilon_t\\) to data and test \\(\\left\\{\\begin{array}{l}H_0: \\beta=1 \\\\ H_1: \\beta&lt;1\\end{array}\\right.\\) E.g. For \\(n=1000\\) | Model | Test statistic | P-value | |———————————-|—————-|———————————| | \\(X_t=X_{t-1}+\\varepsilon_t\\) | -1.9027 | 0.6195 | | \\(X_t=+.95 X_{t-1}+\\varepsilon_t\\) | -5.6161 | \\(&lt;&lt;.01\\) | | \\(X_t=-.95 X_{t-1}+\\varepsilon_t\\) | -232.4851 | \\(&lt;.01\\) | But if we’re trying to check if a linear combination of assets is stationary, its difficult because we don’t know the linear combination to look for. There are two tests for cointegration: Engle-Granger two-step method: Run linear regression and look for potential cointegrated relationship, finding the combination that minimizes squared errors. Then the errors are tested for stationarity (converge to 0) Vector Error Correction models (VECM): Covered in time series, a part of VAR models 11.1.6.2 Two step method example First we fit a linear regression to obtain the slope (not useful), and the intercept (need to subtract from errors). Now we can look at the errors (minus the intercept so its 0 centered) and run stationarity tests on them. We would trade stocks with the highest metrics, although this method is not optimal since our results could be arbitrary. If we did the regression swapping the variables, the p-value could differ a lot 11.1.7 Spurious regression Two totally independent stocks may show some false linear relationship. Consider 2 independent random walks \\(\\left\\{W_t, V_t\\right\\}\\) - When you regress \\(W_t=\\beta_0+\\beta V_t+e_t, t=1, \\ldots, n\\) you are NOT guaranteed that \\(\\hat{\\beta} \\rightarrow 0\\) as the sample size \\(n \\rightarrow \\infty\\) (i.e. not consistent)!!! Effect called spurious (fake) regression - Results of random walk (integrated series) regressions are NOT reliable - To address this problem &amp; estimation errors, use Phillips-Ouliaris stationarity test in 2 step method Bottom line is using VAR models is superior than the two step test/using linear regression. A multivariate time series model is best. 11.2 Financial Indices Indices measure value/performance of financial markets. E.g., - Dow-Jones Industrial Average (DJIA): Simple average of 30 major US stock prices (since 1896) - Standard &amp; Poor (S&amp;P) 500: Weighted (cap-base) average of 500 large NYSE &amp; NASDAQ listed companies Financial indices are NOT traded instruments However, there are many financial products whose value is directly related to indices: - Mutual funds: e.g., Vanguard 500 Index Fund - Exchange-Traded-Funds (ETF’s): e.g., SPDR or iShares S&amp;P500 Index - Futures: e.g., E-Mini S&amp;P futures 11.3 Index Arbitrage Financial products based on indices essentially offer a sophisticated version of multivariate cointegration Index of #N assets \\(\\left\\{S_i\\right\\}_{i=1}^N\\) w/ weights \\(\\left\\{w_i\\right\\}_{i=1}^N\\) \\(\\Rightarrow\\) Index level: \\(I(t)=\\sum_{i=1}^N w_i \\times S_i(t)\\) Instrument tracking index \\(F(t)\\) (e.g. futures) Known cointegration relationship: \\[ F(t)-I(t)=F(t)-\\sum_{i=1}^N w_i \\times S_i(t) \\sim \\text { stationary } \\] 11.4 Volatility Arbitrage VolArb is implemented with derivatives, primarily options To fix ideas, consider European options: - For Black-Scholes formula, only unobserved input is volatility \\(\\sigma\\), which has to be estimated - How does volatility affect Call/Put prices? - Implied volatility \\(\\sigma_i\\) is input which makes Black-Scholes price equal to observed market price - \\(\\sigma_i\\) is not estimated from underlying asset dynamics - Imagine you know volatility will increase in the future, beyond what current options prices warrant (implied vol σi ). - How can you take advantage of this? - Eliminate effects of asset movement by delta-neutral strategy - Profit by large moves, irrespective of direction VolArb relies on predicting (implied) volatility of underlying asset Common approach is to describe the evolution of volatility with GARCH (Generalized AutoRegressive Conditional Heteroskedasticity) models \\[ \\begin{gathered} y_t=\\sigma_t \\cdot \\varepsilon_t, \\varepsilon_t \\sim^{i i d} N(0,1) \\\\ \\sigma_t^2=\\alpha_0+\\sum_{j=1}^p \\alpha_j y_{t-j}^2+\\sum_{k=1}^q \\beta_k \\sigma_{t-k}^2 \\end{gathered} \\] Essentially, an autoregressive model for conditional variance 11.4.1 E.g. Delta-hedged long call 11.5 High Frequency Trading HFT uses algorithmic trading over very short holding periods, profiting from very small price discrepancies by trading frequently and at large volumes HFT employs predictive algorithms for machine learning and data mining Essentially, tries to discover patterns of trading activity &amp; profit by preempting them This includes traditional method like index arbitrage, but also others which might not have any intuitive interpretation "],["monte-carlo-simulation.html", "Chapter 12 Monte-Carlo Simulation 12.1 Brownian Motion 12.2 Properties of the Multivariate Normal 12.3 Geometric Brownian Motion 12.4 Risk-Neutral Pricing 12.5 European Call 12.6 Multiple Assets 12.7 Cholesky Factorization", " Chapter 12 Monte-Carlo Simulation Primarily dealing with option pricing. There are three basic numerical option pricing methods: Binomial Trees (BT) Don’t need sophisticated mathematical or stochastic analysis, just backward induction Useful in many cases Finite Difference (FD) Uses the Black-Scholes PDE Not discussed much in the stats program Monte-Carlo Simulation Based on the stochastic differential equation (determines evolution of asset-price) Easy to program and risk-neutral Really shines with multiple dimensions (many factors, path/multi-asset) 12.1 Brownian Motion Any martingale can be represented as an integral of brownian motion. Brownian Motion (BM) forms the building block of continuous stochastic models Standard BM \\(\\left\\{W_t\\right\\}\\) is such that \\[ W_0=0 \\quad \\&amp; \\quad (W_t-W_s) \\mid W_s \\sim N(0, t-s) \\] Arithmetic BM (ABM) \\(\\left\\{X_t\\right\\}\\) with drift \\(\\mu\\) &amp; volatility \\(\\sigma\\) is \\[ X_0=0 \\quad \\&amp; \\quad \\left(X_t-X_s\\right) \\mid X_s \\sim N\\left(\\mu(t-s), \\sigma^2(t-s)\\right) \\] In form of Stochastic Differential Equation (SDE) \\[ d X_t=\\mu d t+\\sigma d W_t \\Leftrightarrow X_t-X_0=\\mu t+\\sigma\\left(W_t-W_0\\right) \\] 12.2 Properties of the Multivariate Normal If \\[ \\mathbf{X}=\\left[ \\begin{array}{c} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\\\ \\end{array} \\right] \\sim \\mathbf{N}\\left(\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{array} \\right], \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} \\boldsymbol{\\Sigma}_{11} &amp; \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} &amp; \\boldsymbol{\\Sigma}_{22} \\end{array}\\right] \\right) \\] , then: Property Formula Marginals: \\[\\quad \\mathbf{X}_1 \\sim \\mathbf{N}\\left(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_{11}\\right)\\] Linear combinations: \\[\\mathbf{a}+\\mathbf{B}^{\\top} \\mathbf{X} \\sim N\\left(\\mathbf{a}+\\mathbf{B}^{\\top} \\boldsymbol{\\mu}, \\mathbf{B}^{\\top} \\boldsymbol{\\Sigma} \\mathbf{B}\\right)\\] Conditionals : \\[\\mathbf{X}_1 \\mid\\left(\\mathbf{X}_2=\\mathbf{x}\\right)\\sim\\mathrm{N}\\left(\\boldsymbol{\\mu}_1+\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\left(\\mathbf{x}\\boldsymbol{\\mu}_2\\right),\\boldsymbol{\\Sigma}_{11}-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}\\right)\\] If \\(\\Sigma_{12}=0\\) then \\(\\mathbf{X_{1}}|\\mathbf{X_{2}}=x \\sim N(\\mu_{1},\\Sigma_{11})\\) Using these properties, we can do calculations for brownian motion 12.2.1 Ex: Distribution given X_s Finding the distribution of \\(X_{t}|X_{s} = x, t&gt;s\\) for \\(dX_{t} = \\mu dt+\\sigma dW\\) \\[ \\begin{aligned} \\left[ \\begin{array}{c} X_{s} \\\\ X_{t} \\end{array} \\right] \\sim N( \\left[ \\begin{array}{c} \\mu_{s} \\\\ \\mu_{t} \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma^{2}s &amp; \\sigma s \\\\ \\sigma s &amp; \\sigma^{2}t \\end{array} \\right] ) \\end{aligned} \\iff \\left[ \\begin{array}{c} X_{t} \\\\ X_{s} \\end{array} \\right] \\sim N(\\mu \\left[ \\begin{array}{c} t \\\\ \\hline s \\end{array} \\right], \\sigma^{2} \\left[ \\begin{array}{cc} t &amp; s \\\\ s &amp; s \\end{array} \\right] ) \\] Because \\[ \\begin{aligned} Cov(W_{s},W_{t}) &amp;= Cov(W_{s}, W_{s}+(W_{t}-W_{s}))\\\\ &amp;= Cov(W_{s}, W_{s}) + \\underbrace{ Cov(W_{s}, (W_{t}-W_{s})) }_{ =0 }\\\\ &amp;= Var(W_{s}) = s \\end{aligned} \\] Which gives: \\[ \\begin{aligned} X_{t} | X_{s} = x &amp;\\sim N\\left( \\mu t + \\sigma^{2}s \\frac{1}{\\sigma^{2}s} (x-\\mu s), \\sigma^{2}\\left( t-s \\frac{1}{8}8 \\right) \\right)\\\\ &amp;\\sim N(x + \\mu (t-s), \\sigma^{2}(t-s)) \\end{aligned} \\] 12.2.2 Ex: Brownian Bridge (Distribution given X_t) Find the distribution of \\(X_{s} | X_{t} = x, s\\in (0,t)\\) for \\(dX_{t} = \\mu dt + \\sigma dW_{t}\\) \\[ \\begin{aligned} \\left[ \\begin{array}{c} X_{s} \\\\ X_{t} \\end{array} \\right] &amp;\\sim N(\\mu \\left[ \\begin{array}{c} s \\\\ t \\end{array} \\right], \\sigma^{2} \\left[ \\begin{array}{cc} s &amp; s \\\\ s &amp; t \\end{array} \\right] )\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\implies X_{s} | X_{t} = x &amp;\\sim N\\left( \\underbrace{ \\mu s+\\frac{s}{t}(x-\\mu t) }_{ \\mu s+\\frac{s}{t}x-\\mu s }, \\sigma^{2}\\left( s-\\frac{s}{t}s \\right)\\right)\\\\ &amp;\\sim N\\left( \\frac{s}{t}x, \\sigma^{2} \\frac{s(t-s)}{t}\\right) \\end{aligned} \\] May be useful for generating brownian motion paths where I know that at time \\(T\\), I have value of \\(x\\). 12.3 Geometric Brownian Motion A transformation of the arithmetic brownian motion. We use this to avoid negative values. Process \\(\\left\\{S_t\\right\\}\\) whose logarithm follows ABM \\[ \\begin{aligned} &amp; \\log \\left(S_t\\right)-\\log \\left(S_0\\right)=\\log \\left(\\frac{S_t}{S_0}\\right)=X_t \\sim N\\left(\\mu t, \\sigma^2 t\\right) \\Leftrightarrow \\\\ &amp; \\Leftrightarrow S_t=S_0 \\exp \\left\\{X_t\\right\\} \\sim S_0 \\times \\log \\operatorname{Normal}\\left(\\mu t, \\sigma^2 t\\right) \\end{aligned} \\] As the log of \\(\\exp \\left\\{ X_{t} \\right\\} = X_{t}\\) and \\(X_{t} \\sim Normal(\\mu t, \\sigma^{2}t)\\) Expressed in terms of SDE: \\[ d X_t=d \\log \\left(S_t\\right)=\\mu d t+\\sigma d W_t \\Leftrightarrow d S_t=\\left(\\mu+\\frac{\\sigma^2}{2}\\right) S_t d t+\\sigma S_t d W_t \\] 12.4 Risk-Neutral Pricing Assuming Geometric BM for asset \\(\\left\\{S_t\\right\\}\\) and risk-free interest rate \\(r\\), there exists a probability measure such that \\[ d S_t=r S_t d t+\\sigma S_t d W_t \\Rightarrow S_t \\sim S_0 \\times \\log N\\left(\\left(r-\\frac{\\sigma^2}{2}\\right) t, \\sigma^2 t\\right)\\] Under this measure, discounted asset prices are martingales Called risk-neutral (RN) or equivalent martingale measure The arbitrage-free price of any European derivative with payoff \\(G_T=f\\left(S_T\\right)\\) is given by discounted expectation w.r.t. RN measure \\[ G_0=\\mathbb{E}\\left[e^{-r T} G_T\\right]=\\mathbb{E}\\left[e^{-r T} f\\left(S_T\\right)\\right] \\] We just need to figure out that expected value to obtain the price at time \\(0\\). 12.4.1 Ex: RN Pricing We’ll show that under risk-neutral pricing measure, \\[ \\mathbb{E}(S) = S_{0}e^{rt} \\] (a martingale) then we would have \\[ \\mathbb{E}\\left( \\frac{S_{t}}{e^{rt}}\\right) = S_{0} \\quad \\text{ or more generally } \\mathbb{E}\\left( \\frac{S_{t}}{e^{rt}} |S_{s} \\right) = \\frac{S_{s}}{e^{rt}} \\] Proof: \\[ \\begin{aligned} \\mathbb{E}(S_{t}) &amp;= \\mathbb{E}(S_{0}e^{\\log(S_{t}/S_{0})})\\\\ &amp;= S_{0}\\mathbb{E}(e^{Y}) \\text{ where } Y=\\log\\left( \\frac{S_{t}}{S_{0}} \\right) \\sim N\\left( \\left( r-\\frac{\\sigma^{2}}{2} \\right)t, \\sigma^{2}t \\right)\\\\ \\end{aligned} \\] We can use the Normal MGF \\(\\mathbb{E}(e^{X})\\) if \\(X\\sim N(\\mu, \\sigma^{2})\\): \\[ m_{X}(z) = e^{\\mu z+(1/2) \\cdot \\sigma^{2}z^{2}} \\] In this case, we have \\[ \\begin{aligned} m_{Y}(1) = \\exp \\left\\{ \\overbrace{ \\left( r-\\frac{\\sigma^{2}}{2} \\right) }^{ \\mathbb{E}(Y) } t + \\overbrace{ \\frac{1}{2} \\sigma^{2}t }^{ \\mathbb{V}(Y) }\\right\\} = \\exp \\left\\{ rt-\\frac{\\sigma^{2}}{2} t + \\frac{\\sigma^{2}}{2} t\\right\\} = S_{0}e^{rt} \\end{aligned} \\] 12.4.2 Ex: Find price of forward contract (no dividends) \\[ G_{T} = f(S_{T}) = (S_{T} - F_{0,T}) \\quad \\text{ where } F_{0,T} = S_{0}\\cdot e^{rT} \\] We know \\(G_{0}=0\\) (forward contracts involve no cashflow at \\(t=0\\)) By the RN pricing: \\(G_{0} = \\mathbb{E}[e^{-rT}G_{T}]\\) \\[ \\begin{gathered} \\implies 0 = \\mathbb{E}\\left[\\underbrace{ e^{-rT} }_{ \\text{const, can remove} } (S_{T}-F_{0,T}) \\right]\\\\ \\implies 0=\\mathbb{E}(S_{T}) - F_{0,T}\\\\ \\implies F_{0,T} = \\mathbb{E}(S_{T}) = e^{rT}\\mathbb{E}(\\underbrace{ e^{-rT}S_{T} }_{ S_{0}e^{rT} = S_{T} \\sim mtgl}) = e^{rT}S_{0} \\end{gathered} \\] 12.4.3 Estimating Expectations If \\(\\mathbb{E}(e^{-rT}f(S_{t}))\\) cannot be calculated exactly, it can be estimated/approximated by simulation. Generate # N independent random variates \\(S_{i}(T), i=1,\\dots,N\\) based on RN measure By Strong LLN \\[ \\hat{G}_{0} = \\frac{1}{N} \\sum^{N}_{i=1}e^{-rT}f(S_{i}(T)) \\to \\mathbb{E}(e^{-rT}f(S_{T})) \\] with probability 1 By the CLT \\[ \\frac{\\hat{G}_0-G_0}{s_G / \\sqrt{n}} \\sim^{a p p r} . N(0,1), \\text { where } s_G^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left[e^{-r T} f\\left(S_i(T)\\right)-\\hat{G}_0\\right]^2 \\] If you can express the quantity you’re looking for as an expected value, you can run a simulation and it will approximate the value you’re looking for How do you run a simulation experiment to approximate a probability? \\(\\mathbb{P}(A)=\\mathbb{E}(I_{A})\\) the indicator variable of a given event \\(A\\). 12.4.4 Ex: Show estimator is consistent Estimator: \\(\\mathbb{E}[e^{-rT}f(S_{T})]\\) Build a 95% confidence interval for \\(G_{0}\\) as well \\[ \\begin{aligned} \\mathbb{E}(\\hat{G_{0}}) &amp;= \\mathbb{E}\\left[ \\frac{1}{n}\\sum^{n}_{i=1} e^{-rT}f(S_{i}(T)) \\right]\\\\ &amp;= \\frac{1}{n}\\sum^{n}_{i=1}\\underbrace{ \\mathbb{E}[e^{-rT}f(S_{i}(T))] }_{ G_{1}? } = \\frac{1}{n}nG_{0} = G_{0} \\end{aligned} \\] Confidence interval: \\[ \\hat{G_{0}} \\pm 1.96 \\times \\frac{SG}{\\sqrt{ n }} \\] Need 4 times the samples to double the accuracy (scaled by \\(\\sqrt{ n }\\)) 12.5 European Call Estimating European call price w/ simulation Asset price dynamics: \\(dS_{t} = rS_{t}dt +\\sigma S_{t}dW_{t}\\) Payoff function for strike \\(K\\) and maturity \\(T\\): \\(f(S_{t})=(S_{T} - K_{T})\\) Generate random asset price variates as: \\[ S_i(T)=S(0) \\times \\exp \\left\\{\\left(r-\\frac{\\sigma^2}{2}\\right) T+\\sigma \\sqrt{T} \\times Z_i\\right\\} \\] where \\(Z_i\\) is standard Normal variate Where the x-axis is \\(S_t\\), and y-axis is the payoff \\(G_{t}\\) top=19;bottom=-1;left=-1;right=18 --- y=0|x&lt;k y=x-k|x&gt;=k k=6 After simulation, we get asset-prices that follow: And we can see that the Monte Carlo simulated price converges to the true price: When model options with depend on multiple assets, we need to consider those assets in the model. 12.6 Multiple Assets The payoff of some options depends on the prices of multiple assets. E.g. Exchange (outperformance) option w/ payoff \\[ max \\left\\{ S_{1}(T) - S_{2}(T), 0 \\right\\} = (S_{1}(T)-S_{2}(T)) \\] MC option pricing requires simulating &amp; averaging multiple asset prices/paths - Cannot simply simulate each asset separately - Need to consider cross-asset dependence Assets may have cross-asset dependence, so we cannot simply just simulate each asset independently until time \\(T\\). 12.6.1 Multivariate Brownian Motion Define \\(d\\)-dimensional standard BM \\(\\mathbf{W}(t)=\\left[\\begin{array}{c}W_1(t) \\\\ \\vdots \\\\ W_d(t)\\end{array}\\right]\\) with correlation matrix \\(\\boldsymbol{\\rho}=\\left[\\begin{array}{ccc}1 &amp; \\ldots &amp; \\rho_{1 d} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{d 1} &amp; \\cdots &amp; 1\\end{array}\\right]\\) to have independent Normal increments \\[ \\mathbf{W}(t)-\\mathbf{W}(s) \\mid \\mathbf{W}(s) \\sim N_d(\\mathbf{0},(t-s) \\boldsymbol{\\rho}) \\] - Note: increments are independent over time, but can be dependent across dimensions! Increments are independent of the past, and follow a \\(d\\) dimensional normal with mean 0, with the corr mat being the cov matrix multiplied by time (proportional to time). 3-dimensional standard Brownian Motion 12.6.2 Multivariate Arithmetic BM \\(\\{\\mathbf{X}(t)\\} \\ w / \\operatorname{SDE} \\ d \\mathbf{X}(t)=\\mu d t+\\sigma d \\mathbf{W}(t)\\), where \\[ \\boldsymbol{\\mu}=\\left[\\begin{array}{lll} \\mu_1 &amp; \\cdots &amp; \\mu_d \\end{array}\\right]^{\\top}, \\boldsymbol{\\sigma}=\\left[\\begin{array}{lll} \\sigma_1 &amp; \\cdots &amp; \\sigma_d \\end{array}\\right]^{\\top} \\] \\(\\{\\mathbf{W}(t)\\} \\sim d\\)-dim. standard BM W/ correlations \\(\\rho\\) \\(\\mathbf{X}(t)-\\mathbf{X}(s) \\mid \\mathbf{X}(s) \\sim N_d((t-s) \\boldsymbol{\\mu},(t-s) \\boldsymbol{\\Sigma})\\), where \\[ \\boldsymbol{\\Sigma}=\\left[\\left\\{\\sigma_i \\sigma_j \\rho_{i j}\\right\\}_{i, j=1}^d\\right]=\\left[\\begin{array}{ccc} \\sigma_1^2 &amp; \\cdots &amp; \\sigma_1 \\sigma_d \\rho_{1, d} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_1 \\sigma_d \\rho_{1, d} &amp; \\cdots &amp; \\sigma_d^2 \\end{array}\\right]=\\left(\\boldsymbol{\\sigma}^{\\boldsymbol{\\top}}\\right) \\circ \\boldsymbol{\\rho} \\] How to go from a univariate uniform and invert to a multivariate distribution? We show for multivariate normal below: 12.7 Cholesky Factorization Analog for square root for positive definite matrices Simple way to generate correlated Normal variates from independent ones: If \\(\\mathbf{Z} \\sim N_d(\\mathbf{0}, \\mathbf{I})\\) and \\(\\boldsymbol{\\Sigma}=\\mathbf{L L}^{\\top}\\) is the Cholesky factorization of the covariance matrix \\(\\Sigma\\), then \\(\\mathbf{X}=\\mathbf{L} \\mathbf{Z} \\sim N_d(\\mathbf{0}, \\boldsymbol{\\Sigma})\\) - Note: \\(\\mathbf{L}\\) is lower diagonal \\(\\mathbf{L}=\\left[\\begin{array}{ccc}l_{11} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{d 1} &amp; \\cdots &amp; l_{d d}\\end{array}\\right]\\) - \\(\\mathbb{V}(L\\cdot Z) = \\underline{L} \\underbrace{ \\mathbb{V}(\\underline{Z}) }_{ I }\\underline{L}^{T}=\\underline{L}\\cdot \\underline{L}^{T} = \\underline{\\Sigma}\\) So the process goes: - Get \\(d\\) uniforms - Apply a linear transformation 12.7.1 Example \\[ \\begin{aligned} \\left[ \\begin{array}{c} W_{1} \\\\ W_{2} \\\\ W_{3} \\end{array} \\right] &amp;\\sim N\\left(\\underline{0}, \\left[ \\begin{array}{ccc} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 \\\\ 1 &amp; 2 &amp; 3 \\end{array} \\right] \\right)\\\\ \\\\ \\text{Let } L &amp;= \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{array} \\right] \\implies \\Sigma = L \\times L^{T} = \\left[ \\begin{array}{ccc} z_{1} \\\\ z_{1}+z_{2} \\\\ z_{1}+z_{2}+z_{3} \\end{array} \\right]\\\\ \\\\ \\implies Z_{i} &amp;\\sim N(0, \\Sigma) \\end{aligned} \\] 12.7.2 Multivariate Geometric BM \\(\\{\\mathbf{S}(t)\\} w / \\operatorname{SDE} d \\mathbf{S}(t)=\\mu \\circ \\mathbf{S}(t) d t+\\sigma \\circ \\mathbf{S}(t) d \\mathbf{W}(t)\\) - Solution given by \\(\\mathbf{S}(t)=\\exp \\{\\mathbf{X}(t)\\}\\) where \\[ d \\mathbf{X}(t)=\\left(\\boldsymbol{\\mu}-\\boldsymbol{\\sigma}^2 / 2\\right) d t+\\boldsymbol{\\sigma} d \\mathbf{W}(t) \\] Generate geometric BM variates as \\[ \\begin{aligned} &amp; \\mathbf{S}\\left(t_i\\right)=\\mathbf{S}\\left(t_{i-1}\\right) \\circ \\exp \\left\\{\\left(\\boldsymbol{\\mu}-\\frac{\\boldsymbol{\\sigma}^2}{2}\\right) \\Delta t+\\mathbf{L} \\mathbf{Z}_i \\sqrt{\\Delta t}\\right\\}, i=1, \\ldots, m \\\\ &amp; \\text { where } \\mathbf{Z}_i \\sim^{i i d} N_d(\\mathbf{0}, \\mathbf{I}), \\mathbf{L} \\mathbf{L}^{\\top}=\\mathbf{\\Sigma}=\\left(\\boldsymbol{\\sigma} \\boldsymbol{\\sigma}^{\\top}\\right) \\circ \\mathbf{\\rho} \\end{aligned} \\] Example \\[ \\text { Geometric BM paths w/ } \\rho=\\left[\\begin{array}{ccc} 1 &amp; .4 &amp; .4 \\\\ .4 &amp; 1 &amp; .4 \\\\ .4 &amp; .4 &amp; 1 \\end{array}\\right] \\] Exchange option MC price \\[ \\left(\\begin{array}{l} S_1(0)=50, \\sigma_1=.4 \\\\ S_2(0)=35, \\sigma_2=.3 \\end{array}, r=3 \\%, \\rho=.3\\right) \\] "],["pricing-derivatives.html", "Chapter 13 Pricing Derivatives 13.1 Path Dependent Options 13.2 Simulating Geometric Brownian Motion (GBM) Paths 13.3 Monte Carlo for Barrier Options 13.4 Extrema of Brownian Motion 13.5 Time Discretization", " Chapter 13 Pricing Derivatives 13.1 Path Dependent Options First we’ll look at options where the price depends on (aspects of) the entire asset price path. Unlike a European option, whose payoff depends only on the asset price at expiry. 13.1.1 Barrier Options An example is a barrier option, where the payoff depends on whether the asset price crosses some barrier prior to expiry. At expiry the payoff will be similar to a call/put, but differs between whether the barrier is hit (activated/knocked out) 4 types of Barrier options: Up-and-out (U&amp;O): price starts below barrier &amp; has to move up for option to be knocked out Down-and-out (D&amp;O): price starts above barrier &amp; has to move down for option to be knocked out Up-and-in (U&amp;I): price starts below barrier &amp; has to move up for option to become activated Down-and-in (D&amp;I): price starts above barrier &amp; has to move down for option to become activated 13.1.1.1 Example Let C/P be price of plain Euro call/put option, CD&amp;O be that of Euro down-&amp;-out call, etc Find \\(C_{U\\&amp;O}\\) when barrier \\(B &lt; K\\) strike The option needs to reach \\(K\\) in order to have a positive pay off, but in order to reach that price, it needs to pass the barrier \\(B\\). However once it passes \\(B\\), then the call option is worthless. \\(\\therefore C_{U\\&amp;O} = 0\\) Find \\(P_{U\\&amp;I}+P_{U\\&amp;O}\\), where options have same B, K, T, etc We can create Put-Call Parity, the sum of both of them should be equivalent to the price of a normal European put \\(P=P_{U\\&amp;I}+P_{U\\&amp;O}\\). If under barrier, only \\(P_{U\\&amp;O}\\) has value, while if over the barrier, only \\(P_{U\\&amp;I}\\) has value. Similarly, \\(C=C_{U\\&amp;I}+C_{U\\&amp;O}\\) \\(P=P_{D\\&amp;I}+P_{D\\&amp;O}\\) \\(C=C_{D\\&amp;I}+C_{D\\&amp;O}\\) This is because having both of these options, it is equivalent to having an active option at all times. 13.1.1.2 Barrier options only depend on min/max \\[ M_{T} = max\\left\\{ S_{t} \\right\\} _{0\\leq t\\leq T} \\quad \\&amp; \\quad m_{T} = min\\left\\{ S_{t} \\right\\} _{0\\leq t\\leq T} \\] so where \\(\\mathbb{I}_{\\left\\{ M_{T}&lt;B \\right\\}}\\) is an indicator variable that the max of \\(S_{T}\\) over time \\(T\\) crosses \\(B\\) or not, \\(C_{U\\&amp;O} = e^{-rT}\\mathbb{E}\\left[ (S_{T}-K)_{+}\\mathbb{I}_{\\left\\{ M_{T}&lt;B \\right\\}} \\right]\\) We can simulate \\(S_{T}\\) and \\(M_{T}\\) dependently, as \\(S_{T}\\) is some random walk Use a Brownian Bridge to do it \\(C_{D\\&amp;O} = e^{-rT}\\mathbb{E}\\left[ (S_{T}-K)_{+}\\mathbb{I}_{\\left\\{ m_{T}&gt;B \\right\\}} \\right]\\) Want the minimum to be greater than the barrier for this option to keep it’s value. \\(C_{U\\&amp;I} = e^{-rT}\\mathbb{E}\\left[ (S_{T}-K)_{+}\\mathbb{I}_{\\left\\{ M_{T}&gt;B \\right\\}} \\right]\\) \\(C_{U\\&amp;O} = e^{-rT}\\mathbb{E}\\left[ (S_{T}-K)_{+}\\mathbb{I}_{\\left\\{ m_{T}&lt;B \\right\\}} \\right]\\) 13.2 Simulating Geometric Brownian Motion (GBM) Paths To price general path -dependent options, we need to simulate asset price paths \\(\\left\\{ S_{t} \\right\\}_{0\\leq t\\leq T}\\) In practice, we must discretize time: simulate asset price at #m points \\[ \\left\\{ S(t_{i}) \\right\\} ^{m}_{i=0} \\text{ where }t_{i} = i \\frac{T}{m} = i \\cdot \\Delta t, \\forall i = 1,\\dots ,m \\] Which allows us to create GBM \\(dS_{t} = rS_{t}dt+\\sigma S_{t}dW_{t}\\) (Risk-Neutral Measure) \\[ S(t_{i}) = S(t_{i-1})\\exp \\left\\{ \\left( r- \\frac{\\sigma^{2}}{2} \\right)\\Delta t + \\sigma \\sqrt{ \\Delta t }\\times Z_{i} \\right\\} \\text{where } \\begin{cases} \\Sigma t = \\frac{T}{m} \\\\ Z_{i} \\sim^{iid} N(0,1) \\\\ i=1,\\dots,m \\end{cases} \\] Discounted prices are martingales(?) Only the purple path above K at time \\(T\\) will have a non-zero payoff, the cyan path crossed the barrier above so it is out. The rest are under the strike and worthless. 13.3 Monte Carlo for Barrier Options The above will be biased, even if the number of iterations is large. This is because there may still be some time point that we did not simulate, the barrier could have been crossed and the value of the an option, ex \\(C_{U\\&amp;O}\\) would have been worthless. Hence the simulation will always overestimate the value of an option, because it underestimates the maximum. To address this bias, you could increase \\(m\\), and let it go to \\(\\infty\\), we could be more accurate but the computation would become increasingly expensive. There is a tradeoff between # paths(n) and # steps (m), \\[ n \\uparrow \\iff Var\\ \\downarrow, m \\uparrow \\iff bias \\ \\downarrow \\quad \\text{which represents the Bias/Variance Trade-off} \\] 13.3.1 Example: Find maximum by time T For a standard BM \\(\\left\\{ W_{t} \\right\\}\\) find the distribution of maximum by time T: -Assume std BM always starts at 0 \\[ \\begin{aligned} M_{t} &amp;= max\\left\\{ W_{t} \\right\\} _{0\\leq t\\leq T}\\quad \\text{WTF CDF }P(M_{T}\\leq y)\\\\ \\\\ \\text{First look at: for }&amp; x\\leq y, P(W_{T}\\leq x, M_{T}\\geq y) = \\underbrace{ P(W_{T}\\geq 2y-x, \\overbrace{ M_{T}\\geq y }^{ \\text{always true} }) }_{\\text{By relfection principle} }\\\\ \\end{aligned} \\] All paths that start at 0, cross above \\(y\\) at some point, and end at some value \\(x\\) which is below \\(y\\). By the reflection principle, the path is just as likely to have the reflected path. We first consider the first time the path hits \\(y\\), and we take the symmetric path to the existing one (equally likely as the true path). So the probability that the path ends below \\(x\\) and hits \\(y\\) at some point, we can just look at when it hits \\(y\\) and find when we cross \\((y-x)\\) above \\(y\\), which is equivalent to hitting \\(x\\) from \\(y\\) eventually. This quantity becomes \\(y+(y-x) = 2y-x\\). \\[ \\begin{aligned} \\text{For } x\\leq y,\\ \\underbrace{ P(W_{T}\\leq x, M_{T}\\leq y) }_{ P(A \\cap B) } &amp;= \\underbrace{ P(W_{T}\\leq x) }_{ P(A) } - \\underbrace{ P(W_{T}\\leq x, M_{T}\\geq y) }_{ P(A \\cap B^{C}) } \\\\ &amp;= P(W_{T}\\leq x) - P(W_{T}\\geq 2y-x)\\\\ \\\\ \\implies \\text{For } x=y, P(W_{T}\\leq y, \\overbrace{ M_{T}\\leq y }^{ \\implies W_{T} \\leq y }) &amp;= P(M_{T}\\leq y) = P(W_{T}\\leq y) - P(W_{T}\\geq 2y-y)\\\\ &amp;= P(W_{T}\\leq y)-P(W_{T}\\geq y) \\end{aligned} \\] We know the distribution of \\(W_{T}\\) is normal with \\(\\mu=0, \\sigma^{2}=T\\) (for Brownian motion), and because of symmetry of the Normal distribution, \\[ \\begin{aligned} &amp;= P(W_{T}\\leq y)-P(W_{T}\\leq -y)\\\\ &amp;= P(-y\\leq W_{T} \\leq y)\\\\ &amp;= P(|W_{T}| \\leq y) \\quad \\text{As $y$ will always be positive}\\\\ \\end{aligned} \\] \\[ \\begin{gathered} \\implies M_{T} \\sim |W_{T}| \\quad \\text{Which is the folded normal, only pos } x&gt;0 \\end{gathered} \\] The maximum is distributed just like the absolute value of the Std Brownian motion at time \\(T\\) 13.3.2 Example: Find Prob of W_T hitting barrier B by time T The probability of standard BM \\(\\left\\{ W_{t} \\right\\}\\) hits barrier \\(B=1\\) before time \\(T=1\\) is \\[ \\begin{aligned} M_{T} \\sim |W_{T}| &amp;\\implies P(\\text{BM hits 1 before } T=1)\\\\ &amp;= P(M_{1}\\geq 1) = P(\\overbrace{ |W_{T}| }^{ \\sim N(0,1) } \\geq 1)\\\\ &amp;= 2 P(Z \\geq 1) = 2 \\cdot\\overbrace{ \\Phi(-1) }^{ \\sim \\text{CDF of std Normal} }\\\\ &amp;= 0.317862 \\end{aligned} \\] 13.3.2.1 By simulation If we don’t know the reflection principle, and we wanted to simulate it instead: Simulate \\(m\\) random Normals \\(\\sim N\\left( 0, \\frac{1}{m} \\right)\\) and take their cumulative sum to create a path which ends at time 1 Do this for \\(n\\) paths Calculate the proportion of paths which had a max \\(M_{T}\\) above the barrier \\(B\\) This plot shows the trade off between granularity (\\(m\\)) and number of paths (\\(n\\)) when estimating the probability When \\(m\\) is low the bias is high but the variance is low, whereas on the left side the variance is really high but the bias is low (close to true prob) 13.3.3 Example: MC Simulation w/o bias Estimate probability that standard BM hits 1 before time 1, with MV but without bias? We can look at the distribution of maximum. We can generate values of \\(M_{T}\\) directly by generating a standard brownian motion \\(W_{T}\\) and setting \\(M_{T} = |W_{T}|\\). We then estimate the probability that \\(|W_{T}|\\) crosses 1, and as we generate more Normal RVs (\\(W_{T}\\)), the probability will converge. 13.4 Extrema of Brownian Motion However, one can easily simulate random deviates of maximum using Brownian bridge Construction allows for general treatment of extrema of various processes The reflection principle doesn’t work for arithmetic BM, due to drift. (The paths that go in the direction of the drift will have higher probability than the paths that oppose it.) If we fix the starting and ending point of our arithmetic BM however, we can still kind of use the reflection principle. Conditional on a final point \\(X_{T} = h\\) then the maximum \\((M_{T}|X_{T}) = max_{t}(X_{t}|X_{T})\\) of the Brownian bridge process has a Rayleigh distribution. \\[ P(M_{T}\\leq m | X_{T} = b) = 1 - \\exp \\left\\{ -2 \\frac{m(m-b)}{\\sigma^{2}T} \\right\\} \\quad \\forall m\\geq (0 \\cup B) \\] Note that distribution of conditional maximum is independent of the drift, given \\(X_{T}=b\\) The lower the ending point, the closer the max value is to 0, the starting point. 13.4.1 Procedure to generate maxima Procedure for simulating maxima of arithmetic BM: 1. Generate \\(X_T \\sim N\\left(\\mu T, \\sigma^2 T\\right)\\) 2. Generate \\(U \\sim \\operatorname{Uniform}(0,1)\\) 3. Calculate \\(M_T \\mid X_T=\\frac{X_T+\\sqrt{X_T^2-2 \\sigma^2 T \\log (U)}}{2}\\) For maxima of geometric BM, exponentiate arithmetic BM result 13.4.2 Example: U&amp;O Call 13.4.3 Example: Simulate minimum of arithmetic BM based on max If we have an arithmetic BM with some drift \\(\\mu\\) and volatility \\(\\sigma\\), we can simulate from the exact distribution the max of the process and the ending price. We can also use this to simulate the minimum by using symmetry. You’d simulate arithmetic BM paths with negative drift \\(-\\mu\\) and find it’s maximum. That would become minimum of the normal arithmetic BM with positive \\(\\mu\\). 13.5 Time Discretization What happens when a stochastic process which is not straight forward? (Not GBM or arithmetic?), for example with stochastic drift and stochastic volatility. The process is no longer log Normal, so we must use discretization. \\[ dS_{T} = \\mu_{t}dt + \\sigma_{t}dW_{t} \\] Path-dependent options generally require simulation of entire discretized path Exceptions are options depending on maximum (e.g. barrier, lookback) If prices do not follow GBM, it is not generally possible to simulate from exact distribution of asset prices Need to approximate sample path distribution over discrete times 13.5.1 Euler Discretization Consider general SDE where drift/volatility can depend on time \\((t)\\) and/or process (\\(St\\)) \\[ d S_t=\\mu\\left(t, S_t\\right) d t+\\sigma\\left(t, S_t\\right) d W_t \\] - There is no general explicit solution for \\(S_{t}\\) - Distribution of St is unknown (in closed form) - Notable exceptions are Arithmetic/Geometric BM - Behavior of \\(S_{t}\\) can be approximated using discretization scheme SDE: \\(d S_t=\\mu\\left(t, S_t\\right) d t+\\sigma\\left(t, S_t\\right) d W_t\\) Discretize time \\(t_i=i(T / m)=i \\Delta t, i=0, \\ldots, m\\) Simulate (approx.) path recursively, using \\[ \\begin{gathered} S\\left(t_i\\right)=S\\left(t_{i-1}\\right)+\\mu\\left(S\\left(t_{i-1}\\right), t_{i-1}\\right) \\Delta t+\\sigma\\left(S\\left(t_{i-1}\\right), t_{i-1}\\right) \\sqrt{\\Delta t} Z_i \\\\ \\quad \\text { for } i=1, \\ldots, m, \\text { where } Z_i \\sim^{i i d} N(0,1) \\end{gathered} \\] - To approximate distribution of \\(S(T)\\), generate multiple (#n) discretized paths - Method called Euler (or \\(1^{\\text {st }}\\) order) discretization "],["variance-reduction.html", "Chapter 14 Variance Reduction 14.1 Techniques", " Chapter 14 Variance Reduction Running simulation basically invokes SLLN. To sample the expectation of some RV \\(Z\\), we can sample a fixed \\(n\\) variables and the sample means will converge to \\(\\mathbb{E}(Z)\\) at the rate \\(\\frac{1}{\\sqrt{ n }}\\sigma\\) (scaled by the variance). We can converge faster simply by increase the number of samples, or we can decrease the variance. We’ll tackle the second method. 14.1 Techniques 14.1.1 Antithetic Variables Idea: For any Normal variate \\(Z_{i}\\) consider it’s negative \\(-Z_{i}\\). (It will follow the same distribution, if it’s centered at 0. If not, you can shift it) They come from the same distribution, but are dependent Generally, for Uniform (0,1), use \\(U_{i}\\) and \\(1-U_{i}\\) We can calculate the discounted payoff, denoted by \\(Y\\) under both: \\[ Y_{i} = f(Z_{i}), \\tilde{Y}_{i} = \\tilde{f}(-Z_{i}) \\] And then estimate price as \\[ \\bar{Y}_{AV} = \\frac{1}{2n} \\left( \\sum^{n}_{i=1}Y_{i} + \\sum^{n}_{i=1}\\tilde{Y}_{i} \\right) = \\frac{1}{n}\\sum^{n}_{i=1} \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\] Balance payoffs of paths with “opposite” returns. Instead of generating many pseudo random numbers, just take the antithetic (negative) variable (faster) We can find the asymptotic distribution of antithetic variable estimator in terms of moments of \\(\\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\) \\[ \\begin{gathered} \\bar{Y}_{AN} \\text{ is sample mean of iid } RV_i:\\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\\\ \\text{by CLT } \\bar{Y_{AN}} \\sim^{approx} N\\left( \\mathbb{E}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right], \\frac{1}{n} \\mathbb{V}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right] \\right)\\\\ \\end{gathered} \\] \\[ \\begin{aligned} \\text{where } \\mathbb{E}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right] &amp;= \\frac{1}{2}\\left[ \\mathbb{E}(Y_{i})+ \\mathbb{E}(\\tilde{Y}_{i}) \\right] \\\\ &amp;= \\frac{1}{2} 2 \\mathbb{E}(Y) = \\mathbb{E}\\left(f(Z) \\right) \\\\ \\mathbb{V}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right] &amp;= \\frac{1}{n} \\frac{1}{4}\\left( \\mathbb{V}[Y_{i}] + \\underbrace{ \\mathbb{V}[\\tilde{Y}_{i}] }_{ =\\mathbb{V}[Y_{i}] } + 2Cov(Y_{i},\\tilde{Y}_{i})\\right)\\\\ &amp;= \\frac{1}{n} \\frac{1}{4} \\left( 2\\mathbb{V}(Y_{i}) + 2 Cov(Y_{i}, \\tilde{Y}_{i}) \\right)\\\\ &amp;= \\frac{1}{n} \\frac{1}{2} \\left( \\mathbb{V}(Y_{i}) + Cov(Y_{i}, \\tilde{Y}_{i}) \\right)\\\\ \\end{aligned} \\] Antithetic variables may be simple and easy to generate but they won’t always help, although they will if the original and antithetic variables are negatively related. If they are unrelated/positively related, then you get worse results. Using the antithetic variable vs Generating twice the number of samples Proof that \\(\\mathbb{V}[\\bar{Y}_{AV}] &lt; \\mathbb{V}\\left[ \\frac{1}{2n} \\sum^{2n}_{i=1}Y_{i} \\right]\\) \\[ \\begin{aligned} \\mathbb{V}\\left[ \\frac{1}{2n} \\sum^{2n}_{i=1}Y_{i} \\right] &amp;= \\frac{1}{2n} \\mathbb{V}(Y)\\\\ \\mathbb{V}[\\bar{Y}_{AV}] &amp;= \\frac{1}{2n}\\cdot \\left( \\mathbb{V}(Y) + Cov(Y_{i}, \\tilde{Y}_{i}) \\right) \\\\ \\left( \\mathbb{V}(Y) + Cov(Y_{i}, \\tilde{Y}_{i}) \\right) &amp;\\leq \\mathbb{V}(Y) \\text{ iff } Cov(Y_{i}, \\tilde{Y}_{i}) \\leq 0 \\end{aligned} \\] This shows that the payoff is only worthwhile if \\(f(Z)\\) is not an even function, as if it was, then \\(\\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\) would simply be \\(Y_{i}\\) and that wouldn’t help us decrease varaince. For a European call, using the antithetic variables will give us improved results because the payoff of the European call is not an even function. There is an improvement in the accuracy of prediction using antithetic variables. Antithetic variables are a very crude way of estimation, but it’s easy to program. It ensures that you have as many numbers on one side of the distribution as the other. 14.1.2 Stratification This is inspired by statistical sampling. Idea: Split RV domain into equi-probable strata and draw equal number of variates from within each one. E.g. (2 strata) draw equal number of independent positive and negative \\(Z_{i}\\) Stratification ensures equal representation of each stratum in the RV domain Always reduces variance, but could be marginal improvements. It’s worth doing stratification when the target RV (payoff) changes over its domain. Like a payoff function that may be exponential. Why don’t we always use it? It’s expensive, as you may need to calculate conditional distribution, and many details that require work to figure out. However, for the Normal, a conditional Normal is still Normal, which makes things easy in that particular case. Otherwise, it may require numerical computation and may not be worth it. Consider \\(\\# m\\) equi-probable Normal strata \\(\\left\\{A_j\\right\\}\\) \\[ P\\left(Z \\in A_j\\right)=1 / m \\text { for } Z \\sim N(0,1), j=1, \\ldots, m \\] Stratified estimator of \\(Y=f(Z)\\) (payoff, but in general could be any function) \\[ \\begin{aligned} &amp; \\bar{Y}_{S t r}=\\frac{1}{m} \\sum_{j=1}^m \\bar{Y}^{(j)}, \\text { where } \\bar{Y}^{(j)}=\\frac{1}{n} \\sum_{i=1}^n f\\left(Z_i^{(j)}\\right) \\\\ &amp; Z_i^{(j)} \\sim^{\\text {iid }} N\\left(0,1 \\mid Z_i^{(j)} \\in A_j\\right), j=1, \\ldots, m \\end{aligned} \\] - \\(\\bar{Y}^{(j)}\\) is estimator within each stratum \\(j\\) We can verify that \\(\\bar{Y}_{Str}\\) is an unbiased estimator of \\(\\mathbb{E}(f(Z))\\). \\[ \\begin{aligned} \\mathbb{E}(\\bar{Y}_{Str}) &amp;= \\mathbb{E}\\left[ \\frac{1}{m}\\sum^{m}_{j=1}\\bar{Y}^{(j)} \\right] = \\frac{1}{m}\\sum^{m}_{j=1} \\mathbb{E}[\\bar{Y}^{j}]\\\\ &amp;= \\frac{1}{m} \\sum^{m}_{j=1} \\mathbb{E} \\left( \\frac{1}{n}\\sum^{n}_{i=1} Y_{j}^{(i)} \\right)\\\\ &amp;= \\frac{1}{m}\\sum^{m}_{j=1} \\frac{1}{n}\\sum^{n}_{i=1}\\underbrace{ \\mathbb{E}\\left[ Y_{i}^{(j)} \\right] }_{ \\mathbb{E}[Y | Z \\in A_{j}] = \\mathbb{E}\\left[ f(Z) | Z \\in A_{j} \\right] }\\\\ &amp;= \\frac{1}{m} \\sum^{m}_{j=1} \\frac{1}{n}n \\mathbb{E}[f(Z)|Z \\in A_{j}]\\\\ &amp;= \\frac{1}{m} \\sum^{m}_{j=1} \\mathbb{E}(f(Z)|Z \\in A_{j})\\\\ &amp;= \\sum^{m}_{j=1}\\mathbb{E}\\left[ f(Z) | Z \\in A_{j}\\right] \\cdot P(A \\in A_{j})\\\\ \\text{By Law of Tot. Prob }&amp;= \\mathbb{E}(f(Z)) \\cdot \\mathbb{E}(Y)\\\\ \\end{aligned} \\] We want to show that stratified sampling cannot do worse (higher variance) than simple random sampling ie \\(\\mathbb{V}(\\bar{Y}_{Str}) \\leq \\mathbb{V}(\\bar{Y})\\) where \\(\\bar{Y} = \\frac{1}{nm} \\sum^{nm}_{i=1}f(Z_{i})\\), with \\(n\\) numbers per \\(m\\) strata Stratification can’t always be used because it’s computationally expensive \\[ \\begin{aligned} \\mathbb{V}(\\bar{Y}) = \\mathbb{V}\\left( \\frac{1}{nm} \\sum^{nm}_{i=1} Y_{i} \\right) &amp;= \\frac{1}{nm} \\mathbb{V}(Y_{i})\\\\ &amp;= \\frac{1}{nm} \\left( \\mathbb{E}[Y_{i}^{2}] \\cdot [\\mathbb{E}(Y_{i})]^{2} \\right) \\\\ &amp;= \\frac{1}{nm} \\left[ \\mathbb{E}(f^{2}(z)) \\right] - \\mu^{2}\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\mathbb{V}(\\bar{Y}_{Str}) &amp;= \\mathbb{V}\\left( \\frac{1}{m} \\sum^{m}_{j=1} \\bar{Y}^{(j)} \\right) \\\\ &amp;= \\frac{1}{m^{2}} \\sum^{m}_{j=1} \\mathbb{V}(\\bar{Y}^{(j)})\\\\ &amp;= \\frac{1}{m^{2}} \\sum^{m}_{j=1} \\mathbb{V}\\left[ \\frac{1}{n} \\sum^{n}_{i=1} \\underbrace{ Y^{(j)} }_{ \\text{iid in each sample} } \\right] \\\\ &amp;= \\frac{1}{m^{2}n^{2}} \\sum^{m}_{j=1} \\sum^{n}_{i=1} \\mathbb{V}\\left( Y_{i}^{(j)} \\right) \\\\ &amp;= \\frac{1}{m^{2}n^{2}} \\sum^{m}_{j=1} n\\mathbb{V}(Y^{(j)})\\\\ &amp;= \\frac{1}{m^{2}n} \\sum^{m}_{j=1} \\mathbb{V}(\\underbrace{ Y }_{ f(z) }|z\\in A_{j})\\\\ &amp;= \\frac{1}{m^{2}n} \\sum^{m}_{j=1}\\left[ \\mathbb{E} \\left( f^{2}(z)|z \\in A_{j} \\right) - \\left( \\mathbb{E}(f(z))|z \\in A_{j} \\right)^{2} \\right] \\\\ &amp;= \\frac{1}{mn} \\left\\{ \\overbrace{ \\sum^{m}_{j=1}\\mathbb{E}\\left[ f^{2}(z)|z \\in A_{j} \\right] \\cdot \\underbrace{ \\frac{1}{m} }_{ P(z\\in A_{j}) } }^{ \\text{By LTP, } \\mathbb{E}[f^{2}(z)] } - \\frac{1}{m} \\sum^{m}_{j=1} \\underbrace{ \\left( \\mathbb{E}(f(z)|z \\in A_{j})^{2} \\right) }_{ \\mu_{j} } \\right\\}\\\\ &amp;= \\frac{1}{mn} \\left\\{ \\mathbb{E}\\left[ f^{2}(z) - \\frac{1}{m}\\sum^{m}_{j=1}\\mu_{j}^{2} \\right] \\right\\} \\\\ &amp;\\leq \\mathbb{V}\\left[ \\bar{Y} \\right] = \\frac{1}{mn}\\left\\{ \\mathbb{E}\\left[ f^{2}(z)-\\mu^{2} \\right] \\right\\} \\\\ &amp;\\iff \\frac{1}{m} \\sum^{m}_{j=1} \\mu_{j}^{2} \\geq \\mu^{2} \\end{aligned} \\] Which is true by Jensens Inequality. Note \\(\\mu = \\frac{1}{m} \\sum^{m}_{j=1}\\mu_{j} = \\sum^{m}_{j=1}\\underbrace{ \\mathbb{E}\\left[ f(z)|z\\in A_{j} \\right] }_{ \\mu_{j} }\\underbrace{ P(A_{j}) }_{ 1/m }\\) For convex functions: \\[ g(x) = x^{2}, \\text{then } \\mathbb{E}(g(X)) = \\underbrace{ \\mathbb{E}(X^{2}) }_{ \\frac{1}{m}\\sum^{m}_{j=1}\\mu_{j}^{2} } \\geq \\underbrace{ \\left( \\mathbb{E}(X) \\right)^{2} }_{ \\left( \\frac{1}{m}\\sum^{m}_{j=1}\\mu_{j} \\right)^{2} = \\mu^{2} } \\] Because we’re subtracting a larger value in the variance, then we get a smaller variance. If we have a payoff that has lots of jumps/large values, then this would give us lots of benefit. The more strata used, the bigger the benefit. This may be known as quasi Monte Carlo, but in the multivariate case it becomes a large pain. 14.1.3 Control Variates Comes from regression To estimate \\(\\mathbb{E}(Y) = \\mathbb{E}[f(z)]\\) using MC, we generate iid \\(Z_{i}\\) and use \\[ \\bar{Y} = \\sum^{n}_{i=1} \\frac{Y_{i}}{n} = \\sum^{n}_{i=1} \\left( \\frac{f(Z_{i})}{n} \\right) \\] For our purposes, \\(f(\\cdot)\\) is the option’s discounted payoff We assume there is another option with payoff \\(g(\\cdot)\\) whose price we already know \\(\\mathbb{E}(X) = \\mathbb{E}(g(Z))\\) Idea: Use MC with same variates to estimate both \\(\\mathbb{E}(Y)\\) and \\(\\mathbb{E}(X)\\), but adjust estimate \\(\\bar{Y}\\) to take into account the error of estimate \\(\\bar{X}\\) E.g., if \\(\\bar{X}\\) underestimates \\(\\mathbb{E}[X]\\) then we can adjust \\(\\bar{Y}\\) upward because it most likely also underestimates \\(\\mathbb{E}(Y)\\). For our purposes, we’ll focus on linear adjustments. 14.1.3.1 Example Adjust \\(\\bar{Y}\\) for estimation error \\(\\bar{X}-\\mathbb{E}(X)\\) linearly, as \\[ \\bar{Y}(b) = \\bar{Y}-b(\\bar{X}-\\mathbb{E}(X)) \\] No matter what value of \\(b\\) we choose, the estimator will be unbiased (even though it may inefficient) We will show that this is an unbiased estimator. \\[ \\begin{aligned} \\mathbb{E}(\\bar{Y}(b)) &amp;= \\mathbb{E}[\\bar{Y}-b(\\bar{X}-\\mathbb{E}(X))]\\\\ &amp;= \\underbrace{ \\mathbb{E}(\\bar{Y}) }_{ \\mathbb{E}(Y) } - b(\\underbrace{ \\mathbb{E}(\\bar{X}) }_{ \\mathbb{E}(X) }-\\mathbb{E}(X))\\\\ &amp;= \\mathbb{E}(Y) - b\\cdot(\\mathbb{E}(X)-\\mathbb{E}(X)) \\end{aligned} \\] On average the adjustment will always be 0, but for certain choices of \\(b\\), we can make the estimator much more efficient. We find \\(b\\) using regression on the random variables of \\(Y,X\\) that we’ve simulated. We can also find the variance of \\(\\bar{Y}(b)\\) \\[ \\begin{aligned} \\mathbb{V}[\\bar{Y}(b)] &amp;= \\mathbb{V}[\\bar{Y}-b(\\bar{X}-\\mathbb{E}(X))] = \\mathbb{V}[\\bar{Y}-b\\bar{X}]\\\\ &amp;= \\mathbb{V}(\\bar{Y}) + b^{2}\\mathbb{V}(\\bar{X})-2bCov(\\bar{Y},\\bar{X})\\\\ &amp;= \\frac{1}{n}\\mathbb{V}(Y)+b^{2} \\frac{1}{n}\\mathbb{V}(X) - 2bCov\\left( \\frac{1}{n}\\sum^{n}_{i=1}\\underbrace{ f(z_{i}) }_{ Y_{i} },\\ \\ \\frac{1}{n}\\sum^{n}_{i=1}\\underbrace{ g(z_{i}) }_{ X_{i} } \\right)\\\\ &amp;= \\frac{1}{n}\\underbrace{ \\mathbb{V}(Y) }_{ \\sigma^{2}_{y} }+b^{2}\\frac{1}{n}\\underbrace{ \\mathbb{V}(X) }_{ \\sigma^{2}_{x} }-2b \\frac{1}{n^{2}}nCov(f(Z)), g(Z))\\\\ &amp;= \\frac{1}{n}\\left[ \\sigma^{2}_{y}+b^{2}\\sigma^{2}_{x}-2b \\sigma_{xy}\\right] \\end{aligned} \\] We can also show that the optimal value of \\(b\\) is \\(b^{\\star}= \\frac{Cov(X,Y)}{Var[X]}\\) which is the regression slope coefficient \\[ \\begin{aligned} \\frac{ \\partial }{ \\partial b } \\mathbb{V}(\\bar{Y}(b)) = 0 \\implies&amp; \\frac{ \\partial }{ \\partial b } \\left( \\frac{1}{n} [\\sigma^{2}_{y}+b^{2}\\sigma^{2}_{x}-2b\\sigma_{xy}] \\right) = 0\\\\ b\\sigma^{2}_{x}-\\sigma_{xy}&amp;= 0\\\\ \\implies&amp; b = \\frac{\\sigma_{xy}}{\\sigma_{x}^{2}} = \\frac{Cov(X,Y)}{Var(X)} = \\left( \\frac{\\sigma_{xy}\\sigma_{x}\\sigma_{y}}{\\sigma_{x}^{2}} = \\sigma_{xy} \\frac{\\sigma_{y}}{\\sigma_{x}} \\right) \\end{aligned} \\] We want the expected val of \\(Y\\), which we can calculate by finding the above. In practice, don’t know \\(\\operatorname{Cov}[X, Y], \\operatorname{Var}[X]\\) so estimate \\(b^*\\) using MC sample \\[ \\hat{b}=\\frac{\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right)}{\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2} \\] We want to find \\(\\mathbb{E}(Y)\\) with the knowledge of what \\(\\mathbb{E}(X)\\) is. We can then estimate the correlation/slope. Furthermore, we now show that the optimal variance is \\(Var(\\bar{Y}(b^{\\star})=Var[\\bar{Y}](1-\\rho^{2}_{XY})\\) \\[ \\begin{aligned} \\mathbb{V}[\\bar{Y}(b^{\\star})] &amp;= \\frac{1}{n} (\\sigma_{y}^{2}-b^{\\star 2}\\sigma_{x}^{2}-2b^{\\star}\\sigma_{XY})\\\\ &amp;= \\frac{1}{n} \\left( \\sigma_{y}^{2}+\\left( \\rho_{XY} \\frac{\\sigma_{Y}}{\\sigma_{X}} \\right) \\sigma_{x}\\sigma_{Y}\\rho_{XY} \\right) \\\\ &amp;= \\frac{1}{n} \\left( \\sigma_{Y}^{2}+\\rho_{XY}^{2}\\sigma_{y}^{2} - 2\\rho^{2}_{XY}\\sigma_{Y}^{2} \\right) \\\\ &amp;= \\frac{1}{n}\\sigma_{Y}^{2}(1-\\rho_{XY}^{2})\\\\ &amp;= \\mathbb{V}(\\bar{Y})\\cdot(1-\\rho^{2}_{XY}) \\end{aligned} \\] - In practice, use sample estimates of \\(\\operatorname{Var}[\\bar{Y}], \\rho_{X Y}\\) Good control variates have high absolute correlation with option payoff (high \\(\\left.\\rho_{X Y}\\right)\\) ! We can apply this to price a European option 14.1.3.2 Example Price a European option using final asset price (\\(S_{T}\\)) as control assuming GBM w/ \\(r,\\sigma\\). We know: \\(X=S_{T}=g(Z)=S_{0}\\exp \\left\\{ \\left( r-\\frac{\\sigma^{2}}{2} \\right)T+\\sigma \\sqrt{ T }\\cdot Z \\right\\}\\) \\(\\mathbb{E}(X) = \\mathbb{E}(S_{T}) = S_{0}e^{rT}\\) We guess the correlation of control with the following: (Deep) In-the-money call (most likely to be exercised): \\(\\rho_{XY} \\approx 1\\) \\(Cov((S_{T}-K)_{+}, S_{T})\\) which is most likely positively correlated Out-of-the-money call: \\(\\rho_{XY}\\approx 0\\) \\(Cov(0, S_{T}) \\approx 0\\) In the money put: \\(\\rho_{XY} \\approx -1\\) Out of the money put: \\(\\rho_{XY} \\approx 0\\) This produced a closer estimate than simple MC, but it is much more complex to calculate (almost double the computation, as it uses twice as many samples). If we doubled the samples for simple MC, then the SE would drop by a factor of \\(\\frac{1}{\\sqrt{ n }}\\) 14.1.4 Importance Sampling Idea: Attempt to reduce variance by changing the distribution (probability measure) from which paths (random variates) are generated. Change measure to give more weight to important outcomes, thereby increasing sample efficiency E.g. for European call, put more weight to paths with positive payoff i.e., for which we exercise) Performance of importance sampling relies heavily on equivalent measure being used. We want to estimate \\(a=\\mathbb{E}_{\\phi}[f(Z)]=\\int _{z} f(z)\\phi(z) \\, dz\\) where \\(\\phi(z)\\) is the pdf of \\(Z\\) (Normal in this case) In the Simple MC: Generate sample \\(Z_{i} \\sim^{iid} \\phi, \\quad i=1,\\dots,n\\) and use \\(\\hat{\\alpha}=\\frac{\\sum^{n}_{i=1}f(Z_{i})}{n}\\) which by CLT and LLN it will converge to \\(\\mathbb{E}_{\\phi}(f(Z))\\) Assuming you ahve a same \\(Z&#39;_{i} \\sim^{iid} \\psi, \\quad i = 1,\\dots,n\\) from a new pdf \\(\\psi\\), you can still estimate \\(\\alpha\\) as follows: \\[ \\begin{gathered} \\alpha = \\int _{z} f(z) \\phi(z) \\, dz = \\int _{z} f(z) \\frac{\\phi(z)}{\\psi(z)} \\psi(z) \\, dz = \\mathbb{E}_{z}\\left[ f(Z&#39;) \\frac{\\phi(Z&#39;)}{\\psi(Z&#39;)} \\right]\\\\ \\implies\\hat{\\alpha} = \\frac{1}{n} \\sum^{n}_{i=1} f(Z&#39;_{i}) \\frac{\\phi(Z&#39;)}{\\psi(Z&#39;)} \\end{gathered} \\] Also used in some cases where you can’t simulate efficiently from \\(\\phi(Z)\\), but you have a way to calculate this density. You can simulate from a distribution that has a cdf \\(\\psi(Z)\\) instead, and has a method of simulating variates from. There is a way to simulate from a Normal distribution (which doesn’t have a CDF) other than this, he did not mention exactly what, nor should it be that important. 14.1.4.1 Importance Sampling is unbiased Proof that \\(\\hat{\\alpha&#39;}\\) is unbiased provided the simple MC estimate \\(\\hat{\\alpha}\\) is unbiased. \\[ \\begin{aligned} \\mathbb{E}_{\\psi}[\\hat{\\alpha&#39;}] &amp;= \\mathbb{E}_{\\psi} \\left[ \\frac{1}{n} \\sum f(Z_{i}&#39;) \\frac{\\phi(Z&#39;_{i})}{\\psi(Z&#39;_{i})} \\right]\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}_{\\psi} \\left[ f(Z&#39;_{i}) \\frac{\\phi(Z&#39;_{i})}{\\psi(Z&#39;_{i})} \\right]\\\\ &amp;= \\int _{-\\infty}^{\\infty} \\underbrace{ f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} }_{ value } \\underbrace{ \\psi(z&#39;) }_{ prob } \\, dz&#39;\\\\ &amp;= \\int ^{\\infty}_{-\\infty} f(z&#39;) \\phi(z&#39;) \\, dz&#39;\\\\ &amp;= \\mathbb{E}_{\\phi} \\left[ f(z) \\right] \\\\ &amp;= \\alpha \\end{aligned} \\] But when is this efficient (better than simple MC)? When the variance of \\(\\hat{\\alpha}&#39;\\) is lower \\[ \\begin{aligned} \\mathbb{V}_{\\psi}(\\hat{\\alpha}&#39;) &amp;= \\mathbb{V}_{\\psi} \\left[ \\frac{1}{n} \\sum^{n}_{i=1} f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\right] \\\\ &amp;= \\frac{1}{n} \\mathbb{V} \\left[ f(z&#39;_{i}) \\frac{\\phi(z&#39;_{i})}{\\psi(z&#39;_{i})} \\right] \\\\ &amp;= \\frac{1}{n} \\left\\{ \\mathbb{E}_{\\psi} \\left[ \\left( f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\right)^{2} \\right] - \\left( \\mathbb{E}_{\\psi} \\left[ f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\right] \\right)^{2} \\right\\} \\end{aligned} \\] Now we show that this variance is lower than \\(\\mathbb{V}_{\\phi}(\\hat{\\alpha})\\) iff \\(\\mathbb{E}_{\\phi} \\left[ f^{2}(Z) \\frac{\\phi(Z)}{\\psi(Z)} \\right] \\leq \\mathbb{E}_{\\phi}\\left[ f^{2}(Z) \\right]\\) \\[ \\begin{aligned} \\mathbb{V}_{\\psi}\\left[ \\hat{\\alpha}&#39; \\right] \\leq \\mathbb{V}_{\\phi}[\\hat{\\alpha}] &amp;\\iff \\frac{1}{n} \\left\\{ \\mathbb{E}_{\\psi}\\left[ \\left( f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\right)^{2} \\right] - \\alpha^{2} \\right\\} \\leq \\frac{1}{n} \\left\\{ \\mathbb{E}_{\\phi} \\left[ f^{2}(z) \\right] -\\alpha^{2} \\right\\} \\\\ &amp;\\iff \\mathbb{E}_{\\psi}\\left[ f^{2}(z&#39;) \\frac{\\phi^{2}(z&#39;)}{\\psi^{2}(z&#39;)} \\right] \\leq \\mathbb{E}_{\\phi} \\left[ f^{2}(z) \\right]\\\\ &amp;\\iff \\int f^{2}(z&#39;) \\frac{\\phi^{2}(z&#39;)}{\\psi^{2}(z&#39;)} \\psi(z&#39;) \\, dz&#39; = \\int f^{2}(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\phi(z&#39;) \\, dz&#39;\\\\ &amp;\\quad \\quad \\quad = \\mathbb{E}_{\\phi}\\left[ f^{2}(z) \\frac{\\phi(z)}{\\psi(z)} \\right] \\end{aligned} \\] We can even show that \\(\\mathbb{V}_{\\psi}[\\hat{\\alpha}] = 0\\) iff \\(\\psi(z) \\propto f(z)\\psi(z)\\) for positive \\(f\\). Then \\(\\psi(z) = \\frac{1}{c} f(z)\\phi(z) \\implies \\int \\psi(z) \\, dz = 1\\) \\(\\implies \\int \\frac{1}{c} f(z)\\phi(z) \\, dz = 1 \\implies \\int f(z)\\phi(z) \\, dz = \\mathbb{E}_{\\phi}[f(Z)]\\) We need the price of the option to be finite, or the integral will not be finite. If the \\(\\phi\\) idstirbution is completely proportional to the functions, then the normalizing value is exactly what we’re looking for. This is an extreme case, although the overall idea is that we want \\(\\psi\\) to be as similar as possible \\[ \\begin{aligned} \\mathbb{V}_{\\psi} [\\hat{\\alpha}&#39;] &amp;= \\frac{1}{n} \\left\\{ \\mathbb{E}_{\\psi} \\left[ \\left( f(z&#39;) \\frac{\\phi(z&#39;)}{\\psi(z&#39;)} \\right)^{2} \\right] -\\alpha^{2} \\right\\} \\\\ &amp;= \\frac{1}{n} \\left\\{ \\mathbb{E}_{\\psi} \\left[ \\left( \\frac{f(z&#39;)\\phi(z&#39;)}{\\frac{1}{c} f(z&#39;)\\phi(z&#39;)} \\right)^{2} \\right] -\\alpha^{2} \\right\\}\\\\ &amp;= \\frac{1}{n} \\left\\{ \\underbrace{ \\mathbb{E}_{\\psi} \\left[ c^{2}\\right] }_{ \\alpha^{2} } -\\alpha^{2} \\right\\}\\\\ &amp;= \\frac{1}{n} \\left( \\alpha^{2} -\\alpha^{2}\\right) =0\\\\ \\end{aligned} \\] Importance sampling works best when new pdf \\(\\psi\\) “resembles” payoff \\(\\times\\) (original pdf) \\(f \\times \\varphi\\) 14.1.4.2 Extending to paths Importance sampling can be extended to multiple random variates per path - E.g. path-dependent option, with payoff \\(f\\left(Z_1, \\ldots, Z_m\\right)\\) a function of \\(\\# m\\) variates forming discretized path \\[ E_{\\varphi}\\left[f\\left(Z_1, \\ldots, Z_m\\right)\\right]=E_\\psi\\left[f\\left(Z_1^{\\prime}, \\ldots, Z_m^{\\prime}\\right) \\frac{\\varphi\\left(Z_1^{\\prime}, \\ldots, Z_m^{\\prime}\\right)}{\\psi\\left(Z_1^{\\prime}, \\ldots, Z_m^{\\prime}\\right)}\\right] \\] - If in addition, \\(Z_j \\sim^{i i d} \\varphi_j \\&amp; Z_j^{\\prime} \\sim^{i i d} \\psi_j\\), then \\[ E_{\\varphi}\\left[f\\left(Z_1, \\ldots, Z_m\\right)\\right]=E_\\psi\\left[f\\left(Z_1^{\\prime}, \\ldots, Z_m^{\\prime}\\right) \\prod_{j=1}^m \\frac{\\varphi_j\\left(Z_j^{\\prime}\\right)}{\\psi_j\\left(Z_j^{\\prime}\\right)}\\right] \\] 14.1.5 Ex. Deep OOTM European Call \\(S_{0} = 50, K = 65\\) Find the price of the option using simple MC Generate final prices from the log Normal distribution With simple MC, generate final prices as \\[ S_T=S_0 e^Z, \\text { where } Z \\sim \\varphi=N\\left(\\left(r-\\frac{\\sigma^2}{2}\\right) T, \\sigma^2 T\\right) \\] What would be a good candidate for \\(\\psi\\) ? \\[ Z^{\\prime} \\sim \\psi=N\\left(\\log \\left(\\frac{90}{50}\\right)-\\frac{\\sigma^2}{2} T, \\sigma^2 T\\right) \\text { or } N\\left(\\log \\left(\\frac{30}{50}\\right)-\\frac{\\sigma^2}{2} T, \\sigma^2 T\\right) \\] Only diff is the mean, but the first distrbution has a higher mean, which is what we want to simluate from as we want the final price to be closer to higher values (65) Most of the values simulated in the black distribution (RN measure) you would get 0 payoff, except in the tail. We want to shift the distribution so most of the simulations give us a positive pay off, such as in the green distribution. This will do much better in simulation. The blue distribution is the payoff x RN measure that we ideally want to simulate (but we can’t actually get since \\(\\phi\\) is hard to get?) "],["optimization.html", "Chapter 15 Optimization 15.1 Role Optimization 15.2 Types of Optimization 15.3 Stochastic Optimization", " Chapter 15 Optimization 15.1 Role Optimization Most real-world problems involve making decision, often under uncertainty. In finance, we must typically decide how to invest over time and across assets. Making good/optimal decisions typically involves some optimizations, such as mean-variance analysis or Kelly criterion. 15.2 Types of Optimization Different types of optimization problems: Straightforward (closed-form or polynomial complexity): Linear, Quadratic, Convex Equality/linear/convex constraints Difficult Discrete optimization (discrete variable) E.g. individisble assets, transaction costs Dynamic optimization (previous decisions affect future ones) Investing over time Stochastic optimization (uncertainty) 15.2.1 Example Imagine you could foresee the price of a stock. You want to make optimal use of such knowledge, assuming: You can only trade integer units of an asset Every transaction costs you a fixed amount You cannot short-sell the asset This is a discrete, dynamic optimization problem. There is no randomness, we have perfect knowledge. Nevertheless, the issue is not trivial. State Time no position long position t = 1 \\(V_{np}(1) = max(0 + V_{np}(2), -S(1)-tc + V_{lp}(2))\\) XXX t=2 \\(V_{np}(2) = max(0 + V_np(3), -S(2) - tc + V_{lp}(3))\\) \\(V_{lp} = max(0 + V_{lp}(3), S(2)- tc + V_{np}(3))\\) t=3=n \\(V_{np}(3) = 0\\) \\(V_{lp}(3) = S(s) - tc\\) t &gt;n \\(0\\) XXX The result of our strategy In reality, we won’t have perfect information like this, which means we need to include some randomness. This leads up to stochastic optimization 15.3 Stochastic Optimization We’ll first introduce the binomial model, and assume prices follow this tree with some probabilities. We want to find the best trading strategy that maximizes the expected P/L.. We define: State: \\(X_{t}\\) is a RV (contains the price \\(S_{t}\\) and position) Action: \\(a_{t}\\) is a change in the state, such as buying/selling Reward: \\(F_{t}\\) is some reward function, such as cashflow [](Notes/Obsidian-Attachments/13-Optimization-in-Finance-3.png|300]] To maximize the expected reward over stochastic actions, let \\(V(t, X_{t})\\) be the optimal value function: \\[ \\begin{aligned} V\\left(t, X_t\\right) &amp; =\\max _{a_{t \\rightarrow T}}\\left\\{\\mathbb{E}\\left[\\sum_{s=t}^T f\\left(s, X_s, a_s\\right) \\mid X_t\\right]\\right\\} \\\\ &amp; =\\max _{a_{t \\rightarrow T}}\\left\\{f\\left(t, X_t, a_t\\right)+\\mathbb{E}\\left[\\sum_{s=t+1}^T f\\left(s, X_s, a_s\\right) \\mid X_t\\right]\\right\\} \\\\ &amp; =\\max _{a_t}\\left\\{f\\left(t, X_t, a_t\\right)+\\max _{a_{(t+1) \\rightarrow T}}\\left\\{\\mathbb{E}\\left[\\sum_{s=t+1}^T f\\left(s, X_s, a_s\\right) \\mid X_t\\right]\\right\\}\\right\\} \\\\ &amp; =\\max _{a_t}\\left\\{f\\left(t, X_t, a_t\\right)+\\mathbb{E}\\left[\\max _{a_{(t+1) \\rightarrow T}}\\left\\{\\mathbb{E}\\left[\\sum_{s=t+1}^T f\\left(s, X_s, a_s\\right) \\mid X_{t+1}^{\\left(a_t\\right)}\\right]\\right\\} \\mid X_t\\right]\\right\\} \\\\ &amp; =\\max _{a_t}\\left\\{f\\left(t, X_t, a_t\\right)+\\mathbb{E}\\left[V\\left(t+1, X_{t+1}^{\\left(a_t\\right)}\\right) \\mid X_t\\right]\\right\\} \\end{aligned} \\] Essentially, instead of maximizing over the entire sequence of options, you use backward induction to maximize one step at a time. 15.3.1 Example "],["problem-set-1.html", "Chapter 16 Problem Set 1 16.1 Q1 16.2 Q2 16.3 Q3 16.4 Q4", " Chapter 16 Problem Set 1 16.1 Q1 16.1.1 Data Prep Loading libraries required: library(tidyverse) library(ggplot2) library(plotly) library(lubridate) Settings column names for our data (as per the demo file) msg_columns &lt;- c( &quot;Time&quot; , &quot;Type&quot; , &quot;OrderID&quot; , &quot;Size&quot; , &quot;Price&quot; , &quot;TradeDirection&quot; ) ordr_columns &lt;- c(&quot;ASKp1&quot; , &quot;ASKs1&quot; , &quot;BIDp1&quot;, &quot;BIDs1&quot;) # Levels nlevels = 10; # naming the columns of data frame if (nlevels &gt; 1) { for ( i in 2:nlevels ) { ordr_columns &lt;- c (ordr_columns,paste(&quot;ASKp&quot;,i,sep=&quot;&quot;), paste(&quot;ASKs&quot;,i,sep=&quot;&quot;), paste(&quot;BIDp&quot;,i,sep=&quot;&quot;), paste(&quot;BIDs&quot;,i,sep=&quot;&quot;)) } } Reading in the data message = read_csv(paste0(path,&quot;/AMZN_2012-06-21_34200000_57600000_message_10.csv&quot;), col_names=msg_columns) order_book = read_csv(paste0(path,&quot;/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv&quot;), col_names=ordr_columns) head(message) ## # A tibble: 6 × 6 ## Time Type OrderID Size Price TradeDirection ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 34200. 5 0 1 2238200 -1 ## 2 34200. 1 11885113 21 2238100 1 ## 3 34200. 1 3911376 20 2239600 -1 ## 4 34200. 1 11534792 100 2237500 1 ## 5 34200. 1 1365373 13 2240000 -1 ## 6 34200. 1 11474176 2 2236500 1 head(order_book) ## # A tibble: 6 × 40 ## ASKp1 ASKs1 BIDp1 BIDs1 ASKp2 ASKs2 BIDp2 BIDs2 ASKp3 ASKs3 BIDp3 BIDs3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2239500 100 2.23e6 100 2.24e6 100 2.23e6 200 2.24e6 220 2.23e6 100 ## 2 2239500 100 2.24e6 21 2.24e6 100 2.23e6 100 2.24e6 220 2.23e6 200 ## 3 2239500 100 2.24e6 21 2.24e6 20 2.23e6 100 2.24e6 100 2.23e6 200 ## 4 2239500 100 2.24e6 21 2.24e6 20 2.24e6 100 2.24e6 100 2.23e6 100 ## 5 2239500 100 2.24e6 21 2.24e6 20 2.24e6 100 2.24e6 100 2.23e6 100 ## 6 2239500 100 2.24e6 21 2.24e6 20 2.24e6 100 2.24e6 100 2.24e6 2 ## # … with 28 more variables: ASKp4 &lt;dbl&gt;, ASKs4 &lt;dbl&gt;, BIDp4 &lt;dbl&gt;, BIDs4 &lt;dbl&gt;, ## # ASKp5 &lt;dbl&gt;, ASKs5 &lt;dbl&gt;, BIDp5 &lt;dbl&gt;, BIDs5 &lt;dbl&gt;, ASKp6 &lt;dbl&gt;, ## # ASKs6 &lt;dbl&gt;, BIDp6 &lt;dbl&gt;, BIDs6 &lt;dbl&gt;, ASKp7 &lt;dbl&gt;, ASKs7 &lt;dbl&gt;, ## # BIDp7 &lt;dbl&gt;, BIDs7 &lt;dbl&gt;, ASKp8 &lt;dbl&gt;, ASKs8 &lt;dbl&gt;, BIDp8 &lt;dbl&gt;, ## # BIDs8 &lt;dbl&gt;, ASKp9 &lt;dbl&gt;, ASKs9 &lt;dbl&gt;, BIDp9 &lt;dbl&gt;, BIDs9 &lt;dbl&gt;, ## # ASKp10 &lt;dbl&gt;, ASKs10 &lt;dbl&gt;, BIDp10 &lt;dbl&gt;, BIDs10 &lt;dbl&gt; Setting up the X-axis (time of day) and Y-axis (price) variables # Trading hours (start &amp; end) startTrad = 9.5*60*60 # 9:30:00.000 in ms after midnight endTrad = 16*60*60 # 16:00:00.000 in ms after midnight # Define interval length freq = 5*60; # Interval length in ms 5 minutes # Number of intervals from 9:30 to 4:00 no_int= (endTrad-startTrad)/freq # Convert prices into dollars # Note: LOBSTER stores prices in dollar price times 10000 # Sample rows closest to 5 minute increments message_data = message %&gt;% mutate(Price = Price / 10000, idx = row_number(), five_min_grp = ((Time - startTrad) %/% freq) * freq + startTrad) %&gt;% group_by(five_min_grp) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% arrange(idx) %&gt;% # Format the Time column to be understandable, # needed to add 4 hours to be in right time zone mutate(X_Time = (Time + 4*60*60) %&gt;% as.POSIXct.numeric(origin = &#39;2012-06-21&#39;) %&gt;% lubridate::round_date(unit=&#39;minute&#39;)) order_data = order_book %&gt;% mutate(idx = row_number()) price = order_data %&gt;% dplyr::select(contains(&quot;p&quot;),idx) %&gt;% pivot_longer(!idx, # First column is Order Type, takes first capture group, second # capture group is excluded, third capture group of levels # becomes col names, indicated by .value names_to = c(&quot;Order Type&quot;, &quot;.value&quot;), names_pattern = &quot;(...)(?:p)(\\\\d{1,2})&quot;) %&gt;% # need to pivot longer again, but excluding Order Type and row idx # this pivots the levels into its own column pivot_longer(!c(`Order Type`,idx), names_to = c(&quot;Level&quot;), values_to= &quot;Price&quot;) quantity = order_data %&gt;% dplyr::select(!contains(&quot;p&quot;), idx) %&gt;% pivot_longer(!idx, names_to = c(&quot;Order Type&quot;, &quot;.value&quot;), names_pattern = &quot;(...)(?:s)(\\\\d{1,2})&quot;) %&gt;% pivot_longer(!c(`Order Type`,idx), names_to = c(&quot;Level&quot;), values_to= &quot;Quantity&quot;) %&gt;% dplyr::select(Quantity) order_longer = price %&gt;% bind_cols(quantity) %&gt;% mutate(Limit_Price = Price / 10000) %&gt;% filter(idx %in% message_data$idx) %&gt;% dplyr::select(-Price) plt_data = order_longer %&gt;% left_join(message_data %&gt;% dplyr::select(Time, Price, idx, X_Time), by=&quot;idx&quot;) 16.1.2 Plot (plt_data %&gt;% ggplot(aes(x=X_Time)) + geom_point(aes(y=Limit_Price, size=Quantity, color=`Order Type`)) + geom_line(aes(y=Price)) + scale_color_manual(values=c(&quot;red2&quot;, &quot;green3&quot;)) + scale_x_datetime(breaks = seq(min(plt_data$X_Time), max(plt_data$X_Time), by = 30 * 60), date_labels=&quot;%H:%M&quot;) + scale_y_continuous(labels = scales::dollar_format(prefix=&quot;$&quot;)) + labs(title = &quot;Evolution of AMZN Limit Order Book at 5-minute intervals&quot;, y=&quot;Price($)&quot;, x=&quot;Time of Day&quot;)) %&gt;% ggplotly() 16.2 Q2 R (Daily Return Data) Download the closing and adjusted closing daily prices for Apple Inc. (symbol APPL) for the previous year (Jan 1, 2022 to Dec 31, 2022). Use the the function get.hist.quote() from the tseries package. Loading Data library(tseries) aapl = get.hist.quote(&#39;aapl&#39;, start=&quot;2022-01-01&quot;, end=&quot;2022-12-31&quot;, quote=c(&quot;Close&quot;, &quot;Adjusted&quot;), compression=&quot;d&quot;) ## time series starts 2022-01-03 ## time series ends 2022-12-30 16.2.1 2.a Plot the closing and adjusted-closing price series on the same plot. Do you see any differences? plot(aapl$Close) lines(aapl$Adjusted, col=&quot;green4&quot;) It seems there are differences in the first half of the year. 16.2.2 2.b Calculate and plot the net returns based on the adjusted closing price. # we shouldn&#39;t have a value for Jan 3rd, because that was the first trading day # no way to calculate return from the day before. time series starts jan 4th net_ret = aapl / stats::lag(aapl, -1) - 1 log = log(aapl) - log(stats::lag(aapl, -1)) returns = cbind(net_ret, log) plot(net_ret$Adjusted) 16.2.3 2.c Apple paid a dividend of $0.23 on Nov 4, 2022 (such events are documented here). Calculate the net return of Nov 4, using both the adjusted closing prices (designated way), and the actual closing prices, where you manually adjust for the dividend (i.e., you add the dividend amount to the Nov 4 close price before calculating the log-return). Are the two returns equal? adj_nov4_ret = net_ret[&quot;2022-11-04&quot;]$Adjusted[[1]] act_nov4_ret = (aapl[&quot;2022-11-04&quot;]$Close + 0.23)[[1]] / aapl[&quot;2022-11-03&quot;]$Close[[1]] - 1 log_nov4_ret = log(aapl[&quot;2022-11-04&quot;]$Close + 0.23)[[1]] - log(aapl[&quot;2022-11-03&quot;]$Close)[[1]] # Net Return using adjusted close adj_nov4_ret ## [1] -0.001947376 # Net Return using actual close act_nov4_ret ## [1] -0.001944124 # Log Return using actual close log_nov4_ret ## [1] -0.001946017 They are the same up to 5 significant digits. 16.2.4 2.d Consider a simple momentum strategy whereby: - you buy APPLE stock for 1 day, if its price increased by more than 2% on the previous day, and - you (short) sell APPLE stock for 1 day, if its price decreased by more than 2% on the previous day Calculate and plot the daily and cumulative net returns of this strategy. # If return at t-1 &gt; 2%, buy APPLE for 1 day, sell next day # If return at t-1 &lt; 2%, short sell APPLE for 1 day, buy back next day # Dates where AAPL went up 2% days_plus2 = time(returns[returns$Adjusted.log &gt; 0.02]) # Dates where AAPL went down 2% days_minus2 = time(returns[returns$Adjusted.log &lt; 0.02]) ## records the return of the day after a &gt;2% return return_2d_pos = stats::lag(returns,1)[days_plus2] return_2d_neg = stats::lag(returns,1)[days_minus2] return_2d = rbind(return_2d_pos, return_2d_neg) net_cum_2d_ret = cbind(&quot;Daily Returns&quot; = return_2d$Adjusted.log, &quot;Cumulative Returns&quot; = cumsum(return_2d$Adjusted.log)) (net_cum_2d_ret %&gt;% ggplot(aes(x=time(net_cum_2d_ret))) + geom_line(aes(y=`Daily Returns`, color=&quot;one&quot;)) + geom_line(aes(y=`Cumulative Returns`,color=&quot;two&quot;)) + scale_color_discrete(name = &quot;Return Type&quot;, labels = c(&quot;Daily Net Return&quot;, &quot;Cumulative Net Return&quot;)) + labs(title=&quot;Momentum strategy returns&quot;)) %&gt;% ggplotly() 16.2.5 2.e Find the maximum drawdown of the strategy, i.e. the biggest drop in cumulative net returns from their highest point. You can use the maxdrawdown() function from the tseries package. maxdrawdown(net_cum_2d_ret$`Cumulative Returns`) ## $maxdrawdown ## [1] 0.3460492 ## ## $from ## [1] 58 ## ## $to ## [1] 247 16.3 Q3 A stock has an expected annual log-return of 8% and an annual volatility (i.e. st. deviation of annual log-return) of 20%. You want to model the daily log-return using a Normal Random Walk model. What should the mean and variance of the Normal distribution of the daily log-returns be, so that your model matches the annual parameters above? Assume that a calendar year has 252 trading days of daily returns. # num trading days num_days = 252 16.3.1 Mean of daily log-return Question implies \\(log(\\frac{P_{252}}{P_0}) = 0.08\\) We want the daily mean, so \\(log(\\frac{P_{252}}{P_{251}}) + ... + log(\\frac{P_1}{P_0}) = 0.08\\) assuming \\(log(\\frac{P_{252}}{P_{251}}) = ... = log(P_1/P_0)\\) implies \\(log(P_t/P_{t-1}) = 0.08/252 = 0.0003174603\\) Can also use formula from class \\[ E[r_{1-n}] = n\\mu \\\\ \\mu = E[r_{1-n}] / n \\] daily_mean = 0.08 / 252 16.3.2 Volatility of daily log-return Using formula from class \\[ \\sqrt{Var[r_{1-n}]} = \\sqrt{n}\\sigma\\\\ \\sigma = \\frac{sd[r_{1-n}]}{\\sqrt{n}} \\] # volatility/variance std_dev = 0.20 daily_std = std_dev / sqrt(num_days) 16.3.3 Plot set.seed(123) q3_ret = rnorm(num_days, daily_mean, daily_std) hist(q3_ret) plot(q3_ret, type=&quot;l&quot;) plot(cumsum(q3_ret), type=&quot;l&quot;) 16.4 Q4 Assume that the daily log returns on a stock with current price \\(S_0 = 100\\) are i.i.d. with a Normal distribution with mean 0.0003 and standard deviation 0.01. 16.4.1 4.a Calculate the probability that the price will be above $120 after 50 days. mean = 0.0003 std = 0.01 s_0 = 100 days = 50 # using the normal distribution, just need to find params for the 50 log-returns # and then find prob above 120 (ie. return is 20%) q4_50_day_mean = days * mean q4_50_day_std = std * sqrt(days) # probability prob_4a = pnorm(.20, q4_50_day_mean, q4_50_day_std, lower.tail = FALSE) prob_4a ## [1] 0.004444485 16.4.2 4.b Calculate the probability that the price will increase (i.e., have +ve return) in 30 or more of the next 50 days. Note: You will have to use (pnorm(), pbinom()) to find the numerical values of these probabilities. prob_4b = pbinom(30, 50, pnorm(0, mean, std, lower.tail = FALSE)) prob_4b ## [1] 0.9177322 "],["problem-set-2.html", "Chapter 17 Problem Set 2 17.1 Q1 17.2 Q2 17.3 Q3 17.4 Q4 17.5 Q5", " Chapter 17 Problem Set 2 17.1 Q1 An MLE can be derived from finding the derivative of the log \\(L(\\alpha)\\), \\(l(\\alpha)\\) function, where \\[ \\begin{aligned} L(\\alpha) &amp;= \\prod^{n}_{i=1} \\left[ \\frac{\\alpha x_{i}^{-(\\alpha+1)}}{\\ell^{-\\alpha}} \\right]\\\\ \\log L(\\alpha) = l(\\alpha) &amp;= \\sum^{n}_{i=1} \\left[ \\log(\\alpha) - (\\alpha+1)\\log(x_{i}) +\\alpha\\log(\\ell) \\right]\\\\ \\\\ l^{&#39;}(\\alpha) &amp;= \\sum^{n}_{i=1} \\left( \\frac{1}{\\alpha} - [\\log (x_{i}) + \\log(\\ell)]\\right) = 0\\\\ 0 &amp;= \\frac{n}{\\alpha} - \\sum^{n}_{i=1} \\left( \\log(x_{i}) -\\log(\\ell) \\right)\\\\ \\frac{n}{\\alpha} &amp;= \\sum^{n}_{i=1} \\left( \\log(x_{i}) -\\log(\\ell) \\right)\\\\ \\frac{1}{\\alpha} &amp;= \\frac{1}{n} \\left[ \\sum^{n}_{i=1}\\log\\left( \\frac{x_{i}}{\\ell} \\right) \\right]\\\\ \\\\ \\therefore \\hat{\\alpha} &amp;= \\left( \\frac{1}{n} \\left[ \\sum^{n}_{i=1}\\log\\left( \\frac{x_{i}}{\\ell} \\right) \\right] \\right)^{-1} \\end{aligned} \\] 17.2 Q2 17.2.1 Part A Heavy tail distributions are those with polynomial tails which follow by \\[f(x) \\propto x^{-(1+\\alpha)}\\] where smaller \\(\\alpha \\to\\) heavier tails We say that distribution \\(F\\) is heavy tailed if the integral on the positive half of \\(\\mathbb{R}^{+}\\) is infinite for some t \\[ \\begin{aligned} m_{X}(t) &amp;= \\mathbb{E}(e^{tX})\\\\ &amp;= \\sum^{\\infty}_{k=1}\\frac{t^{k}}{k!}\\mathbb{E}[X^{k}]\\\\ &amp;= \\left[ 1 + t\\mathbb{E}(X) + \\frac{t^2\\mathbb{E}(X^2)}{2!} + \\frac{t^3\\mathbb{E}(X^3)}{3!} + ... + \\frac{t^n\\mathbb{E}(X^n)}{n!} + ... \\right]\\\\ \\end{aligned} \\] But for any \\(n^{th}\\) moment where \\(n&gt;\\alpha\\), \\(\\mathbb{E}(X^{n})=\\infty\\). This implies that the mfg of any heavy tail distribution (which by definition has \\(\\alpha &gt;0\\)) will be infinite. 17.2.2 Part B \\[ \\begin{aligned} \\phi_{X}(t) &amp;= \\mathbb{E}[e^{itX}] = \\mathbb{E}[\\cos(tX) + i\\sin(tX)]\\\\ &amp;= \\mathbb{E}[\\cos(tX)] + i\\mathbb{E}[\\sin(tX)]\\\\ \\mathbb{E}[\\sin(tX)] &amp;= \\int ^{\\infty}_{-\\infty} \\sin(tX)f(x)\\, dx \\\\ &amp;= \\int ^{0}_{-\\infty} \\sin(tX)f(x)\\, dx + \\int ^{\\infty}_{0} \\sin(tX)f(x)\\, dx \\\\ &amp;= -\\int ^{\\infty}_{0} \\sin(tX)f(x)\\, dx + \\int ^{\\infty}_{0} \\sin(tX)f(x)\\, dx \\\\ &amp;= 0\\\\ \\therefore \\phi_{X}(t) &amp;= \\mathbb{E}[\\cos(tX)] \\text{ which only takes real values } \\end{aligned} \\] 17.2.3 Part C \\[ \\begin{aligned} \\phi_{X}(t) &amp;= \\mathbb{E}[e^{itX}]\\\\ \\phi_{Y}(t) &amp;= \\mathbb{E}[e^{itY}]\\\\ \\phi_{Z}(t) &amp;= \\mathbb{E}[e^{itZ}]\\\\ &amp;= \\mathbb{E}[e^{it(X+Y)}]\\\\ &amp;=\\mathbb{E}[e^{itX}e^{itY}]\\\\ &amp;=\\mathbb{E}[e^{itX}]\\mathbb{E}[e^{itY}] \\quad \\text{As they are independent}\\\\ &amp;= e^{-c|t|^{\\alpha}}\\cdot e^{-d|t|^{\\alpha}}\\\\ &amp;= e^{-(c+d)|t|^{\\alpha}} \\end{aligned} \\] Which shows that \\(\\phi_{Z}(t)\\) shares the same form as \\(X\\) and \\(Y\\). 17.2.4 Part D \\[ \\begin{aligned} \\phi_{X}(t) &amp;= \\mathbb{E}[e^{itX}] = e^{-|t|}\\\\ \\phi_{\\bar{X}}(t) &amp;= \\mathbb{E}[e^{it\\bar{X}}]\\\\ &amp;= \\mathbb{E}\\left[ e^{it \\frac{1}{n}\\left(\\sum^{n}_{i=1}X_{i} \\right)} \\right]\\\\ &amp;=\\mathbb{E}\\left[ e^{it\\frac{1}{n}X_{1}} e^{it\\frac{1}{n}X_{2}}\\dots e^{it\\frac{1}{n}X_{n}}\\right]\\\\ &amp;= \\mathbb{E}\\left[ e^{it\\frac{1}{n}X_{1}} \\right] \\mathbb{E}\\left[e^{it\\frac{1}{n}X_{2}}\\right] \\dots \\mathbb{E}\\left[e^{it\\frac{1}{n}X_{n}}\\right]\\\\ &amp;= [e^{-|t|/n}]^{n}\\\\ &amp;= e^{-|t|/n\\cdot n} = e^{-|t|} \\quad \\text{Which is exactly the $t(1)$ distribution} \\end{aligned} \\] 17.2.5 Part E According to theory, \\(\\mathbb{E}[X^{k}]=\\infty\\) for any \\(k\\geq \\alpha\\) where \\(\\alpha\\) is the tail index. If the tail index is greater than 2, then the second order moment exists, implying both the mean and variance both exist/are finite. By definition, a sum of stable distributions would converge to a stable distribution as shown by the characteristic functions above, but according to the Central Limit Theorem, any sum of iid random variables with finite mean and variance will produce a Normal distribution. 17.3 Q3 \\[ \\begin{aligned} Z&amp;=\\frac{X}{Y}\\\\ X &amp;\\sim Exp(1) \\quad Y \\sim Exp(1)\\\\ \\bar{F}_{X}(x) &amp;= 1-(1-e^{-x}) = e^{-x}\\quad x &gt; 0\\\\ \\bar{F}_{Y}(y) &amp;= 1-(1-e^{-y}) = e^{-y}\\quad y &gt; 0 \\end{aligned} \\] Show \\(Z\\) follows a heavy-tailed distribution, and find the tail index: \\(Z\\) follows a heavy-tailed distribution if it has polynomial tails ie \\(Z \\propto x^{-(1+\\alpha)}\\) We will integrate w.r.t \\(F_{Y}(y)\\), as we condition on the fact that \\(y&gt;0\\). We can also convert the probability to be of \\(\\mathbb{P}(X&lt;\\alpha y)\\) so that it becomes a function of \\(y\\), which will work in our integral. \\[ \\begin{aligned} \\bar{F}_{Z}(z) = \\mathbb{P}(Z &gt; z) &amp;= \\mathbb{P}\\left( \\frac{X}{Y} &gt; z, x &gt; 0, y &gt; 0\\right)\\\\ &amp;= \\int_{y=0}^{\\infty}\\mathbb{P}\\left( \\frac{X}{Y} &gt; z | Y = y \\right) \\, dF_{y}(y)\\\\ &amp;= \\int _{y=0}^{\\infty}\\underbrace{ \\mathbb{P}(X &gt; zy) }_{ e^{-zy} }\\underbrace{ f_{Y}(y) }_{ e^{-y} } \\, dy\\\\ &amp;\\quad \\quad \\quad \\ \\ \\text{as } Y = y\\\\ &amp;= \\int ^{\\infty}_{y=0} e^{-y(z+1)}\\, dy\\\\ &amp;= \\left[ \\frac{e^{-y(z+1)}}{-(z+1)} \\right]^{\\infty}_{y=0}\\\\ &amp;= 0 + \\frac{1}{z+1} = (z+1)^{-1} \\quad z &gt; 0\\\\ \\implies f_{Z}(z) &amp;= (z+1)^{-2} \\propto x^{-(1+\\alpha)} \\text{ Where } \\alpha = 1 \\end{aligned} \\] \\(\\therefore Z\\) is heavy tailed with tail index \\(\\alpha=1\\) 17.4 Q4 Libraries required: # install.packages(&quot;yahoofinancer&quot;) library(yahoofinancer) library(tidyverse) library(MASS) Obtain stock data from Yahoo using yahoofinancer methods: lto &lt;- Ticker$new(&#39;L.TO&#39;) lto_adj_close = lto$get_history(start = &#39;2015-01-01&#39;, end = &quot;2022-12-31&quot;, interval = &#39;1d&#39;) %&gt;% dplyr::select(date, adj_close) 17.4.1 Part A Calculate daily net returns, plot the returns, sample auto-correlation plot, and Normal QQ plot. head(lto_adj_close) ## date adj_close ## 1 2015-01-02 14:30:00 44.02380 ## 2 2015-01-05 14:30:00 43.10342 ## 3 2015-01-06 14:30:00 42.23924 ## 4 2015-01-07 14:30:00 42.65377 ## 5 2015-01-08 14:30:00 42.65377 ## 6 2015-01-09 14:30:00 41.93714 net_ret = lto_adj_close %&gt;% mutate(net_returns = (adj_close - lag(adj_close, 1)) / lag(adj_close, 1)) %&gt;% filter(!is.na(net_returns)) net_ret %&gt;% ggplot(aes(x = date, y = net_returns)) + geom_point() + labs(title=&quot;Daily returns of Loblaws 2015-Jan-01 - 2022-Dec-31&quot;) The plot of the daily net returns demonstrate volatility clutering right after the start of 2020. acf(net_ret$net_returns) There are no simple auto-correlations (current returns are not correlated with past returns) qqnorm(net_ret$net_returns,datax = TRUE) qqline(net_ret$net_returns,datax = TRUE) Based on the QQ plot, the distribution seems leptokurtic as the sample quantiles are more distributed than the theoretical, implying heavy tails. This supports the stylized fact that returns follow heavy tail distributions. 17.4.2 Part B fitted = fitdistr(net_ret$net_returns, &quot;t&quot;) mu = fitted$estimate[1] sigma = fitted$estimate[2] v = fitted$estimate[3] var = sigma^2 * (v / (v-2)) sample_mu = mean(net_ret$net_returns) sample_var = var(net_ret$net_returns) We have a fitted mean \\(\\mu\\) of 3.2239313^{-4} and variance \\(\\sigma^{2}\\) of 1.4635424^{-4}. This is compared to the sample mean \\(\\mu\\) of 5.6615327^{-4} and variance \\(\\sigma^2\\) of 1.3827263^{-4}. These differences can occur due to the fitted t-distribution smoothing away more extreme values. 17.4.3 Part C We will now compare two different return distribution approaches in an practical investment setting. Assume you invest all of your wealth in L.TO for 4 years (4 × 252 = 1008 days). Simulate 5,000 iterations of 1008 daily returns from the following models: 17.4.3.1 Part i set.seed(123) n = 5000 m = 1008 # create a matrix of 5000 simulations of 1008 daily returns R_mat = matrix(rnorm(n*m, sample_mu, sqrt(sample_var)), nrow = n, ncol = m) R_final = apply(1+R_mat, 1, cumprod) - 1 sim_norm_returns = R_final[,m] # max drawdown mdd = function(R){ return(tseries::maxdrawdown(R)$maxdrawdown) } MDD = apply(R_final, 1, mdd) 17.4.3.2 Part ii # using a t distribution # create a matrix of 5000 simulations of 1008 daily returns R_mat_t = matrix(rnorm( n*m, mu, sigma ) * sqrt( v / rchisq( n*m , df = v) ), nrow = n, ncol = m) R_final_t = apply(1+R_mat_t, 1, cumprod) - 1 sim_t_returns = R_final_t[,m] MDD_t = apply(R_final_t, 1, mdd) 17.4.3.3 Part iii library(fGarch) ## NOTE: Packages &#39;fBasics&#39;, &#39;timeDate&#39;, and &#39;timeSeries&#39; are no longer ## attached to the search() path when &#39;fGarch&#39; is attached. ## ## If needed attach them yourself in your R script by e.g., ## require(&quot;timeSeries&quot;) garch_model=garchFit(~garch(1,1), data=net_ret$net_returns, trace = FALSE) # fit GARCH(1,1) model GARCH.param=garch_model@fit$coef # fitted coefficients GARCH.spec=garchSpec(model=list(mu=GARCH.param[&#39;mu&#39;], # define GARCH model specification for simulation omega=GARCH.param[&#39;omega&#39;], alpha=GARCH.param[&#39;alpha1&#39;], beta=GARCH.param[&#39;beta1&#39;] )) R.garch = matrix( 0, nrow = n, ncol =m ) for(i in 1:n){ R.garch[i,]=as.numeric(garchSim(GARCH.spec,m)) } Rcm.garch = t( apply( 1+R.garch, MARGIN = 1, FUN = cumprod) - 1 ) Rfn.garch = Rcm.garch[,m] MDD.garch = apply(Rcm.garch, MARGIN = 1, FUN = mdd ) par(mfrow=c(2,3)) hist(sim_norm_returns) hist(sim_t_returns) hist(Rfn.garch) hist(MDD) hist(MDD_t) hist(MDD.garch) The final returns for t-distr. and GARCH gave lower negative values, but are comparable to those of Normal. In terms of maximum drawdown, the GARCH values have a smaller range, but are otherwise also comparable. Not that for for both metrics we combine/compound many returns, so the effects of heavy tails is suppressed. I definitely fucked up somewhere in these simulations…don’t know where though. 17.5 Q5 17.5.1 Part A n = 5000 # simulate from t distribution with df=1 plot( cumsum( rt(n , df = 1) ) / (1:n), type = &quot;l&quot;); abline(0,0, lty =2) If the WLLN held for this distribution, you would expect to see the average converge to 0. The cauchy distribution will not converge to 0 because of the extreme values we have sampled will always cause large shifts in the cumulative sum. 17.5.2 Part B x = rt(n , df = 1) y = rt(n , df = 1) w = x+y/2 # sort w so we can create a qq plot w_sorted = sort(w) cauchy_quantiles = qt(ppoints(n), df = 1) plot(w_sorted, cauchy_quantiles, xlab=&quot;Sample Quantiles&quot;, ylab=&quot;Theoretical Quantiles&quot;) abline(0,1) On average, the sample and theoretical quantiles agree. With heavy tailed distributions like the Cauchy, we can expect some (but few) extreme values like the outliers we see here. 17.5.3 Part C A simulation experiment demonstrating the first EVT (convergence of the max of RVs goes to one of three distributions) m = 1000 n = 100 partc = function(func, an, bn, ...){ mn = c() for(i in 1:m){ mn[i] = max(func(n,...)) } yj = (mn - bn)/an return(yj); } i = partc(runif, 1/n, 1) ii = partc(rnorm, 1/(n*dnorm(qnorm(1-1/n))), qnorm(1-1/n)) iii = partc(rt, n/pi, 0, df =1) # overlay theoretical distribution par(mfrow=c(2,2)) hist(-i, probability = T) x=seq(0,10,.01) lines(x, dexp(x), col = 2) hist(ii, probability = T) x=seq(-5,5,.01) lines(x, exp( - x - exp(-x)), col = 2) hist(iii, probability = T) x=seq(0,150,.01) lines(x, 1/x^2 * exp(- 1/x), col = 2) From answer key: &gt;In cases i. and ii. the convergence to the theoretical distribution is evident, but in iii. the histogram is not as informative because of the extreme values involved. For the last case, one can plot the histogram &amp; density of the log-transformed normalized maxima, which actually show the convergence. (Note: the log-transformation of the Frechet(\\(\\alpha=1\\)) is the Gumbel distribution.) I think it is incorrect, as I think the exponential transform of the Frechet(\\(\\alpha=1\\)) is the Gumbel distribution. The solutions continue on to show: hist( log(iii), probability = T, main = &quot;iii. log-transform&quot; ) x=seq(-2,15,.01) lines(x, exp( -x-exp(-x)), col = 2) legend( &quot;topright&quot;, lwd = 2, col = 1:2, c(&quot;log-Sample hist.&quot;,&quot;log-Frechet&quot;)) "],["problem-set-3.html", "Chapter 18 Problem Set 3 18.1 Q1 18.2 Q2 18.3 Q3 18.4 Q4", " Chapter 18 Problem Set 3 18.1 Q1 Easy copula question about certain configurations of the marginal distributions. In the first case, we have two exactly equal uniforms and the second case is when one uniform is the exact complement of the other. 18.1.1 Part A \\[ \\begin{aligned} U_{1} &amp;= U_{2} \\sim U(0,1)\\\\ \\implies C(u_{1},u_{2}) &amp;= \\mathbb{P}(U \\leq u_{1},U \\leq u_{2})\\\\ &amp;= \\mathbb{P}(U \\leq \\min(u_{1},u_{2}))\\\\ &amp;= \\min(u_{1},u_{2}) \\quad \\forall \\ u_{1},u_{2} \\in [0,1]\\quad \\text{by def of Uniform}\\\\ &amp;= \\bar{C}(u_{1},u_{2}) \\end{aligned} \\] 18.1.2 Part B \\[ \\begin{aligned} U_{1} &amp;= 1 - U_{2}\\\\ C(u_{1},u_{2}) &amp;= \\mathbb{P}(U_{1} \\leq u_{1}, U_{2} \\leq u_{2})\\\\ &amp;= \\mathbb{P}(1-U \\leq u_{1}, U \\leq u_{2})\\\\ &amp;= \\mathbb{P}(U \\geq 1-u_{1}, U \\leq u_{2})\\\\ &amp;= \\mathbb{P}(1-u_{1} \\leq U \\leq u_{2})\\\\ &amp;= \\begin{cases} 0 &amp; 1-u_{1} \\geq u_{2} \\implies 1\\geq u_{1}+u_{2}\\\\ u_{2}-(1-u_{1}) = u_{1}+u_{2}-1 &amp; 1-u_{1}\\leq u_{2} \\implies 1 \\leq u_{1}+u_{2} \\end{cases}\\\\ &amp;= \\max(u_{1}+u_{2}-1, 0)\\\\ &amp;= \\underline{C}(u_{1},u_{2}) \\end{aligned} \\] 18.2 Q2 This question concerns Archimedean copula’s, where instead of uniforms, it uses some convex generator function \\(\\phi\\). \\[ \\phi(u) = \\ln \\left( \\frac{1-\\theta(1-u)}{u} \\right) \\quad \\theta \\in [-1,1) \\] 18.2.1 Part A Closed form expression for the resulting bivariate copula: First we need to find the inverse, where instead of \\(y\\) being a function of \\(u\\), we have \\(u\\) is some function of \\(y\\): \\[ \\begin{aligned} y = \\phi(u) &amp;= \\ln \\left( \\frac{1-\\theta(1-u)}{u} \\right)\\\\ e^{y}&amp;= \\frac{1-\\theta(1-u)}{u}\\\\ ue^{y} &amp;= 1-\\theta(1-u)\\\\ ue^{y} &amp;= 1-\\theta +\\theta u\\\\ u(e^{y}-\\theta) &amp;= 1-\\theta\\\\ \\\\ \\implies \\phi ^{-1}(y) = u &amp;= \\frac{1-\\theta}{e^{y}-\\theta} \\end{aligned} \\] Now we can plug in the two variables: \\[ \\begin{aligned} C(u_{1}, u_{2}) &amp;= \\phi ^{-1}(\\phi(u_{1})+\\phi(u_{2}))\\\\ &amp;= \\phi ^{-1} \\left( \\ln \\left( \\frac{1-\\theta(1-u_{1})}{u_{1}} \\right) + \\ln \\left( \\frac{1-\\theta(1-u_{2})}{u_{2}} \\right) \\right)\\\\ &amp;= \\phi ^{-1}\\left( \\ln \\left( \\frac{1-\\theta(1-u_{1})}{u_{1}} \\cdot \\frac{1-\\theta(1-u_{2})}{u_{2}} \\right) \\right)\\\\ &amp;= \\frac{1-\\theta}{\\left( \\frac{1-\\theta(1-u_{1})}{u_{1}} \\cdot \\frac{1-\\theta(1-u_{2})}{u_{2}} \\right)-\\theta}\\\\ \\\\ &amp;\\text{Multiply top and bottom by } u_{1}\\cdot u_{2}\\\\ \\\\ &amp;= \\frac{(1-\\theta)\\cdot u_{1}\\cdot u_{2}}{\\left[ ((1-\\theta) + \\theta u_{1}) \\cdot ((1-\\theta) + \\theta u_{2}) \\right]-\\theta\\cdot u_{1}\\cdot u_{2}}\\\\ &amp;= \\frac{(1-\\theta)\\cdot u_{1}\\cdot u_{2}} {\\left[((1-\\theta)^{2} + (1-\\theta)\\theta u_{1} + (1-\\theta)\\theta u_{2}) + \\theta^{2} u_{1}u_{2} \\right] -\\theta\\cdot u_{1}\\cdot u_{2}}\\\\ &amp;= \\frac{(1-\\theta)\\cdot u_{1}\\cdot u_{2}} {(1-\\theta)\\left[(1-\\theta) + \\theta u_{1} + \\theta u_{2} \\right] + \\theta^{2} u_{1}u_{2} -\\theta\\cdot u_{1}\\cdot u_{2}}\\\\ &amp;= \\frac{(1-\\theta)\\cdot u_{1}\\cdot u_{2}} {(1-\\theta)\\left[(1-\\theta) + \\theta u_{1} + \\theta u_{2} \\right]- (1-\\theta)(\\theta u_{1} u_{2})}\\\\ &amp;= \\frac{u_{1}u_{2}} {1-\\theta + \\theta u_{1} + \\theta u_{2} - \\theta u_{1} u_{2}}\\\\ &amp;= \\frac{u_{1}u_{2}} {1-\\theta(1 - u_{1} - u_{2} + u_{1} u_{2})}\\\\ &amp;= \\frac{u_{1}u_{2}} {1-\\theta(1 - u_{1})(1-u_{2})}\\\\ \\end{aligned} \\] 18.2.2 Part B \\[ \\begin{aligned} M_{n} &amp;= \\max\\{U_{1},\\dots,U_{n}\\}\\\\ F_{M_{n}}(m) &amp;= P(M_{n} \\leq m) = P\\left( \\prod^{n}_{i=1} (U_{i} \\leq m) \\right)\\\\ &amp;= P(U_{1}\\leq m, \\dots, U_{n}\\leq m)\\\\ &amp;= C(U_{1}\\leq m, \\dots, U_{n}\\leq m)\\\\ &amp;= \\phi ^{-1}(\\phi(m)+\\dots +\\phi(m))\\\\ &amp;= \\phi ^{-1}(n\\phi(m))\\\\ &amp;= \\frac{1-\\theta}{e^{n\\phi(m)}-\\theta}\\\\ &amp;= \\frac{1-\\theta}{\\exp\\{n\\ln \\left( \\frac{1-\\theta(1-m)}{m} \\right)\\}-\\theta}\\\\ &amp;= \\frac{1-\\theta}{\\left( \\frac{1-\\theta(1-m)}{m} \\right)^{n}-\\theta}\\\\ \\end{aligned} \\] 18.3 Q3 We want to show that the Archimedean copula with \\(\\phi(u) = -\\log(u)\\) is exactly the independence copula. \\[ \\begin{aligned} y = \\phi(u) &amp;= -\\log(u) = \\log(u^{-1})\\\\ e^{y} &amp;= u^{-1}\\\\ e^{-y} &amp;= u = \\phi ^{-1}(y) \\end{aligned} \\] Professor’s solution has \\(\\phi ^{-1}=e^{y}\\) which makes is key to showing equality to the independence copula. I have yet to understand why that is the inverse generator function. \\[ \\begin{aligned} C(u_{1}, \\dots, u_{n}) &amp;= \\phi ^{-1}(\\phi(u_{1})+\\dots+\\phi(u_{n}))\\\\ &amp;= \\exp(\\log(u_{1})+\\dots + \\log(u_{n}))\\\\ &amp;= \\exp(\\log(u_{1} \\times\\dots \\times u_{n}))\\\\ &amp;= u_{1}\\times\\dots \\times u_{n}\\\\ &amp;= C_{indep} \\end{aligned} \\] It is easy to show that this holds for any logarithm with any base. 18.4 Q4 A convex combination of k joint CDFs is itself a joint CDF (finite mixture), but is a convex combination of k copula functions a copula function itself? The result can be shown using induction. A convex combination of two copula’s \\(C() = wC_{1}()+(1-w)C_{2}\\) is also a copula. We are also given that a convex combination of \\(k\\) joint CDFs is also a CDF, which means that any convex combination of Copula’s will still be some CDF. We only need to show that the marginals are Uniform(0,1) in order to show that it will be a copula. For 1D Uniform CDFs \\(F_{1}(u) = F_{2}(u) = u \\quad \\forall u \\in [0,1]\\) then \\[ F(u) = wF_{1}(u)+(1-w)F_{2}(u) = wu+(1-w)u = u \\quad \\forall\\ u \\in [0,1] \\] "],["problem-set-4.html", "Chapter 19 Problem Set 4 19.1 Q1.1 19.2 Q1.2 19.3 Q1.3 19.4 Q1.4 19.5 Q2.1 19.6 Q2.2 19.7 Q2.3 19.8 Q3", " Chapter 19 Problem Set 4 Q1-Q3 are from the SDAFE textbook, 16.11 Q4-Qsmth are from 16.10 19.1 Q1.1 Suppose that there are two risky assets, A and B, with expected returns equal to 2.3% and 4.5%, respectively. Suppose that the standard deviations of the returns are \\(\\sqrt{ 6 }\\%\\) and \\(\\sqrt{ 11 }\\%\\) and that the returns on the assets have a correlation of 0.17. 19.1.1 Part A What portfolio of A and B achieves a 3 % rate of expected return? \\[ \\begin{aligned} \\mathbb{E}(wA+ (1-w)B) &amp;= 0.03\\\\ &amp;= w\\mathbb{E}(A) + (1-w)\\mathbb{E}(B)\\\\ &amp;= w\\cdot .023 + (1-w)\\cdot.045\\\\ 0.03 &amp;= w(.023-.045) + .045\\\\ w &amp;= \\frac{.03-.045}{.023-.045}\\\\ w &amp;= 0.6818 \\end{aligned} \\] A portfolio made up of \\(68\\%\\) of asset A and \\(32\\%\\) of asset B will achieve a \\(3\\%\\) rate of expected return. 19.1.2 Part B What portfolios of A and B achieve a 5.5% standard deviation of return? Among these, which has the largest expected return? \\[ \\begin{aligned} \\mathbb{V}(R_{p}) &amp;= \\mathbb{V}(wA + (1-w)B)\\\\ &amp;= w^{2}\\mathbb{V}(A) + (1-w)^{2}\\mathbb{V}(B) + 2\\cdot w(1-w)Cov(A,B)\\\\ &amp;= w^{2}0.06 + (1-w)^{2}0.11 + w(1-w) 2\\cdot0.17\\cdot \\sqrt{ 0.06 \\cdot 0.11}\\\\ &amp;= w^{2}0.06 + (1-2w+w^{2})0.11 + (w-w^{2}) 2\\cdot 0.17\\cdot \\sqrt{ 0.06 \\cdot 0.11}\\\\ &amp;= w^{2}(6 + 11 - 2.7621) + w(2.7621 -2\\cdot 11) + 11\\\\ 0.055^{2} &amp;= 14.2379w^{2} -19.2379 w + 11\\\\ 0 &amp;= 14.2379w^{2} -19.2379 w + 11 - 5.5\\\\ w &amp;= 0.4108 \\text{ and } 0.9404 \\end{aligned} \\] As the portfolio with weight 41% in A will give asset B more weight, and because asset B has a higher expected return, this portfolio will have the largest expected return. 19.2 Q1.2 Suppose there are two risky assets, C and D, the tangency portfolio is 65% C and 35% D, and the expected return and standard deviation of the return on the tangency portfolio are 5% and 7%, respectively. Suppose also that the risk-free rate of return is 1.5%. If you want the standard deviation of your return to be 5%, what proportions of your capital should be in the risk-free asset, asset C, and asset D? \\[ \\begin{aligned} \\text{Let } w_{f} &amp;= \\text{weight of risk-free asset}\\\\ \\mathbb{V}(R_{p}) &amp;= \\mathbb{V}(\\underbrace{ w_{f}r_{f} }_{ 0 } + (1-w_{f})r_{tangency})\\\\ 5^{2}\\% &amp;= (1-w_{f})^{2}\\cdot 7\\%^{2}\\\\ \\frac{5\\%}{7\\%} &amp;= 1-w_{f}\\\\ 0.7142 &amp;= 1-w_{f}\\\\ \\implies w_{c} &amp;= .7142\\cdot .65 = 0.4642\\\\ w_{d} &amp;= .7142\\cdot .35 = 0.25\\\\ w_{f} &amp;= 0.2857\\\\ \\end{aligned} \\] But the solutions states: 2/7 = 0.2857 in risk-free, 0.4643 in C, and 0.2500 in D I have no idea how. Can’t spot my mistake : I’m an idiot, remember to square std dev to get variance and double check your work 🥴 19.3 Q1.3 19.3.1 Part A Suppose that stock A shares sell at $75 and stock B shares at $115. A portfolio has 300 shares of stock A and 100 of stock B. What are the weights w and 1−w of stocks A and B in this portfolio? \\[ \\begin{aligned} w &amp;= \\frac{75\\cdot 300}{75\\cdot 300 + 115 \\cdot 100} = 0.66176\\\\ \\\\ 1-w &amp;= 0.33824 \\end{aligned} \\] 19.3.2 Part B More generally, if a portfolio has \\(N\\) stocks, if the price per share of the \\(j\\)th stock is \\(P_j\\), and if the portfolio has \\(n_j\\) shares of stock \\(j\\), then find a formula for \\(w_j\\) as a function of \\(n_{1},\\dots,n_{N}\\) and \\(P_{1},\\dots,P_{N}\\). \\[ \\begin{aligned} w_{j} = \\frac{n_{j}\\cdot P_{j}}{\\sum^{N}_{i=1}n_{i}P_{i}} \\end{aligned} \\] 19.4 Q1.4 Let \\(R_{P}\\) be a return of some type on a portfolio and let \\(R_{1},\\dots, R_{N}\\) be the same type of returns on the assets in this portfolio. Is \\(R_P = w_{1}R_{1}+ \\dots + w_{N}R_{N}\\) true if \\(R_{P}\\) is a net return? Is this equation true if \\(R_{P}\\) is a gross return? Is it true if \\(R_{P}\\) is a log return? Justify your answers. \\[ \\begin{aligned} \\text{Gross Returns:}&amp;\\\\ R&#39;(t) &amp;= \\frac{S(t)}{S(t-1)}\\\\ \\text{Net Returns:}&amp;\\\\ R(t) &amp;= R&#39;(t)-1 = \\frac{S(t)-S(t-1)}{S(t-1)}\\\\ \\text{Log Returns:}&amp;\\\\ r(t) &amp;= \\log(R&#39;(t)) = \\log\\{S(t)\\}-\\log\\{S(t-1)\\}\\\\ \\end{aligned} \\] We have shown in class that this equation holds for net returns. For gross returns: \\[ \\begin{aligned} R&#39;(t) = R(t)+1 &amp;= w_{1}R_{1}(t) + \\dots + w_{N}R_{N}(t) + 1\\\\ &amp;= w_{1}R_{1}(t) + \\dots + w_{N}R_{N}(t) + (w_{1}+\\dots+w_{N})\\\\ &amp;= w_{1}(R_{1}(t)+1) + \\dots + w_{N}(R_{N}(t)+1)\\\\ &amp;= w_{1}R&#39;_{1}(t) + \\dots + w_{N}R&#39;_{N}(t) \\end{aligned} \\] So it holds for gross returns as well. \\[ \\begin{aligned} r(t) &amp;= \\log(R&#39;(t)) = \\log(w_{1}R&#39;_{1}(t) + \\dots + w_{N}R&#39;_{N}(t))\\\\ &amp;= \\log(w_{1}R&#39;_{1}(t)) \\times \\dots \\times \\log(w_{N}R&#39;_{N}(t))\\\\ &amp;= [\\log(w_{1}) + \\log(R&#39;_{1}(t))] \\times \\dots \\times [\\log(w_{N}) + \\log(R&#39;_{N}(t))]\\\\ &amp;= [\\log(w_{1}) + r_{1}(t)] \\times \\dots \\times [\\log(w_{N}) + \\log(r_{N}(t))]\\\\ &amp;\\ne w_{1}r_{1}(t) + \\dots + w_{N}R_{N}(t) \\end{aligned} \\] So it does not hold for log returns. The following is from 16.10 1-3 19.5 Q2.1 Write an R program to find the efficient frontier, the tangency portfolio, and the minimum variance portfolio, and plot on “reward-risk space” the location of each of the six stocks, the efficient frontier, the tangency portfolio, and the line of efficient portfolios. Use the constraints that −0.1 ≤ wj ≤ 0.5 for each stock. The first constraint limits short sales but does not rule them out completely. The second constraint prohibits more than 50 % of the investment in any single stock. Assume that the annual risk-free rate is 3 % and convert this to a daily rate by dividing by 365, since interest is earned on trading as well as nontrading days. Taken straight from the answer key library(readr) dat = read.csv(&quot;ProblemSets/datasets/Stock_Bond.csv&quot;, header = T) prices = cbind(dat$GM_AC, dat$F_AC, dat$CAT_AC, dat$UTX_AC, dat$MRK_AC, dat$IBM_AC) n = dim(prices)[1] returns = 100 * (prices[2:n, ] / prices[1:(n-1), ] - 1) pairs(returns) mean_vect = colMeans(returns) cov_mat = cov(returns) sd_vect = sqrt(diag(cov_mat)) M = length(mean_vect) library(quadprog) Amat = cbind(rep(1,M),mean_vect,diag(1,nrow=M),-diag(1,nrow=M)) muP = seq(min(mean_vect)+.02,max(mean_vect)-.02,length=10) muP = seq(.05,0.08,length=300) sdP = muP weights = matrix(0,nrow=300,ncol=M) for (i in 1:length(muP)) { result = solve.QP(Dmat=cov_mat,dvec=rep(0,M), Amat=Amat, c(1,muP[i],rep(-.1,M),rep(-.5,M)), meq=2) sdP[i] = sqrt(2*result$value) weights[i,] = result$solution } plot(sdP,muP,type=&quot;l&quot;,xlim=c(0,2.5),ylim=c(0,.1)) mufree = 3/365 points(0,mufree,cex=3,col=&quot;blue&quot;,pch=&quot;*&quot;) sharpe =( muP-mufree)/sdP ind = (sharpe == max(sharpe)) # locates the tangency portfolio weights[ind,] # weights of the tangency portfolio ## [1] -0.091181044 -0.002910879 0.335318542 0.383714329 0.319484849 ## [6] 0.055574204 lines(c(0,sdP[ind]),c(mufree,muP[ind]),col=&quot;red&quot;,lwd=3) points(sdP[ind],muP[ind],col=&quot;blue&quot;,cex=3,pch=&quot;*&quot;) ind2 = (sdP == min(sdP)) points(sdP[ind2],muP[ind2],col=&quot;green&quot;,cex=3,pch=&quot;*&quot;) ind3 = (muP &gt; muP[ind2]) lines(sdP[ind3],muP[ind3],type=&quot;l&quot;,xlim=c(0,.25), ylim=c(0,.3),col=&quot;cyan&quot;,lwd=3) text(sd_vect[1],mean_vect[1],&quot;GM&quot;) text(sd_vect[2],mean_vect[2],&quot;F&quot;) text(sd_vect[3],mean_vect[3],&quot;UTX&quot;) text(sd_vect[4],mean_vect[4],&quot;CAT&quot;) text(sd_vect[5],mean_vect[5],&quot;MRK&quot;) text(sd_vect[6],mean_vect[6],&quot;IBM&quot;) legend(&quot;topleft&quot;,c(&quot;efficient frontier&quot;,&quot;efficient portfolios&quot;, &quot;tangency portfolio&quot;,&quot;min var portfolio&quot;), lty=c(1,1,NA,NA), lwd=c(3,3,1,1), pch=c(&quot;&quot;,&quot;&quot;,&quot;*&quot;,&quot;*&quot;), col=c(&quot;cyan&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;), pt.cex=c(1,1,3,3) ) 19.6 Q2.2 If an investor wants an efficient portfolio with an expected daily return of 0.07%, how should the investor allocate his or her capital to the six stocks and to the risk-free asset? Assume that the investor wishes to use the tangency portfolio computed with the constraints \\(−0.1 \\leq w_j \\leq 0.5\\), not the unconstrained tangency portfolio. options(digits=3) # it divides by the risk free rate converted to daily but theres a typo, # should be 3/365 (unless this is the amt of trading days, but that would be 252) # omega = (.07 - muP[ind]) / (3/265 - muP[ind]) omega = (.07 - muP[ind]) / (3/365 - muP[ind]) omega ## [1] 0.0518 1-omega ## [1] 0.948 (1-omega)*weights[ind] ## [1] -0.08645 -0.00276 0.31794 0.36382 0.30292 0.05269 omega + sum((1-omega)*weights[ind]) ## [1] 1 19.7 Q2.3 Does this data set include Black Monday? Again, straight from the answer key. Yes, Black Monday was October 19, 1987 and data go from January 2, 1987 to Sept 1, 2006. Black Monday is the 202th day in the original data set or the 201st day of returns. If you look in the spread sheet you will see huge price declines that day. The returns that day were: returns[201,] [1] -21.0 -18.2 -21.7 -15.7 -13.0 -23.5 19.8 Q3 From answer key: library(tseries) library(zoo) tickers = c(&quot;ENB.TO&quot;,&quot;CP.TO&quot;,&quot;RCI-A.TO&quot;,&quot;TD.TO&quot;,&quot;L.TO&quot;) N = length(tickers) P=vector(&quot;list&quot;, N) # list for holding prices for (i in 1:N) { cat(&quot;Downloading &quot;, i, &quot; out of &quot;, N , &quot;\\n&quot;) P[[i]] = get.hist.quote(instrument = tickers[i], start = as.Date(&quot;2010-01-01&quot;), end=as.Date(&quot;2020-12-01&quot;), compression=&quot;m&quot;, quote = &quot;AdjClose&quot;, retclass = &quot;zoo&quot;, quiet = T) } ## Downloading 1 out of 5 ## Downloading 2 out of 5 ## Downloading 3 out of 5 ## Downloading 4 out of 5 ## Downloading 5 out of 5 # net returns R = sapply(P, FUN=function(x){ as.numeric(diff(x) / stats::lag(x, -1)) } ) colnames(R) = tickers # assign names 19.8.1 Part A Use only the first two stocks (ENB.TO and CP.TO), and calculate the sample means and (individual) sample variances of their returns. Consider the following hypothetical values for their correlation : \\(\\rho\\) = −1, −0.5, 0, +0.5, +1. For each value of \\(\\rho\\), calculate their corresponding 2D variance-covariance matrix and plot the risk-return profiles of portfolios combining the two assets with weights [w, (1 − w)], \\(\\forall w \\in [−1, 2]\\). Plot all profile curves on the same (\\(\\mu_p, \\sigma_p\\))-space, using a different color for each value of \\(\\rho\\). R2=R[,1:2] MU = colMeans(R2) SD = sqrt(diag(var(R2))) par(mfrow=c(1,1), mar=c(5, 4, 4, 2) + 0.1) plot(SD,MU,pch=16,cex=2, col=2, xlim=c(0,.15), ylim=c(0.005,.025)) abline(v=0, lty=2); abline(h=0, lty=2) w = seq(-2,+3,.01); W = cbind(w, 1-w) # weights MU.p = W %*% MU # portfolio means rho=c(-1, -.5, 0, .5, 1) # correlations for(i in 1:5){ COR=matrix(c(1,rho[i],rho[i],1),2,2) COV= COR * (SD%*%t(SD)) SD.p = sqrt(rowSums((W %*% COV)*W)) # portfolio st.dev. lines(SD.p, MU.p, type=&#39;l&#39;, lwd=2, col=i); } points(SD,MU,pch=16,cex=2, col=2) text(SD, MU, pch=16, colnames(R2), pos=c(1,3)) legend(&#39;topright&#39;, lwd=rep(2,5),col=1:5, c(&quot;rho=-1&quot;,&quot;rho=-.5&quot;,&quot;rho=0&quot;,&quot;rho=+.5&quot;,&quot;rho=+1&quot;) ) 19.8.2 Part B Consider all 5 stocks together now, and use the sample mean and sample variance-covariance matrix of their returns. Plot the efficient frontier and the capital market line on the same (\\(\\mu_p,\\sigma_p\\))-space and report the tangency portfolio weights. (Hint: adapt the code from Example 16.6 on p. 476 of SDAFE.) library(quadprog) COV=cov(R) MU=colMeans(R) SD=sqrt(diag(COV)) N=dim(R)[2] plot(SD, MU, pch=16, cex=1.2, col= 2, xlim=c(0,.1), ylim=c(0,.025)) abline(v=0, lty=2); abline(h=0, lty=2) text(SD, MU, tickers, cex=1, pos=4) Amat = cbind(rep(1,N),MU) mu.p = seq( -.005, .05,length=100) sd.p = mu.p; for (i in 1:length(mu.p)) { bvec=c(1,mu.p[i]) out=solve.QP(Dmat=2*COV,dvec=rep(0,N),Amat=Amat,bvec=bvec,meq=2) sd.p[i] = sqrt(out$value) } lines(sd.p,mu.p,type=&quot;l&quot;, lwd=2, col=2) # plot least variance portfolios mu.f = .002 # monthly risk-free interest rate COV.i=solve(COV) W.tang=COV.i%*%(MU-mu.f) / sum( COV.i%*%(MU-mu.f) ) mu.tang=sum(W.tang*MU) sd.tang=sqrt(sum( (COV %*% W.tang) * W.tang ) ) points( sd.tang, mu.tang, pch=15, cex=1.3, col=2) sharpe=(mu.tang-mu.f)/sd.tang abline(mu.f,sharpe,lwd=2,lty=2,col=2) The Sharpe ratio is 0.335, and the tangency portfolio weights are: round( t(W.tang), 4 ) ## ENB.TO CP.TO RCI-A.TO TD.TO L.TO ## [1,] 0.135 0.362 0.106 0.248 0.149 19.8.3 Part C Repeat the previous part b. (i.e. plot the efficient frontier and capital market line, and report the tangency portfolio weights) with the restriction that all weights are within the bounds \\(0 \\le w_i \\le 0.5,\\;\\; \\forall i = 1, \\ldots, 5\\). (Hint: adapt the code from Example 16.7 on p. 479 of SDAFE.) plot(SD, MU, pch=16, cex=1.2, col= 2, xlim=c(0,.1), ylim=c(.0,.025)) abline(v=0, lty=2); abline(h=0, lty=2) text(SD, MU, tickers, cex=1, pos=4) Amat = cbind(rep(1,N),MU,diag(1,nrow=N),-diag(1,nrow=N)) mu.pot = seq( min(MU), max(MU),length=300) # potential mean returns mu.p = NULL # initialize portofio standard error sd.p = NULL # initialize portofio standard error W.p = NULL # initialize portofio weights for (i in 1:length(mu.pot)) { bvec=c(1,mu.pot[i],rep(0,N),rep(-0.5,N)) #check whether potential mean return can be achieved with given constraints out=tryCatch( solve.QP(Dmat=2*COV,dvec=rep(0,N),Amat=Amat,bvec=bvec,meq=2), error=function(e) NULL) #if mean return is achievable, save its st.dev. &amp; portfolio weights if(!is.null(out)){ # if mu.p=c(mu.p, mu.pot[i]) sd.p=c(sd.p, sqrt(out$value)) W.p=rbind(W.p, out$solution) } } lines(sd.p,mu.p,type=&quot;l&quot;, lwd=2, col=2) # plot least variance portfolios colnames(W.p)=tickers sharpe=( mu.p-mu.f)/sd.p ind.tang=which.max(sharpe) W.tang=W.p[ind.tang,] sd.tang = sd.p[ind.tang] mu.tang = mu.p[ind.tang] points( sd.tang, mu.tang, pch=15, cex=1.3, col=2) abline( c(mu.f, sharpe[ind.tang]),lwd=2, col=2, lty=2) The set of feasible portfolios will be a subset of that of the unconstrained problem. Note that the constrained efficient frontier is not a parabola any more. But the constrained tangency portfolio is the same as the unconstrained one (since the since the unconstrained tangency portfolio weights were all within [0,.5]), and have (approximately) the same Sharpe ratio 0.335. Note that the constrained tangency portfolio weights are (approximately) the same as the unconstrained ones: round( W.tang, 4 ) ## ENB.TO CP.TO RCI-A.TO TD.TO L.TO ## 0.135 0.362 0.106 0.248 0.149 "],["problem-set-5.html", "Chapter 20 Problem Set 5 20.1 Q1 20.2 Q2 20.3 Q3 20.4 Q4", " Chapter 20 Problem Set 5 20.1 Q1 Assume a market of \\(N\\) assets with returns following the 1-factor CAPM model \\[ R_i=\\beta R_M+\\varepsilon_i, \\quad i=1, \\ldots, N \\] where \\(R_M \\sim \\mathrm{N}\\left(\\mu_M, \\sigma_M^2\\right)\\) and \\(\\varepsilon_i \\sim^{i . i . d .} \\mathrm{N}\\left(0, \\sigma_{\\varepsilon}^2\\right), \\quad \\forall i\\). Therefore, the model assumes all assets have the same systematic risk \\(\\left(\\beta^2 \\sigma_M^2\\right)\\) and the same idiosyncratic risk \\(\\left(\\sigma_{\\varepsilon}^2\\right)\\). 20.1.1 Part A Find the weights of the minimum-variance portfolio. \\[ \\begin{aligned} \\mathbf{R} &amp;= \\left[ \\begin{array}{c} R_{1} \\\\ \\vdots \\\\ R_{N} \\end{array} \\right] = \\left[ \\begin{array}{c} \\beta \\\\ \\vdots \\\\ \\beta \\end{array} \\right] R_{M} + \\left[ \\begin{array}{c} \\epsilon_{1} \\\\ \\vdots \\\\ \\epsilon_{N} \\end{array} \\right] \\\\ \\\\ \\Sigma = \\mathbb{V}(\\mathbf{R}) &amp;= \\mathbb{V}(\\beta\\underline{1} R_{M} + \\epsilon_{i})\\\\ &amp;= \\beta^{2} \\underline{1} \\mathbb{V}(R_{M}) \\underline{1}^{T} + \\mathbb{V}(\\epsilon_{i}) \\mathbf{I}\\\\ &amp;= \\beta^{2} \\sigma_{M}^{2} \\left[ \\begin{array}{ccc} 1 &amp; \\dots &amp; 1 \\\\ \\vdots &amp; \\ddots{} &amp; \\vdots \\\\ 1 &amp; \\dots &amp; 1 \\end{array} \\right] + \\left[ \\begin{array}{cccc} \\sigma_{\\epsilon}^{2} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma_{\\epsilon}^{2} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma_{\\epsilon}^{2} \\end{array} \\right] \\\\ &amp;= \\left[ \\begin{array}{ccc} \\beta^{2} \\sigma_{M}^{2} + \\sigma_{\\epsilon}^{2} &amp; \\dots &amp; \\beta^{2} \\sigma_{M}^{2} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\beta^{2} \\sigma_{M}^{2} &amp; \\dots &amp; \\beta^{2} \\sigma_{M}^{2} + \\sigma_{\\epsilon}^{2} \\end{array} \\right] \\\\ \\end{aligned} \\] The minimum variance portfolio weights as calculated in class are: \\[ w^{*} = \\frac{\\Sigma ^{-1} \\underline{1}}{\\underline{1}^{T}\\Sigma ^{-1} \\underline{1}} \\] By symmetry of the variance matrix, all the weights must be equal, implying \\(w = \\frac{1}{N} \\underline{1}\\). Each row sum of the matrix is also equivalent, as intuitively all the assets follow the same distribution and have the same systematic risk. It also means that changing the order of the assets in the vector \\(\\mathbf{R}\\) does not change the covariance matrix. 20.1.2 Lemma for special inverse covariance matrices Given that \\(a,b &gt; 0\\) \\[ \\boldsymbol{\\Sigma}=a \\mathbf{1 1}^T+b \\boldsymbol{I}=\\left[\\begin{array}{ccc} a &amp; \\ldots &amp; a \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a &amp; \\cdots &amp; a \\end{array}\\right]+\\left[\\begin{array}{ccc} b &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; b \\end{array}\\right] \\] then \\[ \\boldsymbol{\\Sigma}^{-1}=c \\mathbf{1 1}^T+d \\boldsymbol{I}=\\left[\\begin{array}{ccc} c &amp; \\ldots &amp; c \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ c &amp; \\cdots &amp; c \\end{array}\\right]+\\left[\\begin{array}{ccc} d &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; d \\end{array}\\right] \\] where \\(c = -\\frac{1}{b} \\frac{a}{Na+b}\\) and \\(d = \\frac{1}{b}\\) 20.1.3 Part B Show that the minimum variance can never be smaller than \\(\\beta^2 \\sigma_M^2\\), no matter how many assets we have (i.e., no matter how large \\(N\\) is). \\[ \\begin{aligned} \\mathbb{E}\\left[R_{m v}\\right] &amp; =\\mathbb{E}\\left[\\boldsymbol{w}^T \\boldsymbol{R}\\right]=\\boldsymbol{w}^T E[\\boldsymbol{R}]=\\frac{1}{N} \\mathbf{1}^T\\left(\\beta \\mu_M \\mathbf{1}\\right) \\\\ &amp; =\\beta \\mu_M \\frac{\\mathbf{1}^T \\mathbf{1}}{N}=\\beta \\mu_M \\quad\\left(\\text { since } \\mathbf{1}^T \\mathbf{1}=N\\right) \\\\ \\\\ \\mathbb{V}\\left[R_{m v}\\right] &amp; =\\mathbb{V}\\left[\\boldsymbol{w}^T \\boldsymbol{R}\\right]=\\boldsymbol{w}^T \\mathbb{V}[\\boldsymbol{R}] \\boldsymbol{w}=\\frac{1}{N^2} \\mathbf{1}^T\\left(\\beta^2 \\sigma_M^2 \\mathbf{1 1}^T+\\sigma_{\\varepsilon}^2 \\boldsymbol{I}\\right) \\mathbf{1}= \\\\ &amp; =\\frac{\\beta^2 \\sigma_M^2}{N^2}\\left(\\mathbf{1}^T \\mathbf{1}\\right)\\left(\\mathbf{1}^T \\mathbf{1}\\right)+\\frac{\\sigma_{\\varepsilon}^2}{N^2}\\left(\\mathbf{1}^T \\mathbf{1}\\right)=\\beta^2 \\sigma_M^2+\\frac{\\sigma_{\\varepsilon}^2}{N}&gt;\\beta^2 \\sigma_M^2, \\forall N \\geq 1 \\end{aligned} \\] Thus, the minimum variance portfolio will always have variance at least \\(β^{2}\\sigma_{M}^{2}\\). This problem illustrates that you cannot “diversify away” systemic risk (i.e., risk from common factors) the same way you can with idiosyncratic risk. 20.2 Q2 Consider the following 2 -factor model with 3 assets: \\[ \\left[\\begin{array}{l} R_1 \\\\ R_2 \\\\ R_3 \\end{array}\\right]=\\left[\\begin{array}{ll} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 0 &amp; 1 \\end{array}\\right]\\left[\\begin{array}{l} F_1 \\\\ F_2 \\end{array}\\right]+\\left[\\begin{array}{l} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\end{array}\\right] \\Leftrightarrow \\boldsymbol{R}=\\boldsymbol{\\beta}^{\\top} \\boldsymbol{F}+\\boldsymbol{\\varepsilon} \\] where \\[ \\mathbb{V}[\\boldsymbol{F}]=\\left[\\begin{array}{cc} \\sigma^2 &amp; 0 \\\\ 0 &amp; \\sigma^2 \\end{array}\\right]=\\sigma^2 \\boldsymbol{I}_2, \\quad \\mathbb{V}[\\varepsilon]=\\left[\\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\end{array}\\right]=\\sigma^2 \\boldsymbol{I}_3, \\quad \\operatorname{Cov}[\\boldsymbol{\\varepsilon}, \\boldsymbol{F}]=0 \\] Find the minimum variance portfolio weights for this model. (Hint: You can use \\(\\mathrm{R}\\) to invert the matrix.) From class, \\[ w^{\\star} = \\frac{\\Sigma ^{-1} \\underline{1}}{\\underline{1}^{T}\\Sigma^{-1} \\underline{1}} \\] So we first need to find \\(\\Sigma\\). \\[ \\begin{aligned} \\Sigma &amp;= \\mathbb{V}(\\mathbf{\\beta^{T}F+\\epsilon})\\\\ &amp;= \\mathbf{\\beta}^{T}\\mathbb{V}(\\mathbf{F})\\mathbf{\\beta} + \\mathbb{V}(\\mathbf{\\epsilon})\\\\ &amp;= \\sigma^{2} \\mathbf{\\beta^{T}}\\mathbf{I}\\mathbf{\\beta} + \\sigma^{2}\\mathbf{I_{3}}\\\\ &amp;= \\sigma^{2} \\left[\\begin{array}{ll} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 0 &amp; 1 \\end{array}\\right]\\left[\\begin{array}{lll} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{array}\\right] + \\sigma^{2}\\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]\\\\ &amp;= \\sigma^{2} \\left[\\begin{array}{lll} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\end{array}\\right] + \\sigma^{2}\\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]\\\\ &amp;= \\sigma^{2}\\left[\\begin{array}{lll} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{array}\\right] = \\left[\\begin{array}{lll} 2\\sigma^{2} &amp; 1\\sigma^{2} &amp; 0 \\\\ 1\\sigma^{2} &amp; 3\\sigma^{2} &amp; 1 \\sigma^{2}\\\\ 0 &amp; 1\\sigma^{2} &amp; 2\\sigma^{2} \\end{array}\\right] \\end{aligned} \\] Solving with R, we get \\(\\Sigma ^{-1}\\) \\[ \\Sigma ^{-1} = \\left[ \\begin{array}{ccc} 0.625 &amp; -0.25 &amp; 0.125 \\\\ -0.250 &amp; 0.50 &amp; -0.250\\\\ 0.125 &amp; -0.25 &amp; 0.625 \\end{array} \\right] \\] So the weights become: \\[ w^{\\star} = \\frac{\\left[ \\begin{array}{ccc} 0.5 &amp; 0.0 &amp; 0.5 \\end{array} \\right]}{1} = \\left[ \\begin{array}{ccc} 0.5 &amp; 0.0 &amp; 0.5 \\end{array} \\right] \\] which means that we split our portfolio equally between the 1st and 3rd assets. Notice that the \\(R_{1}\\) and \\(R_{3}\\) are uncorrelated, so splitting (or “diversifying”) the portfolio between these uncorrelated assets will give the highest reduction in variance. These questions encompass q5-15 on pg 489 from SDAFE 18.8 20.3 Q3 Suppose one has a sample of monthly log returns on two stocks with sample means of 0.0032 and 0.0074, sample variances of 0.017 and 0.025, and a sample covariance of 0.0059. For purposes of resampling, consider these to be the “true population values.” A bootstrap resample has sample means of 0.0047 and 0.0065, sample variances of 0.0125 and 0.023, and a sample covariance of 0.0058. 20.3.1 Part A Using the resample, estimate the efficient portfolio of these two stocks that has an expected return of 0.005; that is, give the two portfolio weights. \\[ \\begin{aligned} 0.005 &amp;= \\mathbb{E}(wS_{1}+(1-w)S_{2}) = w\\cdot 0.0047 + (1-w) \\cdot 0.0065\\\\ 0.005-0.0065&amp;= w(0.0047-0.0065)\\\\ w &amp;= \\frac{0.005-0.0065}{(0.0047-0.0065)}\\\\ &amp;= 0.8333\\\\ 1-w &amp;= 0.1667 \\end{aligned} \\] For some reason, the answers state: (0.0047)w + (0.0065)(1 − w) = 0.005 so that the estimated efficient portfolio is 57.14% stock 1 and 42.86% stock 2. 20.3.2 Part B What is the estimated variance of the return of the portfolio in part (a) using the resample variances and covariances? \\[ \\begin{aligned} \\mathbb{V}(wS_{1}+(1-w)S_{2}) &amp;= w^{2}\\mathbb{V}(S_{1})+(1-w)^{2}\\mathbb{V}(S_{2})+2w(1-w)Cov(S_{1},S_{2})\\\\ &amp;= w^{2}.0125 + (1-w)^{2}\\cdot 0.023 + 2w(1-w)0.0058\\\\ &amp;= 0.01092 \\end{aligned} \\] 20.3.3 Part C What are the actual expected return and variance of return for the portfolio in (a) when calculated with the true population values (e.g., with using the original sample means, variances, and covariance)? \\[ \\begin{aligned} \\mathbb{E}(R_{act}) &amp;= 0.8333 \\cdot 0.0032 + .16667\\cdot 0.0074 = 0.0038\\\\ \\mathbb{E}(R_{act}) &amp;= .8333^2 \\cdot 0.0032+0.16667^{2}\\cdot 0.0074 + 2\\cdot 0.8333 \\cdot 0.16667 \\cdot 0.0059\\\\ &amp;=0.00406 \\end{aligned} \\] 20.4 Q4 Straight from answer key For this problem you will use regression to identify the composition of various mutual funds. 20.4.1 Part A Download the adjusted daily closing prices from Jan 1 2020 to Dec 31 2022 for the 5 mutual funds below (use tseries::get.hist.quote() for each ticker): FCNTX: Fidelity Contrafund PIODX: Pioneer A AIVSX: American Funds Invmt Co of Amer A PRBLX: Parnassus Core Equity Investor VFIAX: Vanguard 500 Index Admiral Note that each of these funds has at least 90% of their weight in the US stocks market. You can actually check the composition of the investment over different stock sectors from Yahoo Finance, under the fund’s holdings tab; e.g. for FCNTX at https://finance.yahoo.com/quote/FCNTX/holdings. library(tseries) N = 5 funds = c(&quot;FCNTX&quot;, &quot;PIODX&quot;, &quot;AIVSX&quot;, &quot;PRBLX&quot;, &quot;VFIAX&quot;) P = list() for(i in 1:N){ P[[i]] = tseries::get.hist.quote(funds[i], start = as.Date(&quot;2020-01-01&quot;), end = as.Date(&quot;2022-12-31&quot;), quote=&quot;AdjClose&quot;, compression = &quot;d&quot;, quiet = T) } R_mut=lapply(P, FUN = function(x){ diff(x) /stats::lag(x,-1) }) # calculate MF returns m_funds = matrix(unlist(R_mut), ncol=N) colnames(m_funds) = funds 20.4.2 Part B Assume you do not have any information about the investment strategy of the funds. Download the daily prices and calculate returns of the following EFTs, which track different sectors of the economy: - XLB: Basic Materials - XLY: Consumer Cyclical - XLF: Financial Services - VNQ: Real Estate - XLP: Consumer Defensive - XLV: Healthcare - XLU: Utilities - XTL: Communication Services - XLE: Energy - XLI: Industrials - XLK: Technology Regress each of the mutual fund returns on the above ETF returns and create barplots of the estimated beta coefficients. Do these accurately reflect the allocation over the different sectors (as described in Yahoo Finance)? etfs = c(&quot;XLB&quot;, &quot;XLY&quot;, &quot;XLF&quot;, &quot;VNQ&quot;, &quot;XLP&quot;, &quot;XLV&quot;, &quot;XLU&quot;, &quot;XTL&quot;,&quot;XLE&quot;, &quot;XLI&quot;, &quot;XLK&quot;) sectors = c(&quot;Basic Materials&quot;,&quot;Consumer Cyclical&quot;,&quot;Financial Services&quot;, &quot;Real Estate&quot;,&quot;Consumer Defensive&quot;, &quot;Healthcare&quot;, &quot;Utilities&quot;, &quot;Communication Services&quot;, &quot;Energy&quot;, &quot;Industrials&quot;, &quot;Technology&quot;) M = length(etfs) length(sectors) ## [1] 11 ETFS = list() for(i in 1:M){ ETFS[[i]] = tseries::get.hist.quote(etfs[i], start = as.Date(&quot;2020-01-01&quot;), end = as.Date(&quot;2022-12-31&quot;), quote=&quot;AdjClose&quot;, compression = &quot;d&quot;, quiet = T) } R_etf = lapply(ETFS, FUN=function(x){ diff(x) / stats::lag(x,-1)}) m_etfs = matrix(unlist(R_etf), ncol = M) colnames(m_etfs) = etfs par(mfrow=c(2,3)) out=list(); for(i in 1:N){ out[[i]]=lm(m_funds[,i] ~ m_etfs) weights = out[[i]]$coef[-1] barplot( rev(weights), names.arg = rev(sectors), main = funds[i], horiz = TRUE, las = 2, cex.names = .8) } The barplots of the regression coefficients (betas) roughly follow the sector weightings for each fund. Nevertheless, they are not always close in actual value (e.g. in some cases the betas are negative, even though weightings are positive). The differences can be due to the fact that we use ETFs as proxies for a sector, but the actual holding of the fund within the sector might be different. Moreover, there will be estimation error in our regression model, which is only based on the last year’s returns. Nevertheless, it is quite impressing that we can (approximately) identify the strategy of a fund, without knowing anything beyond its past returns. This approach works because of the linear formula for net portfolio returns: \\[ R_p = w_1 R_1 + \\cdots + w_N R_N\\] Regressing portfolio returns on other assets, we can estimate the weights, assuming the portfolio composition is constant. 20.4.3 Part C Compare the performance of the mutual funds to that of a portfolio of ETFs by reporting the value of Jensen’s alpha (based on the regressions from the previous part) and its corresponding p-value. alpha=p.val=rep(0,N) for(i in 1:N){ alpha[i]=out[[i]]$coefficients[1]*250 # annualized Jensen alpha p.val[i]=summary(out[[i]])$coefficients[1,4] # first coef (alpha)&#39;s p-val } cbind(alpha, p.val) ## alpha p.val ## [1,] -0.03766 0.251 ## [2,] -0.01396 0.557 ## [3,] -0.02970 0.144 ## [4,] -0.01708 0.458 ## [5,] -0.00494 0.609 All the funds’ alphas are negative, although their p-values are not very small. A likely cause for this is that funds charge a fee which consistently eats up some of the returns of their constituent assets. ETFs have typically lower fees than mutual funds, but our regression does not account for transaction costs (it is more costly to buy multiple assets than a single one), so the comparison is more nuanced. Note that you can find Jensen alphas and other performance measures (e.g., Sharpe &amp; Treynor Ratios) for assets in Yahoo! Finance under the risk tab. These metrics are based on the CAPM/Market factor model, by regressing the asset’s returns on a proxy for the market return (e.g., S&amp;P500). "],["problem-set-6.html", "Chapter 21 Problem Set 6 21.1 Q1 21.2 Q2 21.3 Q3 21.4 Q4 21.5 Q5 21.6 Q6 21.7 Q7", " Chapter 21 Problem Set 6 21.1 Q1 Find a closed form expression for the VaR at confidence level \\((1-\\alpha)\\) of the following continuous loss distributions. 21.1.1 Part A Pareto distribution with \\(\\mathrm{CDF} F_L(x)=1-(x / m)^{-\\beta}, x&gt;m\\), with shape parameter (tail index) \\(\\beta&gt;0\\). \\[ \\begin{aligned} \\mathbb{P}(L \\leq VaR_{\\alpha}) &amp;= 1-\\alpha\\\\ 1-\\left( \\frac{VaR_{\\alpha}}{m} \\right)^{-\\beta} &amp;= 1-\\alpha\\\\ \\alpha^{-1/\\beta}m &amp;= VaR_{\\alpha} \\end{aligned} \\] ### Part B Gumbel distribution with \\(\\operatorname{CDF} F_L(x)=\\exp \\left\\{-\\exp \\left\\{-\\frac{x-\\mu}{\\sigma}\\right\\}\\right\\}, \\quad \\forall x \\in \\mathbb{R}\\), with mean parameter \\(\\mu \\in \\mathbb{R}\\) and scale parameter \\(\\sigma&gt;0\\). \\[ \\begin{aligned} \\mathbb{P}(L \\leq VaR_{\\alpha}) &amp;= 1-\\alpha\\\\ \\exp \\left\\{ -\\exp \\left\\{ - \\frac{VaR_{\\alpha}-\\mu}{\\sigma} \\right\\} \\right\\} &amp;= 1-\\alpha\\\\ VaR_{\\alpha} &amp;= -\\ln(\\ln((1-\\alpha)^{-1}))\\cdot \\sigma+\\mu\\\\ &amp;= \\mu + \\sigma\\ln\\left( \\frac{1}{\\ln\\left( \\frac{1}{1-\\alpha} \\right)} \\right) \\end{aligned} \\] ### Part C Fréchet distribution with \\(\\operatorname{CDF} F_L(x)=\\exp \\left\\{-(x / \\sigma)^{-\\beta}\\right\\}, \\quad \\forall x&gt;0\\), with shape parameter \\(\\beta&gt;0\\) and scale parameter \\(\\sigma&gt;0\\). \\[ \\begin{aligned} \\mathbb{P}(L \\leq VaR_{\\alpha}) &amp;= 1-\\alpha\\\\ \\exp \\left\\{ -\\left( \\frac{VaR_{\\alpha}}{\\sigma} \\right)^{-\\beta} \\right\\} &amp;= 1-\\alpha\\\\ VaR_{\\alpha} &amp;= \\sigma\\ln\\left( \\frac{1}{1-\\alpha} \\right)^{-1/\\beta} \\end{aligned} \\] 21.2 Q2 For a loss RV \\(L\\) with continuous distribution, show that the integral definition of Conditional Value-at-Risk (CVaR), a.k.a. Expected Shortfall (ES), is equivalent to the conditional expectation: \\[ \\frac{1}{\\alpha} \\int_0^\\alpha \\operatorname{VaR}_u d u=\\mathbb{E}\\left[L \\mid L \\geq \\operatorname{VaR}_\\alpha\\right] \\] (Hint: Recall that for absolutely continuous \\(L\\) with CDF \\(F(\\cdot)\\), the VaR is given by the inverse CDF: \\(\\operatorname{VaR}_\\alpha=F^{-1}(1-\\alpha)\\). Perform the change of variable \\(x=F^{-1}(1-u)\\), using the fact that the derivative of the inverse of \\(F\\) is \\(\\left[F^{-1}(x)\\right]^{\\prime}=1 / F^{\\prime}\\left(F^{-1}(x)\\right)=\\) \\(1 / f\\left(F^{-1}(x)\\right)\\), where \\(f=F^{\\prime}\\) is the PDF.) \\[ \\begin{aligned} \\text{Let } x &amp;= F^{-1}(1-u), \\implies VaR_{u} = 1-x\\\\ \\frac{d}{du} x &amp;= [F^{-1}(1-u)]&#39; = \\underbrace{ -\\frac{1}{F&#39;(F^{-1}(1-u))} = -\\frac{1}{f(x)} }_{ \\text{by definition} } \\\\ du &amp;= f(x)dx\\\\ \\frac{1}{\\alpha} \\int ^{\\alpha}_{0}VaR_{u} \\, du &amp;= \\frac{1}{\\alpha} \\int ^{\\alpha}_{0} F^{-1}(1-u) \\, du\\\\ &amp;= \\frac{1}{\\alpha} \\int ^{F^{-1}(1-\\alpha)}_{F^{-1}(1-0)} xf(x)\\, dx\\\\ &amp;= \\int ^{VaR_{\\alpha}}_{\\infty}x \\frac{f(x)}{\\alpha} \\, dx \\\\ &amp;= \\int ^{VaR_{\\alpha}}_{\\infty}x \\frac{f(x)}{\\underbrace{ \\mathbb{P}(L \\geq VaR_{\\alpha}) }_{ = \\text{cond. PDF } f_{L|L\\geq VaR_{\\alpha}}}} \\, dx \\\\ \\end{aligned} \\] Since we are now integrating from \\(VaR_{\\alpha}\\) to \\(\\infty\\), f(x) is still some pdf spanning \\([-\\infty,\\infty]\\), so we need to scale it appropriately by the total probability we are in the space we’re integrating, ie \\([VaR_{\\alpha}, \\infty]\\). \\[ \\begin{aligned} CVaR_{\\alpha} &amp;=\\int ^{\\infty}_{VaR_{\\alpha}} x f_{L|L\\geq VaR_{\\alpha}}(x) \\, dx\\\\ &amp;= \\mathbb{E}(L|L\\geq VaR_{\\alpha}) \\end{aligned} \\] 21.3 Q3 (Exercise 2 from 19.13 of SDAFE) Assume that the loss distribution has a polynomial tail with tail index \\(α = 3.1\\). If \\(VaR_{5\\%} = 252\\), what is \\(VaR(0.005)\\)? (Hint: Read section 19.6) We need to use the complementary CDF of returns to obtain the CDF of losses. A loss distribution with polynomial tail would look something like: \\[ \\bar{F}(x) = P(X&gt;x) \\propto \\int ^{\\infty}_{x} s^{-(\\alpha+1)} \\, ds \\propto [-s^{-\\alpha}]^{\\infty}_{s=x} = x^{-\\alpha} \\] If we plug in \\(x = VaR_{5\\%} = 5\\%\\) then we would have \\[ P(X &gt; VaR_{5\\%}) = 5\\% \\implies 5\\% \\propto (VaR_{5\\%})^{-\\alpha} \\implies VaR_{5\\%} \\propto \\frac{1}{0.05^{1/\\alpha}} \\] Given that \\(VaR_{5\\%} = 252\\), then By definition, \\[ P(R \\leq y) \\sim \\int^{y}_{-\\infty} f(u) \\, du = \\frac{A}{a}y^{-a} \\text{ as } y \\to -\\infty \\] Which somehow means that: \\[ \\begin{aligned} \\frac{P(R &lt; y_{0})}{P(R&lt; -y_{1})} &amp;\\approx \\left( \\frac{y_{0}}{y_{1}} \\right)^{-a}\\\\ \\end{aligned} \\] \\[ \\frac{VaR_{p}}{VaR_{q}} = \\left( \\frac{P(L \\geq p)}{P(L\\geq q)} \\right)^{-1/a} = \\left( \\frac{P(L \\geq q)}{P(L\\geq p)} \\right)^{1/a} \\] Using this definition, we have: \\[ \\begin{aligned} \\frac{P(L \\geq VaR_{0.005})}{P(L \\geq VaR_{0.05})} &amp;= \\left( \\frac{VaR_{0.005}}{VaR_{0.05}}\\right) ^{-a}\\\\ \\left( \\frac{0.005}{0.05} \\right)^{-1/a} &amp;= \\frac{VaR_{0.005}}{252}\\\\ \\left( \\frac{0.05}{0.005} \\right)^{1/a} &amp;= \\frac{VaR_{0.005}}{252}\\\\ \\\\ VaR_{0.005} = 10^{1/a}\\cdot 252 &amp;= 10^{1/3.1}\\cdot 252 \\end{aligned} \\] 21.4 Q4 Consider the example with the two risky zero-coupon bonds priced at $95 per $100 face value, where each has 4% default probability independently of the other. 21.4.1 Part A Calculate the \\(\\alpha = 5\\%\\) Entropic Value-at-Risk (EVaR) for one of these bonds. Note that you will need to use numeric minimization, e.g. optimize() in R, to find EVaR. \\[ \\begin{aligned} EVaR = inf_{z &gt; 0}\\left\\{ \\ln\\left( \\frac{M_{L}(z)}{\\alpha} \\right)/z \\right\\} \\end{aligned} \\] The (marginal) loss distribution of each bond \\(\\left(L_{1 \\ or\\ 2}\\right)\\) is given by the PMF \\[ p_L(\\ell)=\\mathbb{P}(L=\\ell)= \\begin{cases}0.04, &amp; \\ell=95-0=95 \\\\ 0.96, &amp; \\ell=95-100=-5\\end{cases} \\] with MGF (not series expansion, simply expected formula as we have a simple PMF) \\[ M_L(z)=\\mathbb{E}\\left[e^{z L}\\right]=0.04 e^{95 z}+0.96 e^{-5 z} \\] The \\(\\mathrm{EVaR}\\) at \\(\\alpha\\) is given by \\[ \\begin{aligned} E V a R_\\alpha &amp; =\\inf _{z&gt;0}\\left\\{\\ln \\left(M_L(z) / \\alpha\\right) / z\\right\\} \\\\ &amp; =\\inf _{z&gt;0}\\left\\{\\ln \\left\\{\\left(0.04 e^{95 z}+0.96 e^{-5 z}\\right) / 0.05\\right\\} / z\\right\\} \\end{aligned} \\] Running this minimization w.r.t. \\(z\\) in \\(\\mathrm{R}\\), fn = function(z){ log( ( 0.04 * exp( 95*z ) + 0.96 * exp(-5*z) ) / 0.05 ) / z } optimise(fn, c(0,1) ) z must lie between \\([0,1]\\), as the MGF of any value greater than 1 results in \\(\\mathbb{E}[e^{zL}] \\to \\infty\\) since \\(e^{zL}\\) would become an increasing function. gives $minimum [1] 0.06690106 $objective [1] 92.10402 Which leaves us with \\(EVaR_{0.05}(L) = 92.10402\\). 21.4.2 Part B Calculate the EVaR of a portfolio of two of these bonds, and show that it is sub-additive. The loss distribution for the sum of the two bonds \\(\\left(L_1+L_2\\right)\\) is \\[ p_{L_1+L_2} L(\\ell)=\\mathbb{P}\\left(L_1+L_2=\\ell\\right)= \\begin{cases}(0.04)^2=0.0016, &amp; \\ell=95+95=190 \\\\ 2(0.96)(0.04)=.0768, &amp; \\ell=95-5=90 \\\\ (0.96)^2=.9216, &amp; \\ell=-5-5=-10\\end{cases} \\] with MGF \\[ M_{L_1+L_2}(z)=\\mathbb{E}\\left[e^{z\\left(L_1+L_2\\right)}\\right]=0.0016 e^{190 z}+0.0768 e^{90 z}+0.09216 e^{-10} \\] Running this minimization w.r.t. \\(z\\) in \\(\\mathrm{R}\\) fn = function(z){ log( ( 0.0016 * exp( 190*z ) + 0.0768 * exp(90*z) + 0.9216 * exp(-10*z)) / 0.05 ) / z } optimise(fn, c(0,1) ) $minimum [1] 0.03841828 $objective [1] 122.0294 This shows that \\[ EVaR_{0.05}(L_{1}+L_{2}) \\leq EVaR_{0.05}(L_{1}) + EVaR_{0.05}(L_{2}) \\] \\[ 122.0294 \\leq 2 \\times 92.10402 = 184.208 \\] 21.5 Q5 Consider a loss distribution with fat upper tail and some tail index \\(\\beta &gt; 0\\). 21.5.1 Part A Can you find an EVaR for such distributions? Justify your answer. You cannot, as to calculate EVaR you need the MGF, and the MGF is not defined for fat tailed distributions. Specifically, moment \\(i\\), where \\(i\\geq\\beta\\) are infinite. 21.5.2 Part B If the tail index is \\(\\beta = 1\\) (e.g., a Cauchy distribution), can you find CVaR/ES for such distributions? Justify your answer. Because \\(\\beta=1\\), that means the first moment and beyond are infinite. As CVaR calculates the expected value of the distribution given our losses are already larger than some number, this calculation will not be possible with tail index 1. We also know that conditional distributions like the one used in CVaR are proportional to the original distribution, which means that the mean is also proportional to infinite. 21.6 Q6 (Exercise 5 from \\(\\S 19.13\\) of SDAFE) Suppose the risk measure is VaR for some \\(\\alpha\\). Let \\(P_1, P_2\\) be two portfolios whose returns have a joint (2D) normal distribution with means \\(\\mu_1, \\mu_2\\), standard deviations \\(\\sigma_1, \\sigma_2\\), and correlation \\(\\rho\\). Suppose the initially investments are \\(S_1, S_2\\). Show that \\(\\operatorname{VaR}\\left(P_1+P_2\\right) \\leq \\operatorname{VaR}_\\alpha\\left(P_1\\right)+\\operatorname{VaR}_\\alpha\\left(P_2\\right), \\forall \\alpha&lt;1 / 2\\), i.e. that \\(\\operatorname{VaR}\\) is sub-additive in this case, for \\(\\alpha&lt;1 / 2\\). Copied all this cuz no way i’d figure this out myself… Solution: Since \\(P_i \\sim N\\left(\\mu_i, \\sigma_i^2\\right)\\) with initial investment amounts \\(S_i\\) for \\(i=1,2\\), we have that \\(\\operatorname{VaR}_\\alpha\\left(P_i\\right)=-S_i \\times\\left(\\mu_i+\\sigma_i z_\\alpha\\right)=-S_i \\mu_i-S_i \\sigma_i z_\\alpha, \\forall i=1,2\\), where \\(z_\\alpha=\\Phi^{-1}(\\alpha)\\) is the standard Normal quantile function (i.e., \\(\\Phi(\\cdot)\\) is the standard Normal CDF). Thus, \\[ \\operatorname{VaR}_\\alpha\\left(P_1\\right)+\\operatorname{VaR}_\\alpha\\left(P_2\\right)=-\\left(S_1 \\mu_1+S_2 \\mu_2\\right)-\\left(S_1 \\sigma_1+S_2 \\sigma_2\\right) z_\\alpha \\] Just from the VaR definition, now we look at the portfolio Looking at the combined portfolio, with weights \\(w_i=\\frac{S_i}{S_1+S_2}, \\quad i=1,2\\) proportional to the investment amounts in each stock, we have: \\[ w_1 P_1+w_2 P_2 \\sim N\\left(w_1 \\mu_1+w_2 \\mu_2, w_1^2 \\sigma_1^2+w_2^2 \\sigma_2^2+2 w_1 w_2 \\sigma_1 \\sigma_2 \\rho\\right) \\] The portfolio distribution by statistics The resulting VaR is \\[ \\begin{gathered} \\operatorname{VaR}_\\alpha\\left(P_1+P_2\\right)= \\\\ =-\\left(S_1+S_2\\right) \\times\\left(\\left(w_1 \\mu_1+w_2 \\mu_2\\right)+\\sqrt{w_1^2 \\sigma_1^2+w_2^2 \\sigma_2^2+2 w_1 w_2 \\sigma_1 \\sigma_2 \\rho} \\times z_\\alpha\\right) \\\\ =-\\left(S_1 \\mu_1+S_2 \\mu_2\\right)-\\sqrt{S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2 \\rho} \\times z_\\alpha \\end{gathered} \\] Again using the definition of VaR But for any \\(\\rho \\in[-1,+1]\\), we have: \\[ \\begin{aligned} S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2 \\rho &amp;\\leqslant S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2\\\\ &amp;=\\left(S_1 \\sigma_1+S_2 \\sigma_2\\right)^2 \\\\ \\Rightarrow \\sqrt{S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2 \\rho} &amp;\\leqslant S_1 \\sigma_1+S_2 \\sigma_2 \\quad\\left(\\text { for } \\alpha&lt;.5 \\rightarrow z_\\alpha&lt;0\\right) \\\\ \\Rightarrow-\\sqrt{S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2 \\rho} \\times z_\\alpha &amp;\\leqslant-\\left(S_1 \\sigma_1+S_2 \\sigma_2\\right) \\times z_\\alpha\\\\ \\\\ \\Rightarrow-\\left(S_1 \\mu_1+S_2 \\mu_2\\right) z_\\alpha- &amp;\\sqrt{S_1^2 \\sigma_1^2+S_2^2 \\sigma_2^2+2 S_1 S_2 \\sigma_1 \\sigma_2 \\rho} \\times z_\\alpha \\leqslant\\\\ &amp;-\\left(S_1 \\mu_1+S_2 \\mu_2\\right) \\times z_\\alpha-\\left(S_1 \\sigma_1+S_2 \\sigma_2\\right) \\times z_\\alpha \\\\ \\\\ \\Rightarrow \\operatorname{VaR}_\\alpha\\left(P_1+P_2\\right) &amp;\\leqslant \\operatorname{VaR}_\\alpha\\left(P_1\\right)+\\operatorname{VaR}_\\alpha\\left(P_2\\right) \\end{aligned} \\] 21.7 Q7 For an investment of $4,000, what are estimates of \\(VaR^t(0.05)\\) and \\(ES^t(0.05)\\)? Now, fit a ARMA(0,0)+GARCH(1,1) model to the returns and calculate one step forecasts. CokePepsi = read.table(&quot;ProblemSets/datasets/CokePepsi.csv&quot;, header=T) price = CokePepsi[,1] returns = diff(price)/lag(price)[-1] ts.plot(returns) S=4000 alpha = 0.05 library(MASS) res = fitdistr(returns,&#39;t&#39;) mu = res$estimate[&#39;m&#39;] lambda = res$estimate[&#39;s&#39;] nu = res$estimate[&#39;df&#39;] qt(alpha, df=nu) ## [1] -2.33 dt(qt(alpha, df=nu), df=nu) ## [1] 0.0465 library(fGarch) # for qstd() function library(rugarch) garch.t = ugarchspec(mean.model=list(armaOrder=c(0,0)), variance.model=list(garchOrder=c(1,1)), distribution.model = &quot;std&quot;) KO.garch.t = ugarchfit(data=returns, spec=garch.t) show(KO.garch.t) ## ## *---------------------------------* ## * GARCH Model Fit * ## *---------------------------------* ## ## Conditional Variance Dynamics ## ----------------------------------- ## GARCH Model : sGARCH(1,1) ## Mean Model : ARFIMA(0,0,0) ## Distribution : std ## ## Optimal Parameters ## ------------------------------------ ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 0.000675 0.000240 2.81396 0.004894 ## omega 0.000003 0.000004 0.79633 0.425839 ## alpha1 0.093766 0.035130 2.66910 0.007606 ## beta1 0.892222 0.038780 23.00727 0.000000 ## shape 5.890859 1.097148 5.36925 0.000000 ## ## Robust Standard Errors: ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 0.000675 0.000257 2.6235 0.008704 ## omega 0.000003 0.000016 0.1717 0.863675 ## alpha1 0.093766 0.149882 0.6256 0.531575 ## beta1 0.892222 0.171120 5.2140 0.000000 ## shape 5.890859 2.499085 2.3572 0.018413 ## ## LogLikelihood : 4596 ## ## Information Criteria ## ------------------------------------ ## ## Akaike -6.2209 ## Bayes -6.2030 ## Shibata -6.2210 ## Hannan-Quinn -6.2143 ## ## Weighted Ljung-Box Test on Standardized Residuals ## ------------------------------------ ## statistic p-value ## Lag[1] 0.8041 0.3699 ## Lag[2*(p+q)+(p+q)-1][2] 0.8718 0.5417 ## Lag[4*(p+q)+(p+q)-1][5] 2.1466 0.5842 ## d.o.f=0 ## H0 : No serial correlation ## ## Weighted Ljung-Box Test on Standardized Squared Residuals ## ------------------------------------ ## statistic p-value ## Lag[1] 0.9099 0.3401 ## Lag[2*(p+q)+(p+q)-1][5] 5.1826 0.1391 ## Lag[4*(p+q)+(p+q)-1][9] 6.7262 0.2232 ## d.o.f=2 ## ## Weighted ARCH LM Tests ## ------------------------------------ ## Statistic Shape Scale P-Value ## ARCH Lag[3] 0.1194 0.500 2.000 0.7297 ## ARCH Lag[5] 0.8386 1.440 1.667 0.7814 ## ARCH Lag[7] 1.9592 2.315 1.543 0.7260 ## ## Nyblom stability test ## ------------------------------------ ## Joint Statistic: 15.8 ## Individual Statistics: ## mu 0.05684 ## omega 2.24878 ## alpha1 0.28907 ## beta1 0.12949 ## shape 0.14088 ## ## Asymptotic Critical Values (10% 5% 1%) ## Joint Statistic: 1.28 1.47 1.88 ## Individual Statistic: 0.35 0.47 0.75 ## ## Sign Bias Test ## ------------------------------------ ## t-value prob sig ## Sign Bias 1.040 0.29849 ## Negative Sign Bias 1.760 0.07857 * ## Positive Sign Bias 1.041 0.29825 ## Joint Effect 4.374 0.22380 ## ## ## Adjusted Pearson Goodness-of-Fit Test: ## ------------------------------------ ## group statistic p-value(g-1) ## 1 20 20.23 0.3807 ## 2 30 26.52 0.5976 ## 3 40 35.17 0.6455 ## 4 50 41.55 0.7662 ## ## ## Elapsed time : 0.509 plot(KO.garch.t, which = 2) ## ## please wait...calculating quantiles... pred = ugarchforecast(KO.garch.t, data=returns, n.ahead=1) ; pred ## ## *------------------------------------* ## * GARCH Model Forecast * ## *------------------------------------* ## Model: sGARCH ## Horizon: 1 ## Roll Steps: 0 ## Out of Sample: 0 ## ## 0-roll forecast [T0=1974-01-15 19:00:00]: ## Series Sigma ## T+1 0.0006751 0.01038 fitted(pred) ; sigma(pred) ## 1974-01-15 19:00:00 ## T+1 0.000675 ## 1974-01-15 19:00:00 ## T+1 0.0104 nu = as.numeric(coef(KO.garch.t)[5]) q = qstd(alpha, mean = fitted(pred), sd = sigma(pred), nu = nu) ; q ## 1974-01-15 19:00:00 ## T+1 -0.0158 sigma(pred)/sqrt( (nu)/(nu-2) ) ## 1974-01-15 19:00:00 ## T+1 0.00844 qt(alpha, df=nu) ## [1] -1.95 dt(qt(alpha, df=nu), df=nu) ## [1] 0.0688 mu = as.numeric(res$estimate[&#39;m&#39;]) lambda = as.numeric(res$estimate[&#39;s&#39;]) nu = as.numeric(res$estimate[&#39;df&#39;]) qt(alpha, df=nu) ## [1] -2.33 # [1] -2.292 dt(qt(alpha, df=nu), df=nu) ## [1] 0.0465 # [1] 0.048 Finv = mu + lambda * qt(alpha, df=nu) VaR = -S * Finv options(digits=4) VaR ## [1] 75.32 # [1] 75.31 den = dt(qt(alpha, df=nu), df=nu) ES = S * (-mu + lambda*(den/alpha) * (nu+qt(alpha, df=nu)^2 )/(nu-1)) ES ## [1] 124 # [1] 122.1 "],["w2021-midterm.html", "Chapter 22 W2021 Midterm 22.1 Q2", " Chapter 22 W2021 Midterm 22.1 Q2 For this question you will generate GPD random variates and verify that their conditional excess distribution is also GPD. 22.1.1 Part A \\([10\\) points \\(]\\) Simulate \\(n=10,000\\) random variates from a \\(\\operatorname{GPD}(\\gamma=.5, \\sigma=1)\\) distribution using the inverse CDF method from Q1.(d), and plot the histogram of their log-values. u = runif(10000) fx = 1/0.5*((1-u)^(-0.5)-1) hist(log(fx)) 22.1.2 Part B [15 points \\(]\\) Simulate another 10,000 random variates from \\(\\operatorname{GPD}(\\gamma=.5, \\sigma=1)\\), but now use the mixture method from Q1.(e). Create a QQ-plot of the two sets of variates (from this and the previous part0), and comment whether they seem to come from the same distribution? (Hint: Use qqplot \\((\\mathrm{X}, \\mathrm{Y})\\), where \\(\\mathrm{X}, \\mathrm{Y}\\) are the values you generated.) rg = rgamma(10000,shape=2, rate = 2) Y = rexp(10000, rate=rg) qqplot(fx, Y) abline(a=0,b=1) 22.1.3 Part C [15 points] Take the values from Q2.(a) and calculate their excedances above 10, i.e. \\(Z=X-10\\) for values of \\(X\\) that are greater than 10. From Q1.(b), we know that \\(Z\\) must follow \\(G P D(\\gamma, \\sigma+10 \\gamma)\\). Verify that the exceedances follow this distribution, by creating a QQ-plot of \\(Z\\)-values versus their theoretical quantiles. sigma = 1 gamma = 0.5 u = 10 Z = (fx - u)[fx-u&gt;0] Zs = sort(Z) #need sorting for qqplot nZ= length(Zs) sigma_u = sigma + gamma * u Q = sigma_u /gamma * ( (1 - ppoints(nZ) )^(-gamma) - 1 ) plot( Zs, Q ); abline(0,1) 22.1.4 Part D [10 points \\(]\\) Finally, verify through simulation that conditional exceedances of fattail distributions with tail index \\(\\alpha\\) approach a GPD with \\(\\gamma=1 / \\alpha\\). Generate \\(n=10,000\\) values from a \\(t(d f=4)\\) distribution (i.e. \\(\\alpha=4)\\) and take their absolute value (to avoid wasting values by symmetry). Calculate the exceedances above 10 again, and create their QQ-plot versus quantiles from a \\(\\operatorname{GPD}(\\gamma=1 / \\alpha, \\sigma=1)\\). (Note: the points in your QQ-plot should lie close to a straight line, but the slope does not need to be equal to 1 because of the arbitrary GPD scale \\(\\sigma=1\\) ). # t = abs(rt(10000,4)) W = abs( rt( 100000, 2 ) ) # why 2? Ws = (W - u)[W-u&gt;0] Ws = sort(Ws) nW= length(Ws) Q = (1+u) * ( (1 - ppoints(nW) )^(-gamma) - 1 ) qqplot( Ws, Q ) "],["w2020-midterm.html", "Chapter 23 W2020 Midterm 23.1 Q3 23.2 Q4", " Chapter 23 W2020 Midterm 23.1 Q3 You have to perform a simulation experiment to confirm that the t(1) or Cauchy distribution is stable, but not all heavy-tailed distributions are. 23.1.1 Part A [4 points] Simulate 10, 000 Cauchy random variates and plot their cumulative mean versus the sample size, as in the plot below. n = 10000 s = 1:n X = rcauchy(n) cum_m = cumsum(X)/s plot(x=s, y = cum_m, type=&quot;l&quot;) 23.1.2 Part B [8 points] Repeat the following experiment N = 1, 000 times: simulate n = 100 Cauchy variates and calculate their mean (in the end you should have 1,000). From question 1, we know these means should be also Cauchy distributed. Verify this by creating a QQ-plot of the simulated values versus their theoretical quantiles, and comment on the fit. Hint: Use qcauchy and ppoints() to generate theoretical quantiles from the Cauchy distribution.) library(tidyverse) N=1000 n = 100 C = rcauchy(n*N) %&gt;% matrix(ncol=N) %&gt;% colSums()/n %&gt;% sort() Th = qcauchy(ppoints(N)) qqplot(C, Th); abline(0,1) 23.1.3 Part C [8 points] Now consider the Pareto distribution, with CDF FX(x) \\(= 1 − (1/x)^\\alpha, \\forall x &gt; 1\\). Use the inverse CDF method to generate 10,000 Pareto random variates with tail index \\(\\alpha\\) = 3, and plot their cumulative mean vs sample size, as in part (a). u = runif(10000) y = (1 - u)^(-1/3) my = cumsum(y)/(1:10000) plot(my, type = &#39;l&#39; ) abline(h=1.5, col = 2, lty=2) 23.2 Q4 Consider the 3 -factor model for the returns of two assets: \\[ \\left[\\begin{array}{l} R_1 \\\\ R_2 \\end{array}\\right]=\\left[\\begin{array}{l} .03 \\\\ .04 \\end{array}\\right]+\\left[\\begin{array}{ccc} .1 &amp; .2 &amp; .3 \\\\ .3 &amp; .2 &amp; .1 \\end{array}\\right]\\left[\\begin{array}{c} F_1 \\\\ F_2 \\\\ F_3 \\end{array}\\right]+\\left[\\begin{array}{l} e_1 \\\\ e_2 \\end{array}\\right] \\Leftrightarrow \\boldsymbol{R}=\\boldsymbol{\\mu}+\\boldsymbol{\\beta}^{\\top} \\boldsymbol{F}+\\boldsymbol{e} \\] where \\(\\boldsymbol{F} \\sim N_3(\\mathbf{0}, \\boldsymbol{I})\\) and \\(\\boldsymbol{e} \\sim N_2\\left(\\mathbf{0}, \\boldsymbol{\\Sigma}_e\\right)\\) with \\(\\boldsymbol{\\Sigma}_e=\\left[\\begin{array}{cc}.1 &amp; 0 \\\\ 0 &amp; .2\\end{array}\\right]\\) and \\(\\boldsymbol{F} \\perp \\boldsymbol{e}\\) 23.2.1 Part A [5 points] Calculate the variance-covariance matrix of the returns \\(\\Sigma_R = \\mathbb{V}[R]\\). beta = matrix( c(1,2,3,3,2,1)/10, nrow = 2, byrow = TRUE ) S_e = diag( c(.1,.2) ) (S_R = beta %*% t(beta) + S_e) ## [,1] [,2] ## [1,] 0.24 0.10 ## [2,] 0.10 0.34 23.2.1.1 Part B [5 points] Calculate the mean and variance of the return of the minimum-variance portfolio. Si = solve(S_R) w = rowSums(Si)/sum(Si) mu = c(.03, .04) mvp_mu = sum(w*mu) (mvp_s = t(w) %*% S_R %*% w) # also equal to 1/sum(Si) ## [,1] ## [1,] 0.1884 23.2.2 Part C [10 points] Assume the risk-free rate is .015, and find the mean and variance of the tangency portfolio. rf = .015 w_tp = Si %*% (mu - rf) / sum( Si %*% (mu - rf) ) sum( w_tp * mu ) # tangency portfolio mean ## [1] 0.03634 t(w_tp) %*% S_R %*% w_tp # tangency portfolio variance ## [,1] ## [1,] 0.2152 23.2.3 Part D [10 points] Assume you could eliminate one of the factors (i.e. set \\(\\mathbb{V}[F_i] = 0\\)). Which factor would you choose to eliminate in order to further reduce the variance of the minimum- variance portfolio? (justify your answer with a calculation &amp; comparison) mvp_vars = rep(0,3) for(i in 1:3){ beta_t = beta[,-i] S_t = beta_t %*% t(beta_t) + S_e Si_t = solve(S_t) mvp_vars[i] = 1/sum(Si_t) } which.min(mvp_vars) ## [1] 3 "],["cheatsheet.html", "Chapter 24 Cheatsheet 24.1 Probability 24.2 Math 24.3 Series 24.4 Distributions 24.5 Copula 24.6 Portfolio Theory 24.7 Factor Model 24.8 Risk Mgmt 24.9 Betting measures 24.10 Arbitrage 24.11 Simulation 24.12 Variance Reduction", " Chapter 24 Cheatsheet 24.1 Probability \\[ \\begin{gathered} \\text{Variance: } \\mathbb{V}(Z) = \\mathbb{E}(Z^{2})-\\mathbb{E}(Z)^{2}\\\\ \\\\ \\text{Lin Com of RV: } Z = wA + (1-w)B\\\\ \\mathbb{E}(Z) = \\theta \\implies w = \\frac{\\theta -\\mu_{B}}{\\mu_{A}-\\mu_{B}}\\\\ Var(Z) = w^{2}Var(A) + (1-w)^{2}Var(B) - 2\\underbrace{ Cov(A,B) }_{ Cor(A,B)\\cdot \\sigma(A)\\cdot\\sigma(B) } \\end{gathered} \\] MLE Estimator for parameter \\(\\theta\\) in \\(f(x)\\): \\[ \\begin{gathered} L(\\theta) = \\prod^{n}_{i=1} f(x;\\theta)\\\\ l(\\theta) = \\sum^{n}_{i=1} f(x;\\theta)\\\\ \\text{set } \\frac{ \\partial l }{ \\partial \\theta } = l&#39;(\\theta) = 0, \\text{solve for } \\theta \\end{gathered} \\] Moment Generating Functions Characteristic functions \\[ \\begin{gathered} \\text{MGF:} m_{X}(t) = \\mathbb{E}(e^{tX})=\\sum^{\\infty}_{k=1} \\frac{t^{k}}{k!}\\mathbb{E}[X^{k}] = \\int_{-\\infty}^{\\infty} e^{tx}f_{X}(x) \\, dx \\\\ \\text{Char Fnc: } \\phi_{X}(t) =\\mathbb{E}(e^{itX}) = \\mathbb{E}[\\cos (tX)+i\\sin(tX)]\\\\ \\text{Let } X \\sim \\text{Stable, sym about 0} \\implies CF(X) = \\phi(t) = e^{-c|t|^{\\alpha}} \\end{gathered} \\] Normal distribution \\[ \\phi(x) = \\frac{1}{\\sigma\\sqrt{ 2\\pi }}\\exp \\left\\{ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^{2} \\right\\} \\] Facts: Summing i.i.d RVs with infinite variance \\(\\to\\) Heavy-tailed Distributions Summing i.i.d RVs with finite variance \\(\\to\\) Normal Distributions Summing i.i.d STABLE random variables \\(\\to\\) Stable Distribution Summing i.i.d heavy-tail (\\(0&lt;\\alpha &lt; 2)\\) random variables \\(\\to\\) Stable Distribution Finite mixtures: It suffices to show (due to induction) that a convex combination of 2 copulas \\(\\left(C()=w C_1()+(1-w) C_2, \\quad \\forall w \\in[0,1]\\right)\\) is also copula. We are given that a convex combination of two CDFs is a CDF, and this result holds for any number of dimensions. The last piece is to show that the marginals are Uniform \\((0,1)\\). Since the initial copulas have Uniform \\((0,1)\\) marginals, it is straightforward to see that that mixture copula will also have Uniform \\((0,1)\\) marginals: for 1D Uniforms CDFs \\(F_1(u)=F_2(u)=u, \\quad \\forall u \\in[0,1]\\), we have \\(F(u)=w F_1(u)+(1-w) F_2(u)=w u+(1-w) u=u, \\quad \\forall u \\in[0,1]\\) 24.2 Math \\[ \\int_{a}^{b} \\ln(x) \\, dx = \\left[ x\\ln(x) - x\\right] ^{b}_{x=a} \\] 24.3 Series 24.3.1 Integrated series A variable that is not stationary but it’s differences are, is called an integrated series. \\(\\left\\{ X_{t} \\right\\}\\) is not stationary but \\(\\left\\{ \\nabla X_{t} = X_{t} - X_{t-1}\\right\\}\\) is. E.g. If log returns are stationary, then the log prices are integrated 24.3.2 Cointegration Consider two integrated series \\(\\left\\{ X_{t}, Y_{t} \\right\\}\\) which behave as random walks, but if they seem to have some constant (stationary) relationship when linearly combined then they are called cointegrated. \\[ \\exists \\ \\alpha\\ s.t.\\ X_{t} + \\alpha Y_{t} \\sim Stationary \\] 24.3.3 Brownian Motion Brownian Motion (BM) forms the building block of continuous stochastic models Standard BM \\(\\left\\{W_t\\right\\}\\) is such that \\[ W_0=0 \\quad \\&amp; \\quad (W_t-W_s) \\mid W_s \\sim N(0, t-s) \\] \\[ \\begin{gathered} Cov(W_{s},W_{t}) = min(s,t)\\\\ (W_{s}-W_{t}) \\sim N(0, (s-t)) \\quad \\forall t&lt;s \\end{gathered} \\] Using the inverse CDF method to generate random variates: \\[ \\begin{gathered} X \\sim Pareto \\implies F(x) = 1-\\left( \\frac{1}{x} \\right)^{\\alpha} = u \\in [0,1]\\\\ P(X \\leq x) = P(1-\\left( \\frac{1}{U} \\right)^{\\alpha} \\leq x)=P(U \\leq (1-x)^{-1/\\alpha})\\\\ \\implies F^{-1}(u) = (1-u)^{-1/\\alpha} = x \\end{gathered} \\] u = runif(n) y = (1 - u)^(-1/3) 24.3.4 Arithmetic BM/RW Arithmetic BM (ABM) \\(\\left\\{X_t\\right\\}\\) with drift \\(\\mu\\) &amp; volatility \\(\\sigma\\) is \\[ X_0=0 \\quad \\&amp; \\quad \\left(X_t-X_s\\right) \\mid X_s \\sim N\\left(\\mu(t-s), \\sigma^2(t-s)\\right) \\] In form of Stochastic Differential Equation (SDE) \\[ d X_t=\\mu d t+\\sigma d W_t \\Leftrightarrow X_t-X_0=\\mu t+\\sigma\\left(W_t-W_0\\right) \\] 24.3.5 Geometric Brownian Motion A transformation of the arithmetic brownian motion. We use this to avoid negative values. Process \\(\\left\\{S_t\\right\\}\\) whose logarithm follows ABM \\[ \\begin{aligned} &amp; \\log \\left(S_t\\right)-\\log \\left(S_0\\right)=\\log \\left(\\frac{S_t}{S_0}\\right)=X_t \\sim N\\left(\\mu t, \\sigma^2 t\\right) \\Leftrightarrow \\\\ &amp; \\Leftrightarrow S_t=S_0 \\exp \\left\\{X_t\\right\\} \\sim S_0 \\times \\log \\operatorname{Normal}\\left(\\mu t, \\sigma^2 t\\right) \\end{aligned} \\] As the log of \\(\\exp \\left\\{ X_{t} \\right\\} = X_{t}\\) and \\(X_{t} \\sim Normal(\\mu t, \\sigma^{2}t)\\) 24.4 Distributions 24.4.1 Multivariate Normal Property Formula Marginals: \\[\\quad \\mathbf{X}_1 \\sim \\mathbf{N}\\left(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_{11}\\right)\\] Linear combinations: \\[\\mathbf{a}+\\mathbf{B}^{\\top} \\mathbf{X} \\sim N\\left(\\mathbf{a}+\\mathbf{B}^{\\top} \\boldsymbol{\\mu}, \\mathbf{B}^{\\top} \\boldsymbol{\\Sigma} \\mathbf{B}\\right)\\] Conditionals : \\[\\mathbf{X}_1 \\mid\\left(\\mathbf{X}_2=\\mathbf{x}\\right)\\sim\\mathrm{N}\\left(\\boldsymbol{\\mu}_1+\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\left(\\mathbf{x}\\boldsymbol{\\mu}_2\\right),\\boldsymbol{\\Sigma}_{11}-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}\\right)\\] 24.4.2 Heavy-tailed distribution: has an infinite MGF Heavy tailed distributions can also have infinite moments (including the mean!) \\[ \\mathbb{E}(X^{k}) = \\infty \\text{ for } k \\geq \\alpha \\] 24.4.3 Stable Distribution: If the sum of two individual identical ones gives the same distribution (up to location &amp; scale parameters), than distribution is Stable Ex. Sum of two i.i.d Normals is Normal \\(\\implies\\) Normal Distribution is stable All stable distributions (besides the Normal) have heavy tails with tail index \\(=\\alpha\\) in the characteristic function 24.4.4 Cauchy distribution (t distribution with 1 df): \\[ CF(t) = \\phi_{X}(t) = e^{-|t|} \\] 24.4.5 Gamma distribution: \\[ \\begin{gathered} {\\displaystyle f(x)={\\frac {\\beta ^{\\alpha }}{\\Gamma (\\alpha )}}x^{\\alpha -1}e^{-\\beta x}}\\\\ {\\displaystyle F(x)={\\frac {1}{\\Gamma (\\alpha )}}\\gamma (\\alpha ,\\beta x)}\\\\ \\mathbb{E}(x) = {\\frac {\\alpha }{\\beta }} \\quad \\mathbb{V}(x) = {\\displaystyle {\\frac {\\alpha }{\\beta ^{2}}}}\\\\ \\int_0^{\\infty} \\lambda^{\\alpha-1} e^{-(\\beta+x) \\lambda} d \\lambda = \\frac{\\Gamma(\\alpha)}{(\\beta+x)^\\alpha} \\end{gathered} \\] 24.4.6 Exponential distribution: \\[ \\begin{gathered} X \\sim Exp(\\lambda)\\\\ f(x) = \\lambda e^{{-\\lambda x}}\\\\ F(x) = {\\displaystyle 1-e^{-\\lambda x}}\\\\ \\mathbb{E}(x) = \\frac{1}{\\lambda} \\quad \\mathbb{V}(x) = \\frac{1}{\\lambda^{2}} \\end{gathered} \\] Example of integral of a mixture model: \\[ \\begin{gathered} Z = \\frac{X}{Y}\\\\ F_{Z}(z) = P(Z\\leq z) = P(X/Y \\leq z)\\\\ = \\int_{domain \\ Y} P(X/Y &lt; z|Y = y) dF_{Y}(y) = \\int P(X &lt; yz) f_{Y}(y) \\, dy\\\\ \\end{gathered} \\] 24.4.7 1st Theorem: Fisher-Tippet-Gnedenko \\[ \\begin{aligned} H(x) &amp;= \\begin{cases} Gumbel &amp; \\exp\\{-e^{-x}\\} \\quad\\quad x \\in \\mathbb{R}\\\\ Frechet &amp; \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\exp\\{-x^{-\\alpha}\\} &amp; x &gt;0 \\end{cases}\\\\ Weibull &amp; \\begin{cases} \\exp\\{-|x|^{\\alpha}\\} &amp; x&lt;0 \\\\ 1 &amp; x &gt; 0 \\end{cases} \\end{cases} \\end{aligned} \\] 24.4.8 Generalized Extreme Value (GEV) Distribution \\[ \\begin{aligned} H(x) &amp;= \\exp\\left\\{ -\\left( 1+\\xi\\frac{x-\\mu}{\\sigma} \\right)^{-1/\\xi}_{+} \\right\\}\\\\ \\text{Where } \\mu &amp;= \\text{location}\\\\ \\sigma &amp;= \\text{scale}\\\\ \\xi &amp;= \\text{shape parameters} \\begin{cases} \\xi&gt;0 &amp; \\text{heavy tails (Frechet)} \\\\ \\xi=0 &amp; \\text{exponential tails (Gumbel)} \\\\ \\xi &lt; 0 &amp; \\text{short/light tails (Weibull)} \\end{cases} \\end{aligned} \\] 24.4.9 Normal scale mixture \\(Y = \\mu+\\sqrt{ V } \\cdot Z\\) Where \\(V\\) is a RV with non-negative mixing distribution and represents a random sd of \\(Y\\). Example (probably don’t need to memorize…) which is used when simulating returns, as seem in PS2 Q4c part ii and iii. \\[ \\begin{aligned} \\text{t-dist } t &amp;= Z\\sqrt{ v/W } \\quad \\text{ where } W \\sim \\chi^{2}(df=v)\\\\ \\text{GARCH model } r_{t} &amp;= \\mu + \\sigma_{t}Z_{t} \\quad \\text{ where } \\sigma^{2}_{t} = \\omega + \\sum^{p}_{i=1}a_{i}r^{2}_{t-i}+\\sum^{q}_{j=1}\\beta_{j}\\sigma^{2}_{t-j}\\\\ \\end{aligned} \\] 24.5 Copula [[4-Multivariate-Return-Modelling]] https://richardye101.github.io/STAD70/copulas.html \\[ \\begin{gathered} \\text{Independence Copula: } C_{indep} (u_{1}, \\dots u_{d}) = u_{1} \\times \\dots \\times u_{d}\\\\ \\text{Archimedean Copula: } C_{arch}(u_{1}, \\dots u_{d}) = \\phi ^{-1}(\\phi(u_{1})+\\dots+\\phi(u_{d}))\\\\ \\text{Given multi/uni var CDF $\\phi_{\\rho}, \\phi$: } C_{\\rho}(u_{1},\\dots ,u_{d}) = \\Phi_{\\boldsymbol{\\rho}}(\\Phi ^{-1}(u_{1}), \\dots, \\Phi ^{-1}(u_{d}))\\\\ \\\\ \\text{Copula Properties} \\\\ \\underline{C}\\left(u_1, \\ldots, u_d\\right) \\leq C\\left(u_1, \\ldots, u_d\\right) \\leq \\bar{C}\\left(u_1, \\ldots, u_d\\right) \\\\ \\text { where } \\begin{cases} \\underline{C}\\left(u_1, \\ldots, u_d\\right)=\\max \\left\\{1-d+\\sum_{i=1}^d u_i, 0\\right\\} = \\max\\left\\{ 1 - \\left(\\sum^d_{i=1} 1 - u_{i}\\right),0 \\right\\} \\\\ \\bar{C}\\left(u_1, \\ldots, u_d\\right)=\\min \\left\\{u_1, \\ldots, u_d\\right\\} \\end{cases} \\end{gathered} \\] If you have a copula and the marginal CDF’s, you can obtain the multivariate CDF of all the marginals. The inverse is true, where you can take a multivariate CDF and come up with a copula to represent the dependency between the marginal distributions, and the marginal distributions themselves. \\(F(x_{1},\\dots x_{d}) = C(F_{1}(x_{1}), \\dots, F_{d}(x_{d}))\\) 24.6 Portfolio Theory To achieve a \\(\\mu_{p}\\) in a two asset portfolio, the weight \\[w = \\frac{\\theta -\\mu_{B}}{\\mu_{A}-\\mu_{B}}\\] \\[ \\begin{aligned} \\text{Gross Returns:}&amp;\\\\ R&#39;(t) &amp;= \\frac{S(t)}{S(t-1)}\\\\ \\text{Net Returns:}&amp;\\\\ R(t) &amp;= R&#39;(t)-1 = \\frac{S(t)-S(t-1)}{S(t-1)}\\\\ \\text{Log Returns:}&amp;\\\\ r(t) &amp;= \\log(R&#39;(t)) = \\log\\{S(t)\\}-\\log\\{S(t-1)\\}\\\\ \\end{aligned} \\] Min-variance portfolio weights are \\[\\vec{w} = \\frac{\\Sigma^{-1} \\vec{1}}{\\vec{1}^{T}\\Sigma ^{-1} \\vec{1}}\\] if we have a variance matrix, we can calculate the minimum weights for each asset using: weights = rowSums(S_inv)/sum(S_inv) 24.6.1 Sharpe Ratio \\[ \\underbrace{ \\left(\\frac{\\mu_{M}-\\mu_{f}}{\\sigma_{M}} \\right)}_{ \\text{Sharpe Ratio} } \\] 24.7 Factor Model 24.7.1 Assumptions Factors \\(F_j(t)\\) are stationary, with moments: \\[ E[\\mathbf{F}(t)]=\\boldsymbol{\\mu}_F \\quad \\&amp; \\quad \\operatorname{Var}[\\mathbf{F}(t)]=\\boldsymbol{\\Sigma}_F \\] Asset-specific errors \\(\\varepsilon_i(t)\\) are uncorrelated with common factors: \\[ \\operatorname{Cov}[\\mathbf{\\varepsilon}(t), \\mathbf{F}(t)]=\\mathbf{0} \\] Errors are serially &amp; contemporaneously uncorrelated across assets \\[ \\operatorname{Var}[\\boldsymbol{\\varepsilon}(t)]=\\operatorname{diag}\\left[\\left\\{\\sigma_{\\varepsilon_i}^2\\right\\}_{i=1, \\ldots, N}\\right]=\\boldsymbol{\\Sigma}_{\\varepsilon} \\quad \\&amp; \\operatorname{Cov}[\\boldsymbol{\\varepsilon}(t), \\boldsymbol{\\varepsilon}(s)]=\\mathbf{0} \\] 24.7.2 In R Fitting a factor model to a list of returns of assets using factanal(x, factors=2), we can extract the factor loadings \\(\\mathbf{B}\\) and uniqueness \\(\\mathbf{V}\\) which can be used to calculate the sample correlation matrix along with the sample variance matrix \\(\\mathbf{SS}^{T}\\) \\[ Var = (\\mathbf{B}\\mathbf{B}^{T}+diag(\\mathbf{V}) ) (\\mathbf{SS}^{T}) \\] 24.8 Risk Mgmt 24.8.1 VaR Calculating the \\(VaR_{\\alpha}\\) at a \\((1-\\alpha)\\) confidence level: \\[ \\begin{gathered} P(L\\leq VaR_{\\alpha}) = 1-\\alpha = F(VaR_{\\alpha})\\\\ \\text{Essentially Isolate $VaR_{\\alpha}$: } VaR_{\\alpha} = F^{-1}(1-\\alpha) \\end{gathered} \\] If \\(L\\) follows a normal distribution, then \\(VaR_{\\alpha}(L) = -S_{i} \\times (\\mu_{i}+\\sigma_{i}z_{\\alpha})\\) where \\(S_{i}\\) is the initial investment, and \\(z_{\\alpha}=\\Phi ^{-1}(\\alpha)\\). 24.8.2 Conditional VaR (CVaR) is the mean loss in the upper \\(\\alpha\\) area of the loss distribution. \\[ -\\frac{1}{\\alpha} \\int_{0}^{\\alpha} VaR_{u}(L) \\, du = \\mathbb{E}(L|L\\geq VaR_{\\alpha}) \\] 24.8.3 Entropic VaR (EVaR) EVaR defined as: \\(\\mathrm{EVaR}_\\alpha=\\inf _{z&gt;0}\\left\\{z^{-1} \\ln \\left(M_L(z) / \\alpha\\right)\\right\\}\\) can use R to find the infemum (value of z such that the function is minimized) fn = function(z){ log( ( 0.04 * exp( 95*z ) + 0.96 * exp(-5*z) ) / 0.05 ) / z } optimise(fn, c(0,1) ) 24.8.4 Risk Measure Properties \\(\\rho\\) here is VaR, CVaR, or EVaR For this measure \\(\\rho\\) to reasonably quantify risk, it must: 1. be normalized \\(\\rho(0)=0\\) (risk of holding no assets is 0) 2. Translation invariance: \\(\\rho(L+c) = \\rho(L) + c \\ \\forall c \\in \\mathbb{R}\\) - adding a loss \\(c\\) to the portfolio increases risk by exactly \\(c\\) 3. Positive Homogeneity: \\(\\rho(bL) = b\\rho(l)\\) - scaling portfolio returns also will scale risk 4. Monotonicity: \\(L_{1}\\geq L_{2} \\implies \\rho(L_{1}) \\geq \\rho(L_{2})\\) - The ordering of the random variables is almost surely \\(P(L_{1}\\geq L_{2})=1\\) 5. Sub-additivity: \\(\\rho(L_{1}+L_{2}) \\leq \\rho(L_{1}+L_{2})\\) - Only when the two losses are perfectly correlated, then equal - The risk of two combined portfolios cannot exceed the sum of the two portfolio risks 24.9 Betting measures Wealth after betting: \\[ \\begin{aligned} V_n &amp; =V_0 \\prod_{t=1}^n(1+f a)^{I_t}(1-f b)^{1-I_t} \\\\ &amp; =V_0(1+f a)^{\\sum_{t=1}^n I_t}(1-f b)^{\\sum_{t=1}^n\\left(1-I_t\\right)} \\\\ &amp; =V_0(1+f a)^{W_n}(1-f b)^{n-W_n} \\end{aligned} \\] Kelly criterion (which maximizes expected log wealth): \\[ \\begin{gathered} \\text{Win amt:} a \\quad \\text{Lose amt:} b \\quad P(win) = p \\quad P(lose) = q\\\\ \\text{Kelly Crit: } f^{\\star} = \\frac{ap-bq}{ab} \\quad \\text{or } 2p-1 \\text{ in the simple case} \\end{gathered} \\] 24.10 Arbitrage 24.10.1 Pairs Trading We trade if we have: \\[ \\begin{gathered} \\text{Profitable if: } \\frac{P1_{c}}{P1_{o}} - \\frac{P2_{c}}{P2_{o}} \\ne 0\\\\ \\implies \\frac{P1_{c}}{P1_{o}} &lt; \\text{ or }&gt; \\frac{P2_{c}}{P2_{o}}\\\\ \\text{Where: } q_{1} =\\frac{1}{P1_{o}}, q_{2} = \\frac{1}{P2_{o}} \\end{gathered} \\] Our strat is profitable if in the end, we trade P1 and P2 with money left over, in the same quantities we entered the positions with. (Could buy/sell either side) If we assume the prices of the stocks have a stationary linear relationship \\(P_{t} − \\lambda S_{t}\\). Furthermore, assume that you open a trade when \\(P_{o} − \\lambda S_{o} &gt; 0\\), and you close it when \\(P_{c} − \\lambda S_{c} = 0\\). This is profitable given the conditions with net profit of \\((P_{o}−\\lambda S_{o})−(P_{c}−\\lambda S_{c}) &gt; 0\\). 24.11 Simulation In Monte Carlo, there may always be some time point that we did not simulate, where the barrier could have been crossed and the value of the an option, ex \\(C_{U\\&amp;O}\\) would have been worthless. Hence the monte carlo simulation will always overestimate the value of an barrier option, because it underestimates the maximum. 24.11.1 Simple Monte Carlo estimator \\[ \\begin{gathered} \\bar{Y} = \\frac{1}{2n}\\sum^{2n}_{i=1} Y_{i}\\quad \\text{where } Y_{i} = f(Z_{i}), Z_{i} \\sim N(\\mu,\\sigma^{2})\\\\ Var(\\bar{Y}) = \\mathbb{E}[f^{2}(z)]\\\\ \\end{gathered} \\] Generating paths of options: We can generate values of \\(M_{T}\\) directly by generating a standard brownian motion \\(W_{T}\\) and setting \\(M_{T} = |W_{T}|\\). We then estimate the probability that \\(|W_{T}|\\) crosses 1, and as we generate more Normal RVs (\\(W_{T}\\)), the probability will converge. 24.11.2 Procedure to generate maxima Procedure for simulating maxima of arithmetic BM: 1. Generate \\(X_T \\sim N\\left(\\mu T, \\sigma^2 T\\right)\\) 2. Generate \\(U \\sim \\operatorname{Uniform}(0,1)\\) 3. Calculate \\(M_T \\mid X_T=\\frac{X_T+\\sqrt{X_T^2-2 \\sigma^2 T \\log (U)}}{2}\\) 24.11.3 Risk Neutral Pricing The arbitrage-free price of any European derivative with payoff \\(G_T=f\\left(S_T\\right)\\) is given by discounted expectation w.r.t. RN measure \\[ G_0=\\mathbb{E}\\left[e^{-r T} G_T\\right]=\\mathbb{E}\\left[e^{-r T} f\\left(S_T\\right)\\right] \\] The risk-neutral pricing measure: \\[ \\begin{gathered} \\mathbb{E}(S) = S_{0}e^{rt}\\\\ \\mathbb{E}\\left( \\frac{S_{t}}{e^{rt}}\\right) = S_{0} \\quad \\text{ or more generally } \\mathbb{E}\\left( \\frac{S_{t}}{e^{rt}} |S_{s} \\right) = \\frac{S_{s}}{e^{rt}} \\end{gathered} \\] 24.11.4 Black Scholes European Call \\[ \\begin{gathered} C = S_{0}\\Phi(d_{1}) - e^{-rT}K\\Phi(d_{2})\\\\ \\text{Where: }\\ d_{1} = \\frac{\\ln(S_{0}/K)+\\left( 2+\\frac{1}{2}\\sigma^{2} \\right)T}{\\sigma \\sqrt{ T }}, \\quad d_{2} = d_{1}-\\sigma \\sqrt{ T } \\end{gathered} \\] At the money call: \\[ C = S_{0}\\left[ \\Phi(d_{1}) -e^{-rT}\\Phi(d_{2}) \\right] = S_{0} \\left[ \\Phi \\left( \\frac{\\left( r+\\frac{1}{2}\\sigma^{2}T \\right)}{\\sigma \\sqrt{ T }} \\right) -e^{-rT}\\Phi \\left( \\frac{\\left( r-\\frac{1}{2}\\sigma^{2} \\right)T}{\\sigma \\sqrt{ T }} \\right) \\right] \\] ## Pricing Derivatives \\[ \\begin{gathered} \\text{Joint CDF of W and max(W): } P(W_{T}\\leq x, M_{T}\\geq y) = \\underbrace{ P(W_{T}\\geq 2y-x, \\overbrace{ M_{T}\\geq y }^{ \\text{always true} }) }_{\\text{By relfection principle} }\\\\ \\text{W is Normally distributed: } P(W_{t} \\leq x) = \\Phi(x)\\\\ \\\\ \\text{Rayleigh Dist: } P(M_{T}\\leq m | X_{T} = b) = 1 - \\exp \\left\\{ -2 \\frac{m(m-b)}{\\sigma^{2}T} \\right\\} \\quad \\forall m\\geq (0 \\cup B) \\end{gathered} \\] 24.12 Variance Reduction 24.12.1 Anti-thetic Variable \\[ \\begin{gathered} \\text{Anti-thetic Variable: } \\bar{Y}_{AV} = \\frac{1}{2n} \\left( \\sum^{n}_{i=1}Y_{i} + \\sum^{n}_{i=1}\\tilde{Y}_{i} \\right) = \\frac{1}{n}\\sum^{n}_{i=1} \\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\\\ Y_{i} = f(Z_{i}), \\tilde{Y}_{i} = \\tilde{f}(-Z_{i})\\\\ \\\\ \\bar{Y}_{AN} \\text{ is sample mean of iid } RV_i:\\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\\\ \\text{by CLT } \\bar{Y_{AN}} \\sim^{approx} N\\left( \\mathbb{E}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right], \\frac{1}{n} \\mathbb{V}\\left[ \\frac{Y_{i}+\\tilde{Y}_{i}}{2} \\right] \\right)\\\\ \\end{gathered} \\] The payoff is only worthwhile if \\(f(Z)\\) is not an even function, as if it was, then \\(\\frac{Y_{i}+\\tilde{Y}_{i}}{2}\\) would simply be \\(Y_{i}\\) and that wouldn’t help us decrease variance. 24.12.2 Stratification This is inspired by statistical sampling. Idea: Split RV domain into equi-probable strata and draw equal number of variates from within each one. \\[ \\begin{aligned} &amp; \\bar{Y}_{S t r}=\\frac{1}{m} \\sum_{j=1}^m \\bar{Y}^{(j)}, \\text { where } \\bar{Y}^{(j)}=\\frac{1}{n} \\sum_{i=1}^n f\\left(Z_i^{(j)}\\right) \\\\ &amp; Z_i^{(j)} \\sim^{\\text {iid }} N\\left(0,1 \\mid Z_i^{(j)} \\in A_j\\right), j=1, \\ldots, m\\\\ \\mathbb{E}(\\bar{Y}_{Str}) &amp;= \\sum^{m}_{j=1}\\mathbb{E}\\left[ f(Z) | Z \\in A_{j}\\right] \\cdot P(A \\in A_{j})\\\\ \\mathbb{V}(\\bar{Y}_{str})&amp;= \\frac{1}{mn} \\left\\{ \\mathbb{E}\\left[ f^{2}(z) - \\frac{1}{m}\\sum^{m}_{j=1}\\mu_{j}^{2} \\right] \\right\\} \\leq \\underbrace{ \\mathbb{V}\\left[ \\bar{Y} \\right] = \\frac{1}{mn}\\left\\{ \\mathbb{E}\\left[ f^{2}(z)-\\mu^{2} \\right] \\right\\} }_{ \\text{Simple Random Sample} } \\leq \\underbrace{ \\mathbb{E}[f^{2}(z)] }_{ \\text{MC Est Var} }\\\\ &amp;\\iff \\frac{1}{m} \\sum^{m}_{j=1} \\mu_{j}^{2} \\geq \\mu^{2}\\\\ \\text{Where } \\mu &amp;= \\frac{1}{m} \\sum^{m}_{j=1}\\mu_{j} = \\sum^{m}_{j=1}\\underbrace{ \\mathbb{E}\\left[ f(z)|z\\in A_{j} \\right] }_{ \\mu_{j} }\\underbrace{ P(A_{j}) }_{ 1/m } \\end{aligned} \\] W21 Midterm Part e, integrating gamma: \\[ \\begin{aligned} \\bar{F}_X(x) &amp; =P(X&gt;x)(\\text { Law of Total Prob. }) \\\\ &amp; =\\int_0^{\\infty} \\overbrace{P(X&gt;x \\mid \\Lambda=\\lambda)}^{\\sim \\operatorname{Exp}(\\lambda)} \\overbrace{f_{\\Lambda}(\\lambda)}^{\\sim \\operatorname{Gamma}(\\alpha, \\beta)} d \\lambda \\\\ &amp; =\\int_0^{\\infty} e^{-\\lambda x} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda} d \\lambda \\\\ &amp; =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^{\\infty} \\lambda^{\\alpha-1} e^{-(\\beta+x) \\lambda} d \\lambda \\\\ &amp; =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha)}{(\\beta+x)^\\alpha} \\\\ &amp; =\\left(\\frac{\\beta}{\\beta+x}\\right)^\\alpha \\\\ &amp; =1-1+\\left(\\frac{\\beta+x}{\\beta}\\right)^{-\\alpha} \\\\ &amp; =1-\\overbrace{\\left[1-(1+x / \\beta)^{-\\alpha}\\right]}^{\\operatorname{CDF} \\text { or GPD }(\\gamma=1 / \\alpha, \\sigma=\\beta / \\alpha)} \\end{aligned} \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
